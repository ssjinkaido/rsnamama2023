{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nfrom glob import glob\nimport os, shutil\nfrom tqdm import tqdm\ntqdm.pandas()\nimport time\nimport copy\nimport joblib\nfrom collections import defaultdict\nimport gc\nfrom IPython import display as ipd\nimport math\n# visualization\nimport cv2\nfrom glob import glob\n# Sklearn\nfrom sklearn.model_selection import StratifiedKFold, KFold, StratifiedGroupKFold\nfrom sklearn.metrics import accuracy_score, roc_auc_score, f1_score, confusion_matrix, roc_curve\nimport timm\n# PyTorch \nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\nimport torch.nn.functional as F\nfrom torch.optim.swa_utils import AveragedModel, SWALR\nfrom transformers import get_cosine_schedule_with_warmup\nfrom collections import defaultdict\n# import matplotlib.pyplot as plt\n# Albumentations for augmentations\nimport albumentations as A\nimport albumentations\nimport albumentations as albu\nfrom albumentations.pytorch import ToTensorV2\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-02-15T07:31:48.346958Z","iopub.execute_input":"2023-02-15T07:31:48.347435Z","iopub.status.idle":"2023-02-15T07:31:56.874432Z","shell.execute_reply.started":"2023-02-15T07:31:48.347337Z","shell.execute_reply":"2023-02-15T07:31:56.873412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    seed = 1\n    model_name = \"tf_efficientnetv2_b2\"\n    train_bs = 16\n    valid_bs = train_bs*4\n    image_size = 1024\n    epochs = 25\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(CFG.device)","metadata":{"execution":{"iopub.status.busy":"2023-02-15T07:31:56.877320Z","iopub.execute_input":"2023-02-15T07:31:56.878335Z","iopub.status.idle":"2023-02-15T07:31:56.947170Z","shell.execute_reply.started":"2023-02-15T07:31:56.878296Z","shell.execute_reply":"2023-02-15T07:31:56.945909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/10folds/train_10folds.csv\")\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-15T07:31:56.949014Z","iopub.execute_input":"2023-02-15T07:31:56.949770Z","iopub.status.idle":"2023-02-15T07:31:57.104760Z","shell.execute_reply.started":"2023-02-15T07:31:56.949729Z","shell.execute_reply":"2023-02-15T07:31:57.103808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def init_logger(log_file='train1.log'):\n    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=log_file)\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = init_logger()\nnow = datetime.now()\ndatetime_now = now.strftime(\"%m/%d/%Y, %H:%M:%S\")\nLOGGER.info(f\"Date :{datetime_now}\")","metadata":{"execution":{"iopub.status.busy":"2023-02-15T07:31:57.107403Z","iopub.execute_input":"2023-02-15T07:31:57.107781Z","iopub.status.idle":"2023-02-15T07:31:57.118869Z","shell.execute_reply.started":"2023-02-15T07:31:57.107745Z","shell.execute_reply":"2023-02-15T07:31:57.117980Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from albumentations import DualTransform\nimage_size = 1024\ndef isotropically_resize_image(img, size, interpolation_down=cv2.INTER_AREA, interpolation_up=cv2.INTER_CUBIC):\n    h, w = img.shape[:2]\n    if max(w, h) == size:\n        return img\n    if w > h:\n        scale = size / w\n        h = h * scale\n        w = size\n    else:\n        scale = size / h\n        w = w * scale\n        h = size\n    interpolation = interpolation_up if scale > 1 else interpolation_down\n    resized = cv2.resize(img, (int(w), int(h)), interpolation=interpolation)\n    return resized\n\n\nclass IsotropicResize(DualTransform):\n    def __init__(self, max_side, interpolation_down=cv2.INTER_AREA, interpolation_up=cv2.INTER_CUBIC,\n                 always_apply=False, p=1):\n        super(IsotropicResize, self).__init__(always_apply, p)\n        self.max_side = max_side\n        self.interpolation_down = interpolation_down\n        self.interpolation_up = interpolation_up\n\n    def apply(self, img, interpolation_down=cv2.INTER_AREA, interpolation_up=cv2.INTER_CUBIC, **params):\n        return isotropically_resize_image(img, size=self.max_side, interpolation_down=interpolation_down,\n                                          interpolation_up=interpolation_up)\n\n    def apply_to_mask(self, img, **params):\n        return self.apply(img, interpolation_down=cv2.INTER_NEAREST, interpolation_up=cv2.INTER_NEAREST, **params)\n\n    def get_transform_init_args_names(self):\n        return (\"max_side\", \"interpolation_down\", \"interpolation_up\")\n    \ndata_transforms = {\n    \"train\": A.Compose([\n        # A.Resize(image_size, image_size),\n        # IsotropicResize(max_side = image_size),\n        # A.PadIfNeeded(min_height=image_size, min_width=image_size, border_mode=cv2.BORDER_CONSTANT),\n        # A.RandomBrightnessContrast(),\n        A.VerticalFlip(p=0.5),   \n        A.ColorJitter(p=0.5),\n        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.05, rotate_limit=10, p=0.5),\n        A.HorizontalFlip(p=0.5),\n        A.Cutout(max_h_size=int(image_size*0.1), max_w_size=int(image_size*0.1), num_holes=5, p=0.5), \n        # A.OneOf([\n        #         A.OpticalDistortion(),\n        #         A.IAAPiecewiseAffine(),\n        #     ], p=0.1),\n        # A.OneOf([\n        #     A.GaussNoise(),\n        #     A.MotionBlur(blur_limit=(3, 5)),\n        # ], p=0.1),\n        # A.ColorJitter(),\n        # A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.05, rotate_limit=10, p=0.5),\n        # A.HorizontalFlip(p=0.5),\n        # A.Cutout(max_h_size=102, max_w_size=102, num_holes=5, p=0.5),\n        # A.CLAHE(p=1.0),\n        # albumentations.HorizontalFlip(p=0.5),\n        # # albumentations.VerticalFlip(p=0.5),\n        # albumentations.RandomBrightness(limit=0.2, p=0.75),\n        # albumentations.RandomContrast(limit=0.2, p=0.75),\n\n        # albumentations.OneOf([\n        #     albumentations.OpticalDistortion(distort_limit=1.),\n        #     albumentations.GridDistortion(num_steps=5, distort_limit=1.),\n        # ], p=0.75),\n\n        # albumentations.HueSaturationValue(hue_shift_limit=40, sat_shift_limit=40, val_shift_limit=0, p=0.75),\n        # albumentations.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.3, rotate_limit=30, border_mode=0, p=0.75),\n        # A.Cutout(always_apply=False, p=0.5, num_holes=1, max_h_size=409, max_w_size=409),\n        # A.OneOf([ \n        # A.OpticalDistortion(distort_limit=1.0), \n        # A.GridDistortion(num_steps=5, distort_limit=1.),\n        # A.ElasticTransform(alpha=3), ], p=0.2),\n        # A.OneOf([\n        #     # A.GaussNoise(var_limit=[10, 50]),\n        #     A.GaussianBlur(),\n        #     A.MotionBlur(),\n        #     A.MedianBlur(), ], p=0.2),\n        # A.OneOf([\n        #     A.GridDistortion(num_steps=5, distort_limit=0.05, p=1.0),\n        #     A.OpticalDistortion(distort_limit=0.05, shift_limit=0.05, p=1.0),\n        #     A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=1.0)\n        # ], p=0.25),\n        # A.CoarseDropout(max_holes=8, max_height=image_size//20, max_width=image_size//20,\n        #                  min_holes=5, fill_value=0, mask_fill_value=0, p=0.5),\n        # A.Normalize(mean=0, std=1),\n        ToTensorV2(),], p=1.0),\n    \n    \"valid\": A.Compose([\n        # IsotropicResize(max_side =image_size),\n        # A.PadIfNeeded(min_height=image_size, min_width=image_size, border_mode=cv2.BORDER_CONSTANT),\n        # A.Normalize(mean=0, std=1),\n        # A.Resize(image_size, image_size),\n        ToTensorV2(),\n        ], p=1.0)\n}\n\nLOGGER.info(f\"train transform{data_transforms['train']}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-02-15T07:31:57.121124Z","iopub.execute_input":"2023-02-15T07:31:57.121526Z","iopub.status.idle":"2023-02-15T07:31:57.140234Z","shell.execute_reply.started":"2023-02-15T07:31:57.121492Z","shell.execute_reply":"2023-02-15T07:31:57.139193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def pad(array, target_shape):\n    return np.pad(\n        array,\n        [(0, target_shape[i] - array.shape[i]) for i in range(len(array.shape))],\n        \"constant\",\n    )\n\ndef load_img2(img_path):\n    image = cv2.imread(img_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    return image\nclass BreastDataset(Dataset):\n    def __init__(self, df, transforms=None):\n        self.df = df\n        self.transforms = transforms\n        \n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        if (os.path.exists(f\"/kaggle/input/27300next/output/{row.patient_id}_{row.image_id}.png\")):\n            img_path = f\"/kaggle/input/27300next/output/{row.patient_id}_{row.image_id}.png\"\n        else:\n            img_path = f\"/kaggle/input/1024bicubic/output/{row.patient_id}_{row.image_id}.png\"\n        img = load_img2(img_path)\n        label = row['cancer']\n        # img = np.transpose(img, (2, 0, 1))\n        data = self.transforms(image=img)\n        img  = data['image']\n        # img = img/255\n        return torch.tensor(img).float(), torch.tensor(label).long()\n        \n    def __len__(self):\n        return len(self.df)\n    \nfold0 = df[df['fold']==0]\ntrain_dataset = BreastDataset(fold0, transforms = data_transforms['train'])\nimage, label = train_dataset[0]\nprint(image.shape, label)\nprint(image.max())","metadata":{"execution":{"iopub.status.busy":"2023-02-15T07:31:57.141718Z","iopub.execute_input":"2023-02-15T07:31:57.142730Z","iopub.status.idle":"2023-02-15T07:31:57.248826Z","shell.execute_reply.started":"2023-02-15T07:31:57.142694Z","shell.execute_reply":"2023-02-15T07:31:57.247750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass ModelOld(nn.Module):\n    def __init__(self, model_name):\n        super().__init__()\n        # ,drop_rate = 0.3, drop_path_rate = 0.2\n        self.backbone = timm.create_model(CFG.model_name, pretrained=False,drop_rate = 0.3, drop_path_rate = 0.2)\n        self.fc = nn.Linear(self.backbone.classifier.in_features,2)\n        self.dropout = nn.Dropout(0.5)\n        self.backbone.classifier = nn.Identity()\n    def forward(self, x):\n        x = self.backbone(x)\n        x = self.fc(self.dropout(x))\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2023-02-15T07:31:57.250236Z","iopub.execute_input":"2023-02-15T07:31:57.250857Z","iopub.status.idle":"2023-02-15T07:31:57.258210Z","shell.execute_reply.started":"2023-02-15T07:31:57.250820Z","shell.execute_reply":"2023-02-15T07:31:57.257330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef asMinutes(s):\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s / (percent)\n    rs = es - s\n    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))","metadata":{"execution":{"iopub.status.busy":"2023-02-15T07:31:57.261359Z","iopub.execute_input":"2023-02-15T07:31:57.261644Z","iopub.status.idle":"2023-02-15T07:31:57.271660Z","shell.execute_reply.started":"2023-02-15T07:31:57.261604Z","shell.execute_reply":"2023-02-15T07:31:57.270774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef valid_fn_two(val_dataloader, model, criterion, device):\n    losses = AverageMeter()\n    model.eval()\n    truth = []\n    preds = []\n    valid_labels = []\n    start = end = time.time()\n    pbar = tqdm(enumerate(val_dataloader), total=len(val_dataloader), desc='Val')\n    for step, (images, labels) in pbar:\n        images = images.to(device, non_blocking=True)\n        labels = labels.to(device, non_blocking=True)\n        batch_size = labels.size(0)\n        with torch.no_grad():\n            outputs = model(images)\n        valid_labels.append(labels.cpu().numpy())\n        loss = criterion(outputs, labels)\n#         loss = bi_tempered_logistic_loss(outputs, labels, t1=0.8, t2 = 1.4)\n        losses.update(loss.item(), batch_size)\n#         print(outputs)\n        preds.append(F.softmax(outputs).to('cpu').numpy())\n        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n        pbar.set_postfix(eval_loss=f'{losses.avg:0.4f}',\n                        gpu_mem=f'{mem:0.2f} GB')\n    predictions = np.concatenate(preds)\n    valid_labels = np.concatenate(valid_labels)\n    return losses.avg, predictions, valid_labels\n","metadata":{"execution":{"iopub.status.busy":"2023-02-15T07:31:57.273241Z","iopub.execute_input":"2023-02-15T07:31:57.273594Z","iopub.status.idle":"2023-02-15T07:31:57.285452Z","shell.execute_reply.started":"2023-02-15T07:31:57.273560Z","shell.execute_reply":"2023-02-15T07:31:57.284552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef pfbeta(labels, predictions, beta=1):\n    y_true_count = 0\n    ctp = 0\n    cfp = 0\n\n    for idx in range(len(labels)):\n        prediction = min(max(predictions[idx], 0), 1)\n        if (labels[idx]):\n            y_true_count += 1\n            ctp += prediction\n        else:\n            cfp += prediction\n\n    beta_squared = beta * beta\n    c_precision = ctp / (ctp + cfp)\n    c_recall = ctp / y_true_count\n    if (c_precision > 0 and c_recall > 0):\n        result = (1 + beta_squared) * (c_precision * c_recall) / (beta_squared * c_precision + c_recall)\n        return result\n    else:\n        return 0\n\ndef pfbeta_np(labels, preds, beta=1):\n    preds = preds.clip(0, 1)\n    y_true_count = labels.sum()\n    ctp = preds[labels==1].sum()\n    cfp = preds[labels==0].sum()\n    beta_squared = beta * beta\n    c_precision = ctp / (ctp + cfp)\n    c_recall = ctp / y_true_count\n    if (c_precision > 0 and c_recall > 0):\n        result = (1 + beta_squared) * (c_precision * c_recall) / (beta_squared * c_precision + c_recall)\n        return result\n    else:\n        return 0.0\n    \ndef dfs_freeze(module):\n    for param in module.parameters():\n        param.requires_grad = False\n        \ndef dfs_unfreeze(module):\n    for param in module.parameters():\n        param.requires_grad = True\n    \ndef set_seed(seed = 42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    print('> SEEDING DONE')\n\ndef sigmoid(x):\n  return 1 / (1 + math.exp(-x))\ndef valid_fn_two(val_dataloader, model, criterion, device):\n    losses = AverageMeter()\n    model.eval()\n    preds = []\n    pbar = tqdm(enumerate(val_dataloader), total=len(val_dataloader), desc='Val')\n    for step, (images, labels) in pbar:\n        images = images.to(device, non_blocking=True)\n        labels = labels.to(device, non_blocking=True)\n        batch_size = labels.size(0)\n        with torch.no_grad():\n            with autocast(enabled=True):\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n#         loss = bi_tempered_logistic_loss(outputs, labels, t1=0.8, t2 = 1.4)\n        losses.update(loss.item(), batch_size)\n#         print(outputs)\n        preds.append(F.softmax(outputs).to('cpu').numpy())\n        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n        pbar.set_postfix(eval_loss=f'{losses.avg:0.4f}',\n                        gpu_mem=f'{mem:0.2f} GB')\n    predictions = np.concatenate(preds)\n    return losses.avg, predictions\nset_seed(1)\ngc.collect()\ntorch.cuda.empty_cache()\nfor fold in [6]:\n    LOGGER.info(\"5 fold\")\n    LOGGER.info(f\"Fold: {fold}\")\n    LOGGER.info(f\"Model name: {CFG.model_name}\")\n    valid_df = df[df['fold']==fold].reset_index(drop=True)\n    LOGGER.info(f\"Len valid df: {len(valid_df)}\")\n    \n    valid_dataset = BreastDataset(valid_df, transforms=data_transforms['valid'])\n\n    valid_loader = DataLoader(valid_dataset, batch_size = CFG.valid_bs, \n                                  num_workers=1, shuffle=False, pin_memory=True, drop_last=False)\n    # model = Model(model_name=CFG.model_name).to(device)\n    best_f1 = 0\n    best_metric = 0\n    total_epoch = 30\n    warmup = 1\n#     model = ModelOld(model_name=CFG.model_name).to(CFG.device)\n#     checkpoint = torch.load(\"/kaggle/input/10folds/tf_efficientnetv2_b2_fold_0_model_epoch_2_0.5510_0.151.pth\")\n#     model.load_state_dict(checkpoint['state_dict'])\n#     criterion = nn.CrossEntropyLoss().to(CFG.device)\n#     LOGGER.info(f\"Train bs: {CFG.train_bs}\")\n#     # LOGGER.info(f\"Model: {model}\")\n#     LOGGER.info(f\"{model.__class__.__name__}\")\n#     LOGGER.info(f\"optimizer: {optimizer}\")\n#     LOGGER.info(f\"total_epoch :{total_epoch}\")\n#     LOGGER.info(f\"Warmup: {warmup}\")\n#     for epoch in range(1, total_epoch+1):\n#         LOGGER.info(f\"Epoch: {epoch}/{total_epoch}\")\n#         # loss_train = train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, CFG.device)\n#         loss_valid, valid_preds = valid_fn_two(valid_loader, model, criterion, CFG.device)\n#         valid_preds = valid_preds[:, 1]\n#         valid_df['prediction_id'] = valid_df['patient_id'].astype(str) + '_' + valid_df['laterality'].astype(str)\n#         valid_preds = np.array(valid_preds).flatten()\n        \n#         valid_df['raw_pred'] = valid_preds\n#         LOGGER.info(f\"Valid loss:{loss_valid:.4f}\")\n#         # LOGGER.info(f\"Train loss:{loss_train:.4f}, Valid loss:{loss_valid:.4f}\")\n#         # print(valid_df.head())\n#         grp_df = valid_df.groupby('prediction_id')['raw_pred', 'cancer'].mean()\n#         grp_df['cancer'] = grp_df['cancer'].astype(np.int)\n#         valid_labels_mean = grp_df['cancer'].values\n#         valid_preds_mean = grp_df['raw_pred'].values\n#         # print(valid_labels[:5], valid_preds_mean[:5])\n#         val_metric_mean = pfbeta(valid_labels_mean, valid_preds_mean)\n#         LOGGER.info(f\"Val metric mean prob: {val_metric_mean:.4f}\")\n#         best_metric_mean_at_epoch = 0\n#         best_metric1 = 0\n#         best_threshold_mean = 0\n#         best_auc = 0\n#         best_cf = None\n#         for i in np.arange(0.001, 0.599, 0.001):\n#             valid_argmax = (valid_preds_mean>i).astype(np.int32)\n#     #             print(valid_argmax)\n#             # val_metric = pfbeta(valid_labels_mean, valid_argmax)\n#             val_metric1 = pfbeta_np(valid_labels_mean, valid_argmax)\n#             val_acc = accuracy_score(valid_labels_mean, valid_argmax)\n#             val_f1 = f1_score(valid_labels_mean, valid_argmax)\n#             val_auc = roc_auc_score(valid_labels_mean, valid_argmax)\n#             cf = confusion_matrix(valid_labels_mean, valid_argmax)\n#             if val_metric1> best_metric1:\n#                 best_metric1 = val_metric1\n#                 # best_metric_mean_at_epoch = val_metric\n#                 best_threshold_mean = i\n#                 best_auc = val_auc\n#                 best_cf = cf\n#         LOGGER.info(f\"Best metric at epoch {epoch}: {best_metric1:.4f} {best_threshold_mean:.4f}  {best_auc:.4f}\")\n#         LOGGER.info(f\"Cf: {best_cf}\")\n#         if best_metric1> best_metric:\n\n#             LOGGER.info(f\"Model improve: {best_metric:.4f} -> {best_metric1:.4f}\")\n#             best_metric = best_metric1\n#         state = {'epoch': epoch, 'state_dict': model.state_dict()}\n#         # state = {'epoch': epoch, 'state_dict': model.state_dict(),'optimizer': optimizer.state_dict(), 'scheduler':scheduler.state_dict()}\n#         path = f'foldonefive/{CFG.model_name}_fold_{fold}_model_epoch_{epoch}_{best_metric1:.4f}_{best_threshold_mean:.3f}.pth'\n#         torch.save(state, path)\n\n        ","metadata":{"execution":{"iopub.status.busy":"2023-02-15T07:49:12.457566Z","iopub.execute_input":"2023-02-15T07:49:12.457974Z","iopub.status.idle":"2023-02-15T07:49:12.795285Z","shell.execute_reply.started":"2023-02-15T07:49:12.457938Z","shell.execute_reply":"2023-02-15T07:49:12.794104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\nfrom optuna.samplers import TPESampler\n\ndef valid_fn_two(val_dataloader, model, criterion, device):\n    losses = AverageMeter()\n    model.eval()\n    preds = []\n    pbar = tqdm(enumerate(val_dataloader), total=len(val_dataloader), desc='Val')\n    for step, (images, labels) in pbar:\n        images = images.to(device, non_blocking=True)\n        labels = labels.to(device, non_blocking=True)\n        batch_size = labels.size(0)\n        with torch.no_grad():\n            with autocast(enabled=True):\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n#         loss = bi_tempered_logistic_loss(outputs, labels, t1=0.8, t2 = 1.4)\n        losses.update(loss.item(), batch_size)\n#         print(outputs)\n        preds.append(F.softmax(outputs).to('cpu').numpy())\n        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n        pbar.set_postfix(eval_loss=f'{losses.avg:0.4f}',\n                        gpu_mem=f'{mem:0.2f} GB')\n    predictions = np.concatenate(preds)\n    return losses.avg, predictions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# set_seed(1)\n# out_file = 'swa_model_fold0_10.pth' \n# iteration = [\n#     '/kaggle/input/10folds/tf_efficientnetv2_b2_fold_0_model_epoch_2_0.5510_0.151.pth',\n#     '/kaggle/input/10folds/tf_efficientnetv2_b2_fold_0_model_epoch_13_0.5750_0.437.pth',\n#     '/kaggle/input/10folds/tf_efficientnetv2_b2_fold_0_model_epoch_11_0.5610_0.440.pth'\n# ]\n\n# criterion = nn.CrossEntropyLoss().to(CFG.device)\n# best_metric = 0\n# torch.cuda.empty_cache()\n# def objective(trial):\n#     a1 = 0.036839841333967636 \n#     a2 = 0.6490629183820655\n#     a3 = 0.3140972402839668\n# #     a2 = 0.47142151346976024 \n# #     a3 = 0.3596277792186039\n# #     a1 = trial.suggest_uniform('a1', 0.01, 0.79)\n# #     a2 = 1-a1\n# #     a1 = trial.suggest_uniform('a1', 0.01, 0.99)\n# #     a2 = trial.suggest_uniform('a2', 0.01, 1-a1)\n# #     a3 = 1-a1-a2\n#     state_dict = None\n#     for i in iteration:\n#         f = i\n#         print(f)\n#         f = torch.load(f, map_location=lambda storage, loc: storage)\n#         if state_dict is None:\n#             print(\"none: \", i)\n#             state_dict = f['state_dict']\n#             key = list(f['state_dict'].keys())\n#             for k in key:\n#                 state_dict[k] = f['state_dict'][k]*a1\n#         elif i=='/kaggle/input/10folds/tf_efficientnetv2_b2_fold_0_model_epoch_13_0.5750_0.437.pth': \n#             print(\"hehe\", i)\n#             key = list(f['state_dict'].keys())\n#             for k in key:\n#                 state_dict[k] = state_dict[k] + a2*f['state_dict'][k]\n#         elif i=='/kaggle/input/10folds/tf_efficientnetv2_b2_fold_0_model_epoch_11_0.5610_0.440.pth':\n#             print(\"noob\", i)\n#             key = list(f['state_dict'].keys())\n#             for k in key:\n#                 state_dict[k] = state_dict[k] + a3*f['state_dict'][k]\n#     print(a1, a2, a3)\n#     # for k in key:\n#     #     state_dict[k] = state_dict[k] / len(iteration)\n#     print('')\n\n#     # print(out_file)\n#     torch.save({'state_dict': state_dict}, out_file)\n\n#     model = ModelOld(model_name=CFG.model_name).to(CFG.device)\n#     checkpoint = torch.load(\"swa_model_fold0_10.pth\")\n#     model.load_state_dict(checkpoint['state_dict'])\n# #     model = nn.DataParallel(model)\n\n#     loss_valid, valid_preds = valid_fn_two(valid_loader, model, criterion, CFG.device)\n#     valid_preds = valid_preds[:, 1]\n#     valid_df['prediction_id'] = valid_df['patient_id'].astype(str) + '_' + valid_df['laterality'].astype(str)\n#     valid_preds = np.array(valid_preds).flatten()\n    \n#     valid_df['raw_pred'] = valid_preds\n#     LOGGER.info(f\"Valid loss:{loss_valid:.4f}\")\n#     grp_df = valid_df.groupby('prediction_id')['raw_pred', 'cancer'].mean()\n#     grp_df['cancer'] = grp_df['cancer'].astype(np.int)\n#     valid_labels_mean = grp_df['cancer'].values\n#     valid_preds_mean = grp_df['raw_pred'].values\n#     # print(valid_labels[:5], valid_preds_mean[:5])\n#     val_metric_mean = pfbeta(valid_labels_mean, valid_preds_mean)\n#     LOGGER.info(f\"Val metric mean prob: {val_metric_mean:.4f}\")\n#     best_metric_mean_at_epoch = 0\n#     best_metric = 0\n    \n#     best_threshold_mean = 0\n#     best_auc = 0\n#     best_cf = None\n#     for i in np.arange(0.001, 0.599, 0.001):\n#         valid_argmax = (valid_preds_mean>i).astype(np.int32)\n#         val_metric = pfbeta_np(valid_labels_mean, valid_argmax)\n#         val_acc = accuracy_score(valid_labels_mean, valid_argmax)\n#         val_f1 = f1_score(valid_labels_mean, valid_argmax)\n#         val_auc = roc_auc_score(valid_labels_mean, valid_argmax)\n#         cf = confusion_matrix(valid_labels_mean, valid_argmax)\n#         if val_metric> best_metric:\n#             best_metric = val_metric\n#             # best_metric_mean_at_epoch = val_metric\n#             best_threshold_mean = i\n#             best_auc = val_auc\n#             best_cf = cf\n#     state = {'state_dict': model.state_dict()}\n#     path = f'swa_{CFG.model_name}_fold_{fold}_model_{best_metric:.4f}_{best_threshold_mean:.4f}.pth'\n#     torch.save(state, path)\n    \n#     LOGGER.info(f\"Best metric at: {best_metric:.4f} {best_threshold_mean:.4f}  {best_auc:.4f}\")\n#     LOGGER.info(f\"Cf: {best_cf}\")\n#     return best_metric\n\n# study = optuna.create_study(direction='maximize', sampler = TPESampler(seed=1))\n# study.optimize(func=objective, n_trials=1)\n# study.best_params\n# # # 0.5563409550491111 0.4436590449508889 fold 0\n# # # 0.12634002523631388 0.8351954705276587 0.03846450423602743 0.5393 \n# # # 0.583301614081906 0.3673525472043472 0.04934583871374687 fold 2 0.50\n# # # 0.1689507073116359 0.47142151346976024 0.3596277792186039 fold 2 0.5055 0.5055 0.3670  0.7261","metadata":{"execution":{"iopub.status.busy":"2023-02-15T07:34:16.107213Z","iopub.execute_input":"2023-02-15T07:34:16.107590Z","iopub.status.idle":"2023-02-15T07:38:03.554899Z","shell.execute_reply.started":"2023-02-15T07:34:16.107557Z","shell.execute_reply":"2023-02-15T07:38:03.553815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set_seed(1)\n# out_file = 'swa_model_fold2_10.pth' \n# iteration = [\n#     '/kaggle/input/10folds/tf_efficientnetv2_b2_fold_2_model_epoch_5_0.4598_0.286.pth',\n#     '/kaggle/input/10folds/tf_efficientnetv2_b2_fold_2_model_epoch_3_0.4615_0.250.pth',\n#     '/kaggle/input/10folds/tf_efficientnetv2_b2_fold_2_model_epoch_7_0.4848_0.266.pth'\n# ]\n\n# criterion = nn.CrossEntropyLoss().to(CFG.device)\n# best_metric = 0\n# torch.cuda.empty_cache()\n# def objective(trial):\n# #     a1 = 0.015006661988523864 \n# #     a2 = 0.12003546043452194 \n# #     a3 = 0.8649578775769542\n#     a1 = 0.020317850755860567 \n#     a2 = 0.1293785181217534 \n#     a3 = 0.850303631122386\n# #     a1 = 0.12634002523631388\n# #     a2 = 0.8351954705276587\n# #     a3 = 0.03846450423602743\n# #     a1 = trial.suggest_uniform('a1', 0.01, 0.99)\n# #     a2 = trial.suggest_uniform('a2', 0.01, 1-a1)\n# #     a3 = 1-a1-a2\n#     state_dict = None\n#     for i in iteration:\n#         f = i\n#         print(f)\n#         f = torch.load(f, map_location=lambda storage, loc: storage)\n#         if state_dict is None:\n#             print(\"none: \", i)\n#             state_dict = f['state_dict']\n#             key = list(f['state_dict'].keys())\n#             for k in key:\n#                 state_dict[k] = f['state_dict'][k]*a1\n#         elif i=='/kaggle/input/10folds/tf_efficientnetv2_b2_fold_2_model_epoch_3_0.4615_0.250.pth': \n#             print(\"hehe\", i)\n#             key = list(f['state_dict'].keys())\n#             for k in key:\n#                 state_dict[k] = state_dict[k] + a2*f['state_dict'][k]\n#         elif i=='/kaggle/input/10folds/tf_efficientnetv2_b2_fold_2_model_epoch_7_0.4848_0.266.pth':\n#             print(\"noob\", i)\n#             key = list(f['state_dict'].keys())\n#             for k in key:\n#                 state_dict[k] = state_dict[k] + a3*f['state_dict'][k]\n#     print(a1, a2, a3)\n#     # for k in key:\n#     #     state_dict[k] = state_dict[k] / len(iteration)\n#     print('')\n\n#     # print(out_file)\n#     torch.save({'state_dict': state_dict}, out_file)\n\n#     model = ModelOld(model_name=CFG.model_name).to(CFG.device)\n#     checkpoint = torch.load(\"swa_model_fold2_10.pth\")\n#     model.load_state_dict(checkpoint['state_dict'])\n# #     model = nn.DataParallel(model)\n\n#     loss_valid, valid_preds = valid_fn_two(valid_loader, model, criterion, CFG.device)\n#     valid_preds = valid_preds[:, 1]\n#     valid_df['prediction_id'] = valid_df['patient_id'].astype(str) + '_' + valid_df['laterality'].astype(str)\n#     valid_preds = np.array(valid_preds).flatten()\n    \n#     valid_df['raw_pred'] = valid_preds\n#     LOGGER.info(f\"Valid loss:{loss_valid:.4f}\")\n#     grp_df = valid_df.groupby('prediction_id')['raw_pred', 'cancer'].mean()\n#     grp_df['cancer'] = grp_df['cancer'].astype(np.int)\n#     valid_labels_mean = grp_df['cancer'].values\n#     valid_preds_mean = grp_df['raw_pred'].values\n#     # print(valid_labels[:5], valid_preds_mean[:5])\n#     val_metric_mean = pfbeta(valid_labels_mean, valid_preds_mean)\n#     LOGGER.info(f\"Val metric mean prob: {val_metric_mean:.4f}\")\n#     best_metric_mean_at_epoch = 0\n#     best_metric = 0\n    \n#     best_threshold_mean = 0\n#     best_auc = 0\n#     best_cf = None\n#     for i in np.arange(0.001, 0.599, 0.001):\n#         valid_argmax = (valid_preds_mean>i).astype(np.int32)\n#         val_metric = pfbeta_np(valid_labels_mean, valid_argmax)\n#         val_acc = accuracy_score(valid_labels_mean, valid_argmax)\n#         val_f1 = f1_score(valid_labels_mean, valid_argmax)\n#         val_auc = roc_auc_score(valid_labels_mean, valid_argmax)\n#         cf = confusion_matrix(valid_labels_mean, valid_argmax)\n#         if val_metric> best_metric:\n#             best_metric = val_metric\n#             # best_metric_mean_at_epoch = val_metric\n#             best_threshold_mean = i\n#             best_auc = val_auc\n#             best_cf = cf\n#     state = {'state_dict': model.state_dict()}\n#     path = f'swa_{CFG.model_name}_fold_{fold}_model_{best_metric:.4f}_{best_threshold_mean:.3f}.pth'\n#     torch.save(state, path)\n    \n#     LOGGER.info(f\"Best metric at: {best_metric:.4f} {best_threshold_mean:.4f}  {best_auc:.4f}\")\n#     LOGGER.info(f\"Cf: {best_cf}\")\n#     return best_metric\n\n# study = optuna.create_study(direction='maximize', sampler = TPESampler(seed=666))\n# study.optimize(func=objective, n_trials=1)\n# study.best_params","metadata":{"execution":{"iopub.status.busy":"2023-02-15T07:31:57.527364Z","iopub.status.idle":"2023-02-15T07:31:57.528094Z","shell.execute_reply.started":"2023-02-15T07:31:57.527745Z","shell.execute_reply":"2023-02-15T07:31:57.527770Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set_seed(1)\n# out_file = 'swa_model_fold1_10.pth' \n# iteration = [\n#     '/kaggle/input/10folds/tf_efficientnetv2_b2_fold_1_model_epoch_3_0.5055_0.360.pth',\n#     '/kaggle/input/10folds/tf_efficientnetv2_b2_fold_1_model_epoch_5_0.4865_0.324.pth',\n# #     '/kaggle/input/10folds/tf_efficientnetv2_b2_fold_2_model_epoch_7_0.4848_0.266.pth'\n# ]\n\n# criterion = nn.CrossEntropyLoss().to(CFG.device)\n# best_metric = 0\n# torch.cuda.empty_cache()\n# def objective(trial):\n#     a1 = 0.696428379420678 \n#     a2 = 0.30357162057932197\n# #     a1 = 0.1689507073116359 \n# #     a2 = 0.47142151346976024 \n# #     a3 = 0.3596277792186039\n# #     a1 = trial.suggest_uniform('a1', 0.01, 0.99)\n# #     a2 = 1-a1\n# #     a2 = trial.suggest_uniform('a2', 0.1, 1-a1)\n# #     a3 = 1-a1-a2\n#     state_dict = None\n#     for i in iteration:\n#         f = i\n#         print(f)\n#         f = torch.load(f, map_location=lambda storage, loc: storage)\n#         if state_dict is None:\n#             print(\"none: \", i)\n#             state_dict = f['state_dict']\n#             key = list(f['state_dict'].keys())\n#             for k in key:\n#                 state_dict[k] = f['state_dict'][k]*a1\n#         elif i=='/kaggle/input/10folds/tf_efficientnetv2_b2_fold_1_model_epoch_5_0.4865_0.324.pth': \n#             print(\"hehe\", i)\n#             key = list(f['state_dict'].keys())\n#             for k in key:\n#                 state_dict[k] = state_dict[k] + a2*f['state_dict'][k]\n# #         elif i=='/kaggle/input/10folds/tf_efficientnetv2_b2_fold_2_model_epoch_7_0.4848_0.266.pth':\n# #             print(\"noob\", i)\n# #             key = list(f['state_dict'].keys())\n# #             for k in key:\n# #                 state_dict[k] = state_dict[k] + a3*f['state_dict'][k]\n#     print(a1, a2)\n#     # for k in key:\n#     #     state_dict[k] = state_dict[k] / len(iteration)\n#     print('')\n\n#     # print(out_file)\n#     torch.save({'state_dict': state_dict}, out_file)\n\n#     model = ModelOld(model_name=CFG.model_name).to(CFG.device)\n#     checkpoint = torch.load(\"/kaggle/input/10folds/tf_efficientnetv2_b2_fold_1_model_epoch_3_0.5055_0.360.pth\")\n#     model.load_state_dict(checkpoint['state_dict'])\n# #     model = nn.DataParallel(model)\n\n#     loss_valid, valid_preds = valid_fn_two(valid_loader, model, criterion, CFG.device)\n#     valid_preds = valid_preds[:, 1]\n#     valid_df['prediction_id'] = valid_df['patient_id'].astype(str) + '_' + valid_df['laterality'].astype(str)\n#     valid_preds = np.array(valid_preds).flatten()\n    \n#     valid_df['raw_pred'] = valid_preds\n#     LOGGER.info(f\"Valid loss:{loss_valid:.4f}\")\n#     grp_df = valid_df.groupby('prediction_id')['raw_pred', 'cancer'].mean()\n#     grp_df['cancer'] = grp_df['cancer'].astype(np.int)\n#     valid_labels_mean = grp_df['cancer'].values\n#     valid_preds_mean = grp_df['raw_pred'].values\n#     # print(valid_labels[:5], valid_preds_mean[:5])\n#     val_metric_mean = pfbeta(valid_labels_mean, valid_preds_mean)\n#     LOGGER.info(f\"Val metric mean prob: {val_metric_mean:.4f}\")\n#     best_metric_mean_at_epoch = 0\n#     best_metric = 0\n    \n#     best_threshold_mean = 0\n#     best_auc = 0\n#     best_cf = None\n#     for i in np.arange(0.001, 0.599, 0.001):\n#         valid_argmax = (valid_preds_mean>i).astype(np.int32)\n#         val_metric = pfbeta_np(valid_labels_mean, valid_argmax)\n#         val_acc = accuracy_score(valid_labels_mean, valid_argmax)\n#         val_f1 = f1_score(valid_labels_mean, valid_argmax)\n#         val_auc = roc_auc_score(valid_labels_mean, valid_argmax)\n#         cf = confusion_matrix(valid_labels_mean, valid_argmax)\n#         if val_metric> best_metric:\n#             best_metric = val_metric\n#             # best_metric_mean_at_epoch = val_metric\n#             best_threshold_mean = i\n#             best_auc = val_auc\n#             best_cf = cf\n#     state = {'state_dict': model.state_dict()}\n#     path = f'swa_{CFG.model_name}_fold_{fold}_model_{best_metric:.4f}_{best_threshold_mean:.4f}.pth'\n#     torch.save(state, path)\n    \n#     LOGGER.info(f\"Best metric at: {best_metric:.4f} {best_threshold_mean:.4f}  {best_auc:.4f}\")\n#     LOGGER.info(f\"Cf: {best_cf}\")\n#     return best_metric\n\n# study = optuna.create_study(direction='maximize', sampler = TPESampler(seed=666))\n# study.optimize(func=objective, n_trials=40)\n# study.best_params\n# # 0.696428379420678 0.30357162057932197 fold 1\n# # 0.5563409550491111 0.4436590449508889 fold 0\n# # 0.12634002523631388 0.8351954705276587 0.03846450423602743 0.5393 \n# # 0.583301614081906 0.3673525472043472 0.04934583871374687 fold 2 0.50\n# # 0.1689507073116359 0.47142151346976024 0.3596277792186039 fold 2 0.5055 0.5055 0.3670  0.7261","metadata":{"execution":{"iopub.status.busy":"2023-02-15T07:40:54.818305Z","iopub.execute_input":"2023-02-15T07:40:54.818653Z","iopub.status.idle":"2023-02-15T07:47:28.951050Z","shell.execute_reply.started":"2023-02-15T07:40:54.818623Z","shell.execute_reply":"2023-02-15T07:47:28.949330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set_seed(1)\nout_file = 'swa_model_fold6_10.pth' \niteration = [\n    '/kaggle/input/10folds/tf_efficientnetv2_b2_fold_6_model_epoch_6_0.5128_0.307.pth',\n    '/kaggle/input/10folds/tf_efficientnetv2_b2_fold_6_model_epoch_7_0.5385_0.423.pth',\n    '/kaggle/input/10folds/tf_efficientnetv2_b2_fold_6_model_epoch_9_0.5135_0.338.pth'\n]\n\ncriterion = nn.CrossEntropyLoss().to(CFG.device)\nbest_metric = 0\ntorch.cuda.empty_cache()\ndef objective(trial):\n#     a1 = 0.015006661988523864 \n#     a2 = 0.12003546043452194 \n#     a3 = 0.8649578775769542\n#     a1 = 0.020317850755860567 \n#     a2 = 0.1293785181217534 \n#     a3 = 0.850303631122386\n#     a1 = 0.12634002523631388\n#     a2 = 0.8351954705276587\n#     a3 = 0.03846450423602743\n    a1 = trial.suggest_uniform('a1', 0.01, 0.99)\n    a2 = trial.suggest_uniform('a2', 0.01, 1-a1)\n    a3 = 1-a1-a2\n    state_dict = None\n    for i in iteration:\n        f = i\n        print(f)\n        f = torch.load(f, map_location=lambda storage, loc: storage)\n        if state_dict is None:\n            print(\"none: \", i)\n            state_dict = f['state_dict']\n            key = list(f['state_dict'].keys())\n            for k in key:\n                state_dict[k] = f['state_dict'][k]*a1\n        elif i=='/kaggle/input/10folds/tf_efficientnetv2_b2_fold_6_model_epoch_7_0.5385_0.423.pth': \n            print(\"hehe\", i)\n            key = list(f['state_dict'].keys())\n            for k in key:\n                state_dict[k] = state_dict[k] + a2*f['state_dict'][k]\n        elif i=='/kaggle/input/10folds/tf_efficientnetv2_b2_fold_6_model_epoch_9_0.5135_0.338.pth':\n            print(\"noob\", i)\n            key = list(f['state_dict'].keys())\n            for k in key:\n                state_dict[k] = state_dict[k] + a3*f['state_dict'][k]\n    print(a1, a2, a3)\n    # for k in key:\n    #     state_dict[k] = state_dict[k] / len(iteration)\n    print('')\n\n    # print(out_file)\n    torch.save({'state_dict': state_dict}, out_file)\n\n    model = ModelOld(model_name=CFG.model_name).to(CFG.device)\n    checkpoint = torch.load(\"swa_model_fold6_10.pth\")\n    model.load_state_dict(checkpoint['state_dict'])\n#     model = nn.DataParallel(model)\n\n    loss_valid, valid_preds = valid_fn_two(valid_loader, model, criterion, CFG.device)\n    valid_preds = valid_preds[:, 1]\n    valid_df['prediction_id'] = valid_df['patient_id'].astype(str) + '_' + valid_df['laterality'].astype(str)\n    valid_preds = np.array(valid_preds).flatten()\n    \n    valid_df['raw_pred'] = valid_preds\n    LOGGER.info(f\"Valid loss:{loss_valid:.4f}\")\n    grp_df = valid_df.groupby('prediction_id')['raw_pred', 'cancer'].mean()\n    grp_df['cancer'] = grp_df['cancer'].astype(np.int)\n    valid_labels_mean = grp_df['cancer'].values\n    valid_preds_mean = grp_df['raw_pred'].values\n    # print(valid_labels[:5], valid_preds_mean[:5])\n    val_metric_mean = pfbeta(valid_labels_mean, valid_preds_mean)\n    LOGGER.info(f\"Val metric mean prob: {val_metric_mean:.4f}\")\n    best_metric_mean_at_epoch = 0\n    best_metric = 0\n    \n    best_threshold_mean = 0\n    best_auc = 0\n    best_cf = None\n    for i in np.arange(0.001, 0.599, 0.001):\n        valid_argmax = (valid_preds_mean>i).astype(np.int32)\n        val_metric = pfbeta_np(valid_labels_mean, valid_argmax)\n        val_acc = accuracy_score(valid_labels_mean, valid_argmax)\n        val_f1 = f1_score(valid_labels_mean, valid_argmax)\n        val_auc = roc_auc_score(valid_labels_mean, valid_argmax)\n        cf = confusion_matrix(valid_labels_mean, valid_argmax)\n        if val_metric> best_metric:\n            best_metric = val_metric\n            # best_metric_mean_at_epoch = val_metric\n            best_threshold_mean = i\n            best_auc = val_auc\n            best_cf = cf\n    state = {'state_dict': model.state_dict()}\n    path = f'swa_{CFG.model_name}_fold_{fold}_model_{best_metric:.4f}_{best_threshold_mean:.3f}.pth'\n    torch.save(state, path)\n    \n    LOGGER.info(f\"Best metric at: {best_metric:.4f} {best_threshold_mean:.4f}  {best_auc:.4f}\")\n    LOGGER.info(f\"Cf: {best_cf}\")\n    return best_metric\n\nstudy = optuna.create_study(direction='maximize', sampler = TPESampler(seed=666))\nstudy.optimize(func=objective, n_trials=40)\nstudy.best_params","metadata":{"execution":{"iopub.status.busy":"2023-02-15T07:49:17.640847Z","iopub.execute_input":"2023-02-15T07:49:17.641701Z","iopub.status.idle":"2023-02-15T07:54:02.658615Z","shell.execute_reply.started":"2023-02-15T07:49:17.641659Z","shell.execute_reply":"2023-02-15T07:54:02.653970Z"},"trusted":true},"execution_count":null,"outputs":[]}]}