{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-12-30T08:23:20.757433Z","iopub.status.busy":"2022-12-30T08:23:20.756596Z","iopub.status.idle":"2022-12-30T08:23:20.769292Z","shell.execute_reply":"2022-12-30T08:23:20.768338Z","shell.execute_reply.started":"2022-12-30T08:23:20.757395Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/tungnx/miniconda3/envs/zaloenv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import random\n","from glob import glob\n","import os, shutil\n","from tqdm import tqdm\n","tqdm.pandas()\n","import time\n","import copy\n","import joblib\n","from collections import defaultdict\n","import gc\n","from IPython import display as ipd\n","import math\n","# visualization\n","import cv2\n","from glob import glob\n","# Sklearn\n","from sklearn.model_selection import StratifiedKFold, KFold, StratifiedGroupKFold\n","from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, confusion_matrix, roc_curve\n","import timm\n","# PyTorch \n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from torch.utils.data import Dataset, DataLoader\n","from torch.cuda.amp import autocast, GradScaler\n","import torch.nn.functional as F\n","from torch.optim.swa_utils import AveragedModel, SWALR\n","from transformers import get_cosine_schedule_with_warmup\n","from collections import defaultdict\n","# import matplotlib.pyplot as plt\n","# Albumentations for augmentations\n","import albumentations as A\n","import albumentations\n","import albumentations as albu\n","from albumentations.pytorch import ToTensorV2\n","from datetime import datetime\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-12-30T08:23:20.771750Z","iopub.status.busy":"2022-12-30T08:23:20.771281Z","iopub.status.idle":"2022-12-30T08:23:20.782962Z","shell.execute_reply":"2022-12-30T08:23:20.782013Z","shell.execute_reply.started":"2022-12-30T08:23:20.771715Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda:0\n"]}],"source":["class CFG:\n","    seed = 1\n","    model_name = \"tf_efficientnetv2_b2\"\n","    train_bs = 16\n","    valid_bs = 64\n","    image_size = 1024\n","    epochs = 25\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","print(CFG.device)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-12-30T08:23:20.784720Z","iopub.status.busy":"2022-12-30T08:23:20.784325Z","iopub.status.idle":"2022-12-30T08:23:20.912644Z","shell.execute_reply":"2022-12-30T08:23:20.911668Z","shell.execute_reply.started":"2022-12-30T08:23:20.784684Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>site_id</th>\n","      <th>patient_id</th>\n","      <th>image_id</th>\n","      <th>laterality</th>\n","      <th>view</th>\n","      <th>age</th>\n","      <th>cancer</th>\n","      <th>biopsy</th>\n","      <th>invasive</th>\n","      <th>BIRADS</th>\n","      <th>implant</th>\n","      <th>density</th>\n","      <th>machine_id</th>\n","      <th>difficult_negative_case</th>\n","      <th>fold</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2</td>\n","      <td>10006</td>\n","      <td>462822612</td>\n","      <td>L</td>\n","      <td>CC</td>\n","      <td>61.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>29</td>\n","      <td>False</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>10006</td>\n","      <td>1459541791</td>\n","      <td>L</td>\n","      <td>MLO</td>\n","      <td>61.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>29</td>\n","      <td>False</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>10006</td>\n","      <td>1864590858</td>\n","      <td>R</td>\n","      <td>MLO</td>\n","      <td>61.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>29</td>\n","      <td>False</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2</td>\n","      <td>10006</td>\n","      <td>1874946579</td>\n","      <td>R</td>\n","      <td>CC</td>\n","      <td>61.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>29</td>\n","      <td>False</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2</td>\n","      <td>10011</td>\n","      <td>220375232</td>\n","      <td>L</td>\n","      <td>CC</td>\n","      <td>55.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>21</td>\n","      <td>True</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   site_id  patient_id    image_id laterality view   age  cancer  biopsy  \\\n","0        2       10006   462822612          L   CC  61.0       0       0   \n","1        2       10006  1459541791          L  MLO  61.0       0       0   \n","2        2       10006  1864590858          R  MLO  61.0       0       0   \n","3        2       10006  1874946579          R   CC  61.0       0       0   \n","4        2       10011   220375232          L   CC  55.0       0       0   \n","\n","   invasive  BIRADS  implant density  machine_id  difficult_negative_case  \\\n","0         0     NaN        0     NaN          29                    False   \n","1         0     NaN        0     NaN          29                    False   \n","2         0     NaN        0     NaN          29                    False   \n","3         0     NaN        0     NaN          29                    False   \n","4         0     0.0        0     NaN          21                     True   \n","\n","   fold  \n","0     1  \n","1     1  \n","2     1  \n","3     1  \n","4     0  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv(\"train_5folds.csv\")\n","df.head()"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["55864\n"]}],"source":["is_hol = df['cancer'] == 1\n","df_try = df[is_hol]\n","df1 = df.append([df_try]*1,ignore_index=True)\n","print(len(df1))"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Date :01/23/2023, 08:29:04\n"]}],"source":["def init_logger(log_file='train1.log'):\n","    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n","    logger = getLogger(__name__)\n","    logger.setLevel(INFO)\n","    handler1 = StreamHandler()\n","    handler1.setFormatter(Formatter(\"%(message)s\"))\n","    handler2 = FileHandler(filename=log_file)\n","    handler2.setFormatter(Formatter(\"%(message)s\"))\n","    logger.addHandler(handler1)\n","    logger.addHandler(handler2)\n","    return logger\n","\n","LOGGER = init_logger()\n","now = datetime.now()\n","datetime_now = now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n","LOGGER.info(f\"Date :{datetime_now}\")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["train transformCompose([\n","  VerticalFlip(always_apply=False, p=0.5),\n","  ColorJitter(always_apply=False, p=0.5, brightness=[0.8, 1.2], contrast=[0.8, 1.2], saturation=[0.8, 1.2], hue=[-0.2, 0.2]),\n","  ShiftScaleRotate(always_apply=False, p=0.5, shift_limit_x=(-0.0625, 0.0625), shift_limit_y=(-0.0625, 0.0625), scale_limit=(-0.050000000000000044, 0.050000000000000044), rotate_limit=(-10, 10), interpolation=1, border_mode=4, value=None, mask_value=None, rotate_method='largest_box'),\n","  HorizontalFlip(always_apply=False, p=0.5),\n","  Cutout(always_apply=False, p=0.5, num_holes=5, max_h_size=102, max_w_size=102),\n","  ToTensorV2(always_apply=True, p=1.0, transpose_mask=False),\n","], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={})\n"]}],"source":["from albumentations import DualTransform\n","image_size = 1024\n","def isotropically_resize_image(img, size, interpolation_down=cv2.INTER_AREA, interpolation_up=cv2.INTER_CUBIC):\n","    h, w = img.shape[:2]\n","    if max(w, h) == size:\n","        return img\n","    if w > h:\n","        scale = size / w\n","        h = h * scale\n","        w = size\n","    else:\n","        scale = size / h\n","        w = w * scale\n","        h = size\n","    interpolation = interpolation_up if scale > 1 else interpolation_down\n","    resized = cv2.resize(img, (int(w), int(h)), interpolation=interpolation)\n","    return resized\n","\n","\n","class IsotropicResize(DualTransform):\n","    def __init__(self, max_side, interpolation_down=cv2.INTER_AREA, interpolation_up=cv2.INTER_CUBIC,\n","                 always_apply=False, p=1):\n","        super(IsotropicResize, self).__init__(always_apply, p)\n","        self.max_side = max_side\n","        self.interpolation_down = interpolation_down\n","        self.interpolation_up = interpolation_up\n","\n","    def apply(self, img, interpolation_down=cv2.INTER_AREA, interpolation_up=cv2.INTER_CUBIC, **params):\n","        return isotropically_resize_image(img, size=self.max_side, interpolation_down=interpolation_down,\n","                                          interpolation_up=interpolation_up)\n","\n","    def apply_to_mask(self, img, **params):\n","        return self.apply(img, interpolation_down=cv2.INTER_NEAREST, interpolation_up=cv2.INTER_NEAREST, **params)\n","\n","    def get_transform_init_args_names(self):\n","        return (\"max_side\", \"interpolation_down\", \"interpolation_up\")\n","    \n","data_transforms = {\n","    \"train\": A.Compose([\n","        # A.Resize(image_size, image_size),\n","        # IsotropicResize(max_side = image_size),\n","        # A.PadIfNeeded(min_height=image_size, min_width=image_size, border_mode=cv2.BORDER_CONSTANT),\n","        # A.RandomBrightnessContrast(),\n","        # A.VerticalFlip(p=0.5),   \n","        # A.ColorJitter(),\n","        # A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.05, rotate_limit=10, p=0.5),\n","        # A.HorizontalFlip(p=0.5),\n","        # A.Cutout(max_h_size=int(image_size * 0.1), max_w_size=int(image_size * 0.1), num_holes=5, p=0.5),\n","        A.VerticalFlip(p=0.5),   \n","        A.ColorJitter(),\n","        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.05, rotate_limit=10, p=0.5),\n","        A.HorizontalFlip(p=0.5),\n","        A.Cutout(max_h_size=int(image_size * 0.1), max_w_size=int(image_size * 0.1), num_holes=5, p=0.5),\n","        # A.OneOf([ \n","        # A.OpticalDistortion(distort_limit=1.0), \n","        # A.GridDistortion(num_steps=5, distort_limit=1.),\n","        # A.ElasticTransform(alpha=3), ], p=0.2),\n","        # A.OneOf([\n","            # A.GaussNoise(var_limit=[10, 50]),\n","            # A.GaussianBlur(),\n","            # A.MotionBlur(),\n","            # A.MedianBlur(), ], p=0.2),\n","        # A.OneOf([\n","        #     A.GridDistortion(num_steps=5, distort_limit=0.05, p=1.0),\n","        #     A.OpticalDistortion(distort_limit=0.05, shift_limit=0.05, p=1.0),\n","        #     A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=1.0)\n","        # ], p=0.25),\n","        # A.CoarseDropout(max_holes=8, max_height=image_size//20, max_width=image_size//20,\n","        #                  min_holes=5, fill_value=0, mask_fill_value=0, p=0.5),\n","        # A.Normalize(mean=0, std=1),\n","        ToTensorV2(),], p=1.0),\n","    \n","    \"valid\": A.Compose([\n","        # IsotropicResize(max_side =image_size),\n","        # A.PadIfNeeded(min_height=image_size, min_width=image_size, border_mode=cv2.BORDER_CONSTANT),\n","        # A.Normalize(mean=0, std=1),\n","        # A.Resize(image_size, image_size),\n","        ToTensorV2(),\n","        ], p=1.0)\n","}\n","\n","LOGGER.info(f\"train transform{data_transforms['train']}\")\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-12-30T08:23:20.915927Z","iopub.status.busy":"2022-12-30T08:23:20.915346Z","iopub.status.idle":"2022-12-30T08:23:20.931477Z","shell.execute_reply":"2022-12-30T08:23:20.930433Z","shell.execute_reply.started":"2022-12-30T08:23:20.915890Z"},"trusted":true},"outputs":[],"source":["# from albumentations import DualTransform\n","# image_size = 1024\n","# def isotropically_resize_image(img, size, interpolation_down=cv2.INTER_AREA, interpolation_up=cv2.INTER_CUBIC):\n","#     h, w = img.shape[:2]\n","#     if max(w, h) == size:\n","#         return img\n","#     if w > h:\n","#         scale = size / w\n","#         h = h * scale\n","#         w = size\n","#     else:\n","#         scale = size / h\n","#         w = w * scale\n","#         h = size\n","#     interpolation = interpolation_up if scale > 1 else interpolation_down\n","#     resized = cv2.resize(img, (int(w), int(h)), interpolation=interpolation)\n","#     return resized\n","\n","\n","# class IsotropicResize(DualTransform):\n","#     def __init__(self, max_side, interpolation_down=cv2.INTER_AREA, interpolation_up=cv2.INTER_CUBIC,\n","#                  always_apply=False, p=1):\n","#         super(IsotropicResize, self).__init__(always_apply, p)\n","#         self.max_side = max_side\n","#         self.interpolation_down = interpolation_down\n","#         self.interpolation_up = interpolation_up\n","\n","#     def apply(self, img, interpolation_down=cv2.INTER_AREA, interpolation_up=cv2.INTER_CUBIC, **params):\n","#         return isotropically_resize_image(img, size=self.max_side, interpolation_down=interpolation_down,\n","#                                           interpolation_up=interpolation_up)\n","\n","#     def apply_to_mask(self, img, **params):\n","#         return self.apply(img, interpolation_down=cv2.INTER_NEAREST, interpolation_up=cv2.INTER_NEAREST, **params)\n","\n","#     def get_transform_init_args_names(self):\n","#         return (\"max_side\", \"interpolation_down\", \"interpolation_up\")\n","    \n","# data_transforms = {\n","#     \"train\": A.Compose([\n","# #         A.Resize(image_size, image_size),\n","#         # IsotropicResize(max_side = image_size),\n","#        A.PadIfNeeded(min_width=image_size, border_mode=cv2.BORDER_CONSTANT),\n","#         albumentations.HorizontalFlip(p=0.5),\n","#         albumentations.VerticalFlip(p=0.5),\n","#         # albumentations.RandomBrightness(limit=0.2, p=0.75),\n","#         # albumentations.RandomContrast(limit=0.2, p=0.75),\n","\n","#         # albumentations.OneOf([\n","#         #     albumentations.OpticalDistortion(distort_limit=1.),\n","#         #     albumentations.GridDistortion(num_steps=5, distort_limit=1.),\n","#         # ], p=0.75),\n","\n","#         # albumentations.HueSaturationValue(hue_shift_limit=40, sat_shift_limit=40, val_shift_limit=0, p=0.75),\n","#         albumentations.ShiftScaleRotate(p = 0.5),\n","#         A.Cutout(always_apply=False, p=0.5, num_holes=5, max_h_size=image_size//10, max_w_size=image_size//10),\n","#         # A.RandomBrightnessContrast(),\n","#         # A.VerticalFlip(p=0.5),   \n","#         A.ColorJitter(p = 0.7),\n","#         # A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.05, rotate_limit=10, p=0.5),\n","#         # A.HorizontalFlip(p=0.5),\n","#         # A.Cutout(max_h_size=int(image_size * 0.1), max_w_size=int(image_size * 0.1), num_holes=5, p=0.5),\n","#         # albumentations.RandomBrightness(limit=0.2, p=0.75),\n","#         # albumentations.RandomContrast(limit=0.2, p=0.75),\n","\n","#         # albumentations.OneOf([\n","#         #     albumentations.OpticalDistortion(distort_limit=1.),\n","#         #     albumentations.GridDistortion(num_steps=5, distort_limit=1.),\n","#         # ], p=0.75),\n","\n","#         # albumentations.HueSaturationValue(hue_shift_limit=40, sat_shift_limit=40, val_shift_limit=0, p=0.75),\n","#         # albumentations.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.3, rotate_limit=30, border_mode=0, p=0.75),\n","#         # A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=10, val_shift_limit=10, p=0.7),\n","#         # A.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2, 0.2), p=0.7),\n","#         # A.CLAHE(p=0.5),\n","#         # albumentations.OneOf([\n","#         # albumentations.OpticalDistortion(distort_limit=1.),\n","#         # albumentations.GridDistortion(num_steps=5, distort_limit=1.),\n","#         # ], p=0.75),\n","#         # A.OneOf([\n","#         # A.GaussianBlur(),\n","#         # A.MotionBlur(),\n","#         # A.MedianBlur(), ], p=0.5),\n","#         # A.IAASharpen(p = 0.2),\n","#         # A.JpegCompression(p=0.2),\n","#         # A.Downscale(scale_min=0.5, scale_max=0.75),\n","#         # A.OneOf([ A.JpegCompression(), A.Downscale(scale_min=0.1, scale_max=0.15), ], p=0.2), \n","#         # A.IAAPiecewiseAffine(),\n","# #         A.OneOf([ \n","# #         A.OpticalDistortion(distort_limit=1.0), \n","# #         A.GridDistortion(num_steps=5, distort_limit=1.),\n","# #         A.ElasticTransform(alpha=3), ], p=0.2),\n","# #         A.OneOf([\n","# #             A.GaussNoise(var_limit=[10, 50]),\n","# #             A.GaussianBlur(),\n","# #             A.MotionBlur(),\n","# #             A.MedianBlur(), ], p=0.2),\n","#         # A.OneOf([\n","#         #     A.GridDistortion(num_steps=5, distort_limit=0.05, p=1.0),\n","#         #     A.OpticalDistortion(distort_limit=0.05, shift_limit=0.05, p=1.0),\n","#         #     A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=1.0)\n","#         # ], p=0.25),\n","#         # A.CoarseDropout(max_holes=8, max_height=image_size//20, max_width=image_size//20,\n","#         #                  min_holes=5, fill_value=0, mask_fill_value=0, p=0.5),\n","#         # A.Normalize(mean=0, std=1),\n","#         ToTensorV2(),], p=1.0),\n","    \n","#     \"valid\": A.Compose([\n","#         # IsotropicResize(max_side = image_size),\n","#         A.PadIfNeeded(min_height=image_size, min_width=image_size, border_mode=cv2.BORDER_CONSTANT),\n","#         # A.Normalize(mean=0, std=1),\n","# #         A.Resize(image_size, image_size),\n","#         ToTensorV2(),\n","#         ], p=1.0)\n","# }\n","\n","# LOGGER.info(f\"train transform{data_transforms['train']}\")\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-12-30T08:23:20.934703Z","iopub.status.busy":"2022-12-30T08:23:20.933649Z","iopub.status.idle":"2022-12-30T08:23:21.010802Z","shell.execute_reply":"2022-12-30T08:23:21.009678Z","shell.execute_reply.started":"2022-12-30T08:23:20.934663Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([3, 1344, 840]) tensor(0)\n","tensor(255.)\n"]}],"source":["def pad(array, target_shape):\n","    return np.pad(\n","        array,\n","        [(0, target_shape[i] - array.shape[i]) for i in range(len(array.shape))],\n","        \"constant\",\n","    )\n","    \n","def load_img(img_path):\n","    image = cv2.imread(img_path)\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    # image = pad(image, (1024, 800, 3))\n","        # img = img.reshape((*resize))\n","    return image\n","#     image = cv2.resize(image, (320, 320), cv2.INTER_NEAREST)\n","#     image = image.astype(np.float32)\n","#     mx = np.max(image)\n","#     if mx:\n","#         image/=mx\n","#     image = image /255.0\n","    \n","    return image\n","class BreastDataset(Dataset):\n","    def __init__(self, df, transforms=None):\n","        self.df = df\n","        self.transforms = transforms\n","        \n","    def __getitem__(self, index):\n","        row = self.df.iloc[index]\n","        img_path = f\"flip/{row.patient_id}_{row.image_id}.png\"\n","        img = load_img(img_path)\n","        label = row['cancer']\n","        # img = np.transpose(img, (2, 0, 1))\n","        data = self.transforms(image=img)\n","        img  = data['image']\n","        # img = img/255.0\n","        return torch.tensor(img).float(), torch.tensor(label).long()\n","        \n","    def __len__(self):\n","        return len(self.df)\n","    \n","fold0 = df[df['fold']==0]\n","train_dataset = BreastDataset(fold0, transforms = data_transforms['train'])\n","image, label = train_dataset[0]\n","print(image.shape, label)\n","print(image.max())"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["\n","# from pylab import rcParams\n","\n","# f, axarr = plt.subplots(1,15, figsize = (20, 20))\n","# imgs = []\n","# for p in range(15):\n","#     img, label = train_dataset[p]\n","#     img = img.transpose(0, 1).transpose(1,2).cpu().numpy()\n","#     img = img.astype(np.uint8)\n","#     imgs.append(img)\n","#     axarr[p].imshow(img)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-12-30T08:23:21.012682Z","iopub.status.busy":"2022-12-30T08:23:21.012267Z","iopub.status.idle":"2022-12-30T08:23:21.020148Z","shell.execute_reply":"2022-12-30T08:23:21.019023Z","shell.execute_reply.started":"2022-12-30T08:23:21.012626Z"},"trusted":true},"outputs":[],"source":["class Model(nn.Module):\n","    def __init__(self, model_name):\n","        super().__init__()\n","        # ,drop_rate = 0.3, drop_path_rate = 0.2\n","        self.backbone = timm.create_model(model_name, pretrained=True,drop_rate = 0.3, drop_path_rate = 0.2)\n","        self.fc = nn.Linear(self.backbone.classifier.in_features,2)\n","        self.backbone.classifier = nn.Identity()\n","        self.dropout = nn.Dropout(0.5)\n","    def forward(self, x):\n","        x = self.backbone(x)\n","        x = self.fc(self.dropout(x))\n","        return x"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-12-30T08:23:21.022923Z","iopub.status.busy":"2022-12-30T08:23:21.022147Z","iopub.status.idle":"2022-12-30T08:23:21.032555Z","shell.execute_reply":"2022-12-30T08:23:21.031346Z","shell.execute_reply.started":"2022-12-30T08:23:21.022887Z"},"trusted":true},"outputs":[],"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim.optimizer import Optimizer, required\n","import math\n","\n","class AdamP(Optimizer):\n","    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n","                 weight_decay=0, delta=0.1, wd_ratio=0.1, nesterov=False):\n","        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,\n","                        delta=delta, wd_ratio=wd_ratio, nesterov=nesterov)\n","        super(AdamP, self).__init__(params, defaults)\n","\n","    def _channel_view(self, x):\n","        return x.view(x.size(0), -1)\n","\n","    def _layer_view(self, x):\n","        return x.view(1, -1)\n","\n","    def _cosine_similarity(self, x, y, eps, view_func):\n","        x = view_func(x)\n","        y = view_func(y)\n","\n","        return F.cosine_similarity(x, y, dim=1, eps=eps).abs_()\n","\n","    def _projection(self, p, grad, perturb, delta, wd_ratio, eps):\n","        wd = 1\n","        expand_size = [-1] + [1] * (len(p.shape) - 1)\n","        for view_func in [self._channel_view, self._layer_view]:\n","\n","            cosine_sim = self._cosine_similarity(grad, p.data, eps, view_func)\n","\n","            if cosine_sim.max() < delta / math.sqrt(view_func(p.data).size(1)):\n","                p_n = p.data / view_func(p.data).norm(dim=1).view(expand_size).add_(eps)\n","                perturb -= p_n * view_func(p_n * perturb).sum(dim=1).view(expand_size)\n","                wd = wd_ratio\n","\n","                return perturb, wd\n","\n","        return perturb, wd\n","\n","    def step(self, closure=None):\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","\n","                grad = p.grad.data\n","                beta1, beta2 = group['betas']\n","                nesterov = group['nesterov']\n","\n","                state = self.state[p]\n","\n","                # State initialization\n","                if len(state) == 0:\n","                    state['step'] = 0\n","                    state['exp_avg'] = torch.zeros_like(p.data)\n","                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n","\n","                # Adam\n","                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n","\n","                state['step'] += 1\n","                bias_correction1 = 1 - beta1 ** state['step']\n","                bias_correction2 = 1 - beta2 ** state['step']\n","\n","                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n","                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n","\n","                denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n","                step_size = group['lr'] / bias_correction1\n","\n","                if nesterov:\n","                    perturb = (beta1 * exp_avg + (1 - beta1) * grad) / denom\n","                else:\n","                    perturb = exp_avg / denom\n","\n","                # Projection\n","                wd_ratio = 1\n","                if len(p.shape) > 1:\n","                    perturb, wd_ratio = self._projection(p, grad, perturb, group['delta'], group['wd_ratio'], group['eps'])\n","\n","                # Weight decay\n","                if group['weight_decay'] > 0:\n","                    p.data.mul_(1 - group['lr'] * group['weight_decay'] * wd_ratio)\n","\n","                # Step\n","                p.data.add_(perturb, alpha=-step_size)\n","\n","        return loss\n","\n","class SGDP(Optimizer):\n","    def __init__(self, params, lr=required, momentum=0, dampening=0,\n","                 weight_decay=0, nesterov=False, eps=1e-8, delta=0.1, wd_ratio=0.1):\n","        defaults = dict(lr=lr, momentum=momentum, dampening=dampening, weight_decay=weight_decay,\n","                        nesterov=nesterov, eps=eps, delta=delta, wd_ratio=wd_ratio)\n","        super(SGDP, self).__init__(params, defaults)\n","\n","    def _channel_view(self, x):\n","        return x.view(x.size(0), -1)\n","\n","    def _layer_view(self, x):\n","        return x.view(1, -1)\n","\n","    def _cosine_similarity(self, x, y, eps, view_func):\n","        x = view_func(x)\n","        y = view_func(y)\n","\n","        return F.cosine_similarity(x, y, dim=1, eps=eps).abs_()\n","\n","    def _projection(self, p, grad, perturb, delta, wd_ratio, eps):\n","        wd = 1\n","        expand_size = [-1] + [1] * (len(p.shape) - 1)\n","        for view_func in [self._channel_view, self._layer_view]:\n","\n","            cosine_sim = self._cosine_similarity(grad, p.data, eps, view_func)\n","\n","            if cosine_sim.max() < delta / math.sqrt(view_func(p.data).size(1)):\n","                p_n = p.data / view_func(p.data).norm(dim=1).view(expand_size).add_(eps)\n","                perturb -= p_n * view_func(p_n * perturb).sum(dim=1).view(expand_size)\n","                wd = wd_ratio\n","\n","                return perturb, wd\n","\n","        return perturb, wd\n","\n","    def step(self, closure=None):\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","            momentum = group['momentum']\n","            dampening = group['dampening']\n","            nesterov = group['nesterov']\n","\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","                grad = p.grad.data\n","                state = self.state[p]\n","\n","                # State initialization\n","                if len(state) == 0:\n","                    state['momentum'] = torch.zeros_like(p.data)\n","\n","                # SGD\n","                buf = state['momentum']\n","                buf.mul_(momentum).add_(grad, alpha=1 - dampening)\n","                if nesterov:\n","                    d_p = grad + momentum * buf\n","                else:\n","                    d_p = buf\n","\n","                # Projection\n","                wd_ratio = 1\n","                if len(p.shape) > 1:\n","                    d_p, wd_ratio = self._projection(p, grad, d_p, group['delta'], group['wd_ratio'], group['eps'])\n","\n","                # Weight decay\n","                if group['weight_decay'] > 0:\n","                    p.data.mul_(1 - group['lr'] * group['weight_decay'] * wd_ratio / (1-momentum))\n","\n","                # Step\n","                p.data.add_(d_p, alpha=-group['lr'])\n","\n","        return loss\n","\n","class SAM(torch.optim.Optimizer):\n","    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\n","        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n","\n","        defaults = dict(rho=rho, **kwargs)\n","        super(SAM, self).__init__(params, defaults)\n","\n","        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n","        self.param_groups = self.base_optimizer.param_groups\n","\n","    @torch.no_grad()\n","    def first_step(self, zero_grad=False):\n","        grad_norm = self._grad_norm()\n","        for group in self.param_groups:\n","            scale = group[\"rho\"] / (grad_norm + 1e-12)\n","\n","            for p in group[\"params\"]:\n","                if p.grad is None: continue\n","                e_w = p.grad * scale.to(p)\n","                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n","                self.state[p][\"e_w\"] = e_w\n","\n","        if zero_grad: self.zero_grad()\n","\n","    @torch.no_grad()\n","    def second_step(self, zero_grad=False):\n","        for group in self.param_groups:\n","            for p in group[\"params\"]:\n","                if p.grad is None: continue\n","                p.sub_(self.state[p][\"e_w\"])  # get back to \"w\" from \"w + e(w)\"\n","\n","        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n","\n","        if zero_grad: self.zero_grad()\n","\n","    def step(self, closure=None):\n","        raise NotImplementedError(\"SAM doesn't work like the other optimizers, you should first call `first_step` and the `second_step`; see the documentation for more info.\")\n","\n","    def _grad_norm(self):\n","        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n","        norm = torch.norm(\n","                    torch.stack([\n","                        p.grad.norm(p=2).to(shared_device)\n","                        for group in self.param_groups for p in group[\"params\"]\n","                        if p.grad is not None\n","                    ]),\n","                    p=2\n","               )\n","        return norm"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["from torch.optim.lr_scheduler import _LRScheduler\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","\n","\n","class GradualWarmupScheduler(_LRScheduler):\n","    \"\"\" Gradually warm-up(increasing) learning rate in optimizer.\n","    Proposed in 'Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour'.\n","    Args:\n","        optimizer (Optimizer): Wrapped optimizer.\n","        multiplier: target learning rate = base lr * multiplier if multiplier > 1.0. if multiplier = 1.0, lr starts from 0 and ends up with the base_lr.\n","        total_epoch: target learning rate is reached at total_epoch, gradually\n","        after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)\n","    \"\"\"\n","\n","    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n","        self.multiplier = multiplier\n","        if self.multiplier < 1.:\n","            raise ValueError('multiplier should be greater thant or equal to 1.')\n","        self.total_epoch = total_epoch\n","        self.after_scheduler = after_scheduler\n","        self.finished = False\n","        super(GradualWarmupScheduler, self).__init__(optimizer)\n","\n","    def get_lr(self):\n","        if self.last_epoch > self.total_epoch:\n","            if self.after_scheduler:\n","                if not self.finished:\n","                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n","                    self.finished = True\n","                return self.after_scheduler.get_last_lr()\n","            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n","\n","        if self.multiplier == 1.0:\n","            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n","        else:\n","            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n","\n","    def step_ReduceLROnPlateau(self, metrics, epoch=None):\n","        if epoch is None:\n","            epoch = self.last_epoch + 1\n","        self.last_epoch = epoch if epoch != 0 else 1  # ReduceLROnPlateau is called at the end of epoch, whereas others are called at beginning\n","        if self.last_epoch <= self.total_epoch:\n","            warmup_lr = [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n","            for param_group, lr in zip(self.optimizer.param_groups, warmup_lr):\n","                param_group['lr'] = lr\n","        else:\n","            if epoch is None:\n","                self.after_scheduler.step(metrics, None)\n","            else:\n","                self.after_scheduler.step(metrics, epoch - self.total_epoch)\n","\n","    def step(self, epoch=None, metrics=None):\n","        if type(self.after_scheduler) != ReduceLROnPlateau:\n","            if self.finished and self.after_scheduler:\n","                if epoch is None:\n","                    self.after_scheduler.step(None)\n","                else:\n","                    self.after_scheduler.step(epoch - self.total_epoch)\n","                self._last_lr = self.after_scheduler.get_last_lr()\n","            else:\n","                return super(GradualWarmupScheduler, self).step(epoch)\n","        else:\n","            self.step_ReduceLROnPlateau(metrics, epoch)\n","\n","class GradualWarmupSchedulerV2(GradualWarmupScheduler):\n","    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n","        super(GradualWarmupSchedulerV2, self).__init__(optimizer, multiplier, total_epoch, after_scheduler)\n","    def get_lr(self):\n","        if self.last_epoch > self.total_epoch:\n","            if self.after_scheduler:\n","                if not self.finished:\n","                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n","                    self.finished = True\n","                return self.after_scheduler.get_lr()\n","            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n","        if self.multiplier == 1.0:\n","            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n","        else:\n","            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n","\n","class Lookahead(optim.Optimizer):\n","    def __init__(self, base_optimizer, alpha=0.5, k=6):\n","        if not 0.0 <= alpha <= 1.0:\n","            raise ValueError(f'Invalid slow update rate: {alpha}')\n","        if not 1 <= k:\n","            raise ValueError(f'Invalid lookahead steps: {k}')\n","        defaults = dict(lookahead_alpha=alpha, lookahead_k=k, lookahead_step=0)\n","        self.base_optimizer = base_optimizer\n","        self.param_groups = self.base_optimizer.param_groups\n","        self.defaults = base_optimizer.defaults\n","        self.defaults.update(defaults)\n","        self.state = defaultdict(dict)\n","        # manually add our defaults to the param groups\n","        for name, default in defaults.items():\n","            for group in self.param_groups:\n","                group.setdefault(name, default)\n","\n","    def update_slow(self, group):\n","        for fast_p in group[\"params\"]:\n","            if fast_p.grad is None:\n","                continue\n","            param_state = self.state[fast_p]\n","            if 'slow_buffer' not in param_state:\n","                param_state['slow_buffer'] = torch.empty_like(fast_p.data)\n","                param_state['slow_buffer'].copy_(fast_p.data)\n","            slow = param_state['slow_buffer']\n","            slow.add_(group['lookahead_alpha'], fast_p.data - slow)\n","            fast_p.data.copy_(slow)\n","\n","    def sync_lookahead(self):\n","        for group in self.param_groups:\n","            self.update_slow(group)\n","\n","    def step(self, closure=None):\n","        #assert id(self.param_groups) == id(self.base_optimizer.param_groups)\n","        loss = self.base_optimizer.step(closure)\n","        for group in self.param_groups:\n","            group['lookahead_step'] += 1\n","            if group['lookahead_step'] % group['lookahead_k'] == 0:\n","                self.update_slow(group)\n","        return loss\n","\n","    def state_dict(self):\n","        fast_state_dict = self.base_optimizer.state_dict()\n","        slow_state = {\n","            (id(k) if isinstance(k, torch.Tensor) else k): v\n","            for k, v in self.state.items()\n","        }\n","        fast_state = fast_state_dict['state']\n","        param_groups = fast_state_dict['param_groups']\n","        return {\n","            'state': fast_state,\n","            'slow_state': slow_state,\n","            'param_groups': param_groups,\n","        }\n","\n","    def load_state_dict(self, state_dict):\n","        fast_state_dict = {\n","            'state': state_dict['state'],\n","            'param_groups': state_dict['param_groups'],\n","        }\n","        self.base_optimizer.load_state_dict(fast_state_dict)\n","\n","        # We want to restore the slow state, but share param_groups reference\n","        # with base_optimizer. This is a bit redundant but least code\n","        slow_state_new = False\n","        if 'slow_state' not in state_dict:\n","            print('Loading state_dict from optimizer without Lookahead applied.')\n","            state_dict['slow_state'] = defaultdict(dict)\n","            slow_state_new = True\n","        slow_state_dict = {\n","            'state': state_dict['slow_state'],\n","            'param_groups': state_dict['param_groups'],  # this is pointless but saves code\n","        }\n","        super(Lookahead, self).load_state_dict(slow_state_dict)\n","        self.param_groups = self.base_optimizer.param_groups  # make both ref same container\n","        if slow_state_new:\n","            # reapply defaults to catch missing lookahead specific ones\n","            for name, default in self.defaults.items():\n","                for group in self.param_groups:\n","                    group.setdefault(name, default)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-12-30T08:23:21.036233Z","iopub.status.busy":"2022-12-30T08:23:21.035928Z","iopub.status.idle":"2022-12-30T08:23:21.059619Z","shell.execute_reply":"2022-12-30T08:23:21.058523Z","shell.execute_reply.started":"2022-12-30T08:23:21.036197Z"},"trusted":true},"outputs":[],"source":["\n","def train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device):\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","    batch_time = AverageMeter()\n","    data_time = AverageMeter()\n","    losses = AverageMeter()\n","    # switch to train mode\n","    model.train()\n","    start = end = time.time()\n","    truth = []\n","    pred = []\n","    global_step = 0\n","    scaler = GradScaler()\n","    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc='Train')\n","    for step, (images, labels) in pbar:\n","        optimizer.zero_grad()\n","        data_time.update(time.time() - end)\n","        images = images.to(device)\n","        \n","        \n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","        with autocast():\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            # loss.backward()\n","            # optimizer.first_step(zero_grad=True)\n","            # criterion(model(images), labels).backward()\n","            # optimizer.second_step(zero_grad=True)\n","            # record loss\n","        losses.update(loss.item(), batch_size)\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        # global_step += 1\n","        scheduler.step()\n","            # measure elapsed time\n","        batch_time.update(time.time() - end)\n","        end = time.time()\n","#         if step % 100 == 0 or step == (len(train_loader)-1):\n","#             print('Epoch: [{0}][{1}/{2}] '\n","#                       'Data {data_time.val:.6f} ({data_time.avg:.6f}) '\n","#                       'Elapsed {remain:s} '\n","#                       'Loss: {loss.val:.6f}({loss.avg:.6f}) '\n","#                       'LR: {lr:.6f}  '\n","#                       .format(\n","#                        epoch+1, step, len(train_loader), batch_time=batch_time,\n","#                        data_time=data_time, loss=losses,\n","#                        remain=timeSince(start, float(step+1)/len(train_loader)),\n","#                        lr=scheduler.get_lr()[0],\n","#                        ))\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n","        current_lr = optimizer.param_groups[0]['lr']\n","        pbar.set_postfix(train_loss=f'{losses.avg:0.4f}',\n","                        lr=f'{current_lr:0.8f}',\n","                        gpu_mem=f'{mem:0.2f} GB')\n","\n","    return losses.avg\n","\n","def valid_fn_no_sigmoid(val_dataloader, model, criterion, device):\n","    losses = AverageMeter()\n","    model.eval()\n","    truth = []\n","    preds = []\n","    valid_labels = []\n","    start = end = time.time()\n","    pbar = tqdm(enumerate(val_dataloader), total=len(val_dataloader), desc='Val')\n","    for step, (images, labels) in pbar:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","        with torch.no_grad():\n","            outputs = model(images)\n","        valid_labels.append(labels.cpu().numpy())\n","        loss = criterion(outputs, labels)\n","#         loss = bi_tempered_logistic_loss(outputs, labels, t1=0.8, t2 = 1.4)\n","        losses.update(loss.item(), batch_size)\n","#         print(outputs)\n","        preds.append((outputs).to('cpu').numpy())\n","        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n","        pbar.set_postfix(eval_loss=f'{losses.avg:0.4f}',\n","                        gpu_mem=f'{mem:0.2f} GB')\n","    predictions = np.concatenate(preds)\n","    valid_labels = np.concatenate(valid_labels)\n","    return losses.avg, predictions, valid_labels\n","\n","\n","def valid_fn(val_dataloader, model, criterion, device):\n","    losses = AverageMeter()\n","    model.eval()\n","    truth = []\n","    preds = []\n","    valid_labels = []\n","    start = end = time.time()\n","    pbar = tqdm(enumerate(val_dataloader), total=len(val_dataloader), desc='Val')\n","    for step, (images, labels) in pbar:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","        with torch.no_grad():\n","            outputs = model(images)\n","        valid_labels.append(labels.cpu().numpy())\n","        loss = criterion(outputs, labels)\n","#         loss = bi_tempered_logistic_loss(outputs, labels, t1=0.8, t2 = 1.4)\n","        losses.update(loss.item(), batch_size)\n","#         print(outputs)\n","        preds.append(torch.sigmoid(outputs).to('cpu').numpy())\n","        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n","        pbar.set_postfix(eval_loss=f'{losses.avg:0.4f}',\n","                        gpu_mem=f'{mem:0.2f} GB')\n","    predictions = np.concatenate(preds)\n","    valid_labels = np.concatenate(valid_labels)\n","    return losses.avg, predictions, valid_labels\n","def valid_fn_two(val_dataloader, model, criterion, device):\n","    losses = AverageMeter()\n","    model.eval()\n","    truth = []\n","    preds = []\n","    valid_labels = []\n","    start = end = time.time()\n","    pbar = tqdm(enumerate(val_dataloader), total=len(val_dataloader), desc='Val')\n","    for step, (images, labels) in pbar:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","        with torch.no_grad():\n","            outputs = model(images)\n","        valid_labels.append(labels.cpu().numpy())\n","        loss = criterion(outputs, labels)\n","#         loss = bi_tempered_logistic_loss(outputs, labels, t1=0.8, t2 = 1.4)\n","        losses.update(loss.item(), batch_size)\n","#         print(outputs)\n","        preds.append(F.softmax(outputs).to('cpu').numpy())\n","        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n","        pbar.set_postfix(eval_loss=f'{losses.avg:0.4f}',\n","                        gpu_mem=f'{mem:0.2f} GB')\n","    predictions = np.concatenate(preds)\n","    valid_labels = np.concatenate(valid_labels)\n","    return losses.avg, predictions, valid_labels\n","def valid_fn_flip(val_dataloader, model, criterion, device):\n","    losses = AverageMeter()\n","    model.eval()\n","    truth = []\n","    preds = []\n","    valid_labels = []\n","    start = end = time.time()\n","    pbar = tqdm(enumerate(val_dataloader), total=len(val_dataloader), desc='Val')\n","    for step, (images, labels) in pbar:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","        images = torch.flip(images, [3])\n","        with torch.no_grad():\n","            outputs = model(images)\n","        valid_labels.append(labels.cpu().numpy())\n","        loss = criterion(outputs, labels)\n","#         loss = bi_tempered_logistic_loss(outputs, labels, t1=0.8, t2 = 1.4)\n","        losses.update(loss.item(), batch_size)\n","#         print(outputs)\n","        preds.append(torch.sigmoid(outputs).to('cpu').numpy())\n","        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n","        pbar.set_postfix(eval_loss=f'{losses.avg:0.4f}',\n","                        gpu_mem=f'{mem:0.2f} GB')\n","    predictions = np.concatenate(preds)\n","    valid_labels = np.concatenate(valid_labels)\n","    return losses.avg, predictions, valid_labels"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-12-30T08:32:56.198968Z","iopub.status.busy":"2022-12-30T08:32:56.198538Z","iopub.status.idle":"2022-12-30T08:38:54.946641Z","shell.execute_reply":"2022-12-30T08:38:54.945288Z","shell.execute_reply.started":"2022-12-30T08:32:56.198933Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Fold: 0\n"]},{"name":"stdout","output_type":"stream","text":["> SEEDING DONE\n"]},{"name":"stderr","output_type":"stream","text":["Len train df: 44652\n","Train bs: 16\n","optimizer: AdamW (\n","Parameter Group 0\n","    amsgrad: False\n","    betas: (0.9, 0.999)\n","    capturable: False\n","    eps: 1e-08\n","    foreach: None\n","    initial_lr: 0.0001\n","    lr: 0.0\n","    maximize: False\n","    weight_decay: 0.0005\n",")\n","total_epoch :15\n","Epoch: 1/15\n","Train: 100%|██████████| 2790/2790 [29:22<00:00,  1.58it/s, gpu_mem=0.48 GB, lr=0.00010000, train_loss=0.2181]\n","Val: 100%|██████████| 172/172 [03:34<00:00,  1.25s/it, eval_loss=0.1136, gpu_mem=11.47 GB]\n","Train loss:0.2181, Valid loss:0.1136\n","Val metric mean prob: 0.0449\n","Best metric at epoch 1: 0.0964 0.1180 0.5771\n","Cf: [[4312  354]\n"," [  77   23]]\n","Model improve: 0.0000 -> 0.0964\n","Epoch: 2/15\n","Train: 100%|██████████| 2790/2790 [28:51<00:00,  1.61it/s, gpu_mem=0.48 GB, lr=0.00009875, train_loss=0.1571]\n","Val: 100%|██████████| 172/172 [03:29<00:00,  1.22s/it, eval_loss=0.0872, gpu_mem=11.47 GB]\n","Train loss:0.1571, Valid loss:0.0872\n","Val metric mean prob: 0.1265\n","Best metric at epoch 2: 0.3005 0.1650 0.6381\n","Cf: [[4602   64]\n"," [  71   29]]\n","Model improve: 0.0964 -> 0.3005\n","Epoch: 3/15\n","Train: 100%|██████████| 2790/2790 [28:47<00:00,  1.62it/s, gpu_mem=0.48 GB, lr=0.00009505, train_loss=0.1381]\n","Val: 100%|██████████| 172/172 [03:35<00:00,  1.25s/it, eval_loss=0.1040, gpu_mem=11.47 GB]\n","Train loss:0.1381, Valid loss:0.1040\n","Val metric mean prob: 0.1810\n","Best metric at epoch 3: 0.3846 0.4240 0.6700\n","Cf: [[4619   47]\n"," [  65   35]]\n","Model improve: 0.3005 -> 0.3846\n","Epoch: 4/15\n","Train: 100%|██████████| 2790/2790 [28:47<00:00,  1.62it/s, gpu_mem=0.48 GB, lr=0.00008909, train_loss=0.1256]\n","Val: 100%|██████████| 172/172 [03:37<00:00,  1.27s/it, eval_loss=0.0846, gpu_mem=11.47 GB]\n","Train loss:0.1256, Valid loss:0.0846\n","Val metric mean prob: 0.2161\n","Best metric at epoch 4: 0.4151 0.3520 0.6622\n","Cf: [[4640   26]\n"," [  67   33]]\n","Model improve: 0.3846 -> 0.4151\n","Epoch: 5/15\n","Train: 100%|██████████| 2790/2790 [28:42<00:00,  1.62it/s, gpu_mem=0.48 GB, lr=0.00008117, train_loss=0.1134]\n","Val: 100%|██████████| 172/172 [03:40<00:00,  1.28s/it, eval_loss=0.0806, gpu_mem=11.47 GB]\n","Train loss:0.1134, Valid loss:0.0806\n","Val metric mean prob: 0.2228\n","Best metric at epoch 5: 0.4757 0.2300 0.7389\n","Cf: [[4609   57]\n"," [  51   49]]\n","Model improve: 0.4151 -> 0.4757\n","Epoch: 6/15\n","Train: 100%|██████████| 2790/2790 [28:44<00:00,  1.62it/s, gpu_mem=0.48 GB, lr=0.00007169, train_loss=0.1034]\n","Val: 100%|██████████| 172/172 [03:38<00:00,  1.27s/it, eval_loss=0.0838, gpu_mem=11.47 GB]\n","Train loss:0.1034, Valid loss:0.0838\n","Val metric mean prob: 0.2073\n","Best metric at epoch 6: 0.4520 0.1280 0.6960\n","Cf: [[4629   37]\n"," [  60   40]]\n","Epoch: 7/15\n","Train: 100%|██████████| 2790/2790 [28:46<00:00,  1.62it/s, gpu_mem=0.48 GB, lr=0.00006113, train_loss=0.0946]\n","Val: 100%|██████████| 172/172 [03:32<00:00,  1.23s/it, eval_loss=0.0894, gpu_mem=11.47 GB]\n","Train loss:0.0946, Valid loss:0.0894\n","Val metric mean prob: 0.2484\n","Best metric at epoch 7: 0.4510 0.2660 0.7238\n","Cf: [[4608   58]\n"," [  54   46]]\n","Epoch: 8/15\n","Train: 100%|██████████| 2790/2790 [28:46<00:00,  1.62it/s, gpu_mem=0.48 GB, lr=0.00005000, train_loss=0.0824]\n","Val: 100%|██████████| 172/172 [03:29<00:00,  1.22s/it, eval_loss=0.0922, gpu_mem=11.47 GB]\n","Train loss:0.0824, Valid loss:0.0922\n","Val metric mean prob: 0.2635\n","Best metric at epoch 8: 0.4403 0.4150 0.6724\n","Cf: [[4642   24]\n"," [  65   35]]\n","Epoch: 9/15\n","Train: 100%|██████████| 2790/2790 [28:46<00:00,  1.62it/s, gpu_mem=0.48 GB, lr=0.00003887, train_loss=0.0726]\n","Val: 100%|██████████| 172/172 [03:43<00:00,  1.30s/it, eval_loss=0.0919, gpu_mem=11.47 GB]\n","Train loss:0.0726, Valid loss:0.0919\n","Val metric mean prob: 0.2851\n","Best metric at epoch 9: 0.4713 0.4300 0.6829\n","Cf: [[4646   20]\n"," [  63   37]]\n","Epoch: 10/15\n","Train: 100%|██████████| 2790/2790 [28:45<00:00,  1.62it/s, gpu_mem=0.48 GB, lr=0.00002831, train_loss=0.0610]\n","Val:  33%|███▎      | 57/172 [01:07<02:14,  1.17s/it, eval_loss=0.0867, gpu_mem=11.47 GB]"]}],"source":["from exhaustive_weighted_random_sampler import ExhaustiveWeightedRandomSampler\n","def pfbeta(labels, predictions, beta=1):\n","    y_true_count = 0\n","    ctp = 0\n","    cfp = 0\n","\n","    for idx in range(len(labels)):\n","        prediction = min(max(predictions[idx], 0), 1)\n","        if (labels[idx]):\n","            y_true_count += 1\n","            ctp += prediction\n","        else:\n","            cfp += prediction\n","\n","    beta_squared = beta * beta\n","    c_precision = ctp / (ctp + cfp)\n","    c_recall = ctp / y_true_count\n","    if (c_precision > 0 and c_recall > 0):\n","        result = (1 + beta_squared) * (c_precision * c_recall) / (beta_squared * c_precision + c_recall)\n","        return result\n","    else:\n","        return 0\n","    \n","def dfs_freeze(module):\n","    for param in module.parameters():\n","        param.requires_grad = False\n","        \n","def dfs_unfreeze(module):\n","    for param in module.parameters():\n","        param.requires_grad = True\n","    \n","def set_seed(seed = 42):\n","    '''Sets the seed of the entire notebook so results are the same every time we run.\n","    This is for REPRODUCIBILITY.'''\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    # When running on the CuDNN backend, two further options must be set\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    # Set a fixed value for the hash seed\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    print('> SEEDING DONE')\n","\n","def sigmoid(x):\n","  return 1 / (1 + math.exp(-x))\n","\n","set_seed(1)\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","gc.collect()\n","torch.cuda.empty_cache()\n","for fold in [0, 1, 2, 3, 4]:\n","    LOGGER.info(f\"Fold: {fold}\")\n","    model = Model(model_name=CFG.model_name).to(device)\n","    # model = ModelVIT().to(CFG.device)\n","    train_df = df1[df1['fold']!=fold].reset_index(drop=True)\n","    valid_df = df[df['fold']==fold].reset_index(drop=True)\n","    # print(len(valid_df))\n","    LOGGER.info(f\"Len train df: {len(train_df)}\")\n","    cancer_labels = train_df['cancer'].values.tolist()\n","    class_zero =len(train_df[train_df['cancer']==0])\n","    class_one = len(train_df[train_df['cancer']==1])\n","    class_sample_count = np.array([class_zero, class_one*32])\n","    weight = 1. / class_sample_count\n","    samples_weight = np.array([weight[t] for t in cancer_labels])\n","    samples_weight = torch.from_numpy(samples_weight)\n","    samples_weight = samples_weight.double()\n","#     print(samples_weight)\n","    sampler = ExhaustiveWeightedRandomSampler(samples_weight, len(samples_weight))\n","    \n","    train_dataset = BreastDataset(train_df, transforms=data_transforms['train'])\n","\n","    train_loader = DataLoader(train_dataset, batch_size = CFG.train_bs,\n","                                  num_workers=1, shuffle=True, pin_memory=True, drop_last=True)\n","    \n","    valid_dataset = BreastDataset(valid_df, transforms=data_transforms['valid'])\n","\n","    valid_loader = DataLoader(valid_dataset, batch_size = CFG.valid_bs, \n","                                  num_workers=1, shuffle=False, pin_memory=True, drop_last=False)\n","    \n","    LEN_DL_TRAIN = len(train_loader)\n","    best_f1 = 0\n","    best_metric = 0\n","    total_epoch = 15\n","    # checkpoint = torch.load(\"tf_efficientnetv2_b2_fold_0_model_epoch_3_0.3669_0.309.pth\")\n","    # model.load_state_dict(checkpoint['state_dict'])\n","    # base_optimizer = AdamP\n","    # optimizer = SAM(model.parameters(),\n","    #                 base_optimizer,\n","    #                 rho=0.05,\n","    #                 lr=1e-4,\n","    #                 weight_decay=0.0,\n","    #                 nesterov=True)\n","    optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-4, weight_decay = 5e-4)\n","    # optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)  \n","    # optimizer.load_state_dict(checkpoint['optimizer'])\n","    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps = 1*LEN_DL_TRAIN, num_training_steps =total_epoch*LEN_DL_TRAIN)\n","    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, total_epoch)\n","    # scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, total_epoch-1)\n","    # scheduler = GradualWarmupSchedulerV2(optimizer, multiplier=10, total_epoch=1, after_scheduler=scheduler_cosine)\n","    # swa_model = AveragedModel(model)\n","    # swa_scheduler = SWALR(optimizer, swa_lr=1e-4, anneal_epochs=0)\n","    # scheduler.load_state_dict(checkpoint['scheduler'])\n","    criterion = nn.CrossEntropyLoss().to(device)\n","    # criterion1 = nn.BCEWithLogitsLoss().to(device)\n","    # criterion = nn.BCEWithLogitsLoss().to(CFG.device)\n","    LOGGER.info(f\"Train bs: {CFG.train_bs}\")\n","    # LOGGER.info(f\"Model: {model}\")\n","    \n","    LOGGER.info(f\"optimizer: {optimizer}\")\n","    LOGGER.info(f\"total_epoch :{total_epoch}\")\n","#     criterion = FocalLoss().to(device)\n","    for epoch in range(1, total_epoch+1):\n","        # if epoch >=7:\n","        #     swa_model.update_parameters(model)\n","        #     swa_scheduler.step()\n","        # else:\n","        # scheduler.step(epoch-1)\n","        LOGGER.info(f\"Epoch: {epoch}/{total_epoch}\")\n","        loss_train = train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device)\n","        # state = {'epoch': epoch, 'state_dict': model.state_dict(),'optimizer': optimizer.state_dict(), 'scheduler':scheduler.state_dict()}\n","        # path = f'{CFG.model_name}_fold_{fold}_model_epoch_{epoch}.pth'\n","        # torch.save(state, path)\n","        loss_valid, valid_preds, valid_labels = valid_fn_two(valid_loader, model, criterion, device)\n","        # print(valid_preds)\n","        valid_preds = valid_preds[:, 1]\n","        valid_df['prediction_id'] = valid_df['patient_id'].astype(str) + '_' + valid_df['laterality'].astype(str)\n","        valid_preds = np.array(valid_preds).flatten()\n","        \n","        valid_df['raw_pred'] = valid_preds\n","        # LOGGER.info(f\"Valid loss:{loss_valid:.4f}\")\n","        LOGGER.info(f\"Train loss:{loss_train:.4f}, Valid loss:{loss_valid:.4f}\")\n","        # print(valid_df.head())\n","        grp_df = valid_df.groupby('prediction_id')['raw_pred', 'cancer'].mean()\n","        grp_df['cancer'] = grp_df['cancer'].astype(np.int)\n","        valid_labels_mean = grp_df['cancer'].values.tolist()\n","        valid_preds_mean = grp_df['raw_pred'].values.tolist()\n","        # print(valid_labels[:5], valid_preds_mean[:5])\n","        val_metric_mean = pfbeta(valid_labels_mean, valid_preds_mean)\n","        LOGGER.info(f\"Val metric mean prob: {val_metric_mean:.4f}\")\n","        best_metric_mean_at_epoch = 0\n","        best_threshold_mean = 0\n","        best_auc = 0\n","        best_cf = None\n","        for i in np.arange(0.001, 0.599, 0.001):\n","            valid_argmax = (valid_preds_mean>i).astype(np.int32)\n","    #             print(valid_argmax)\n","            val_metric = pfbeta(valid_labels_mean, valid_argmax)\n","            val_acc = accuracy_score(valid_labels_mean, valid_argmax)\n","            val_f1 = f1_score(valid_labels_mean, valid_argmax)\n","            val_auc = roc_auc_score(valid_labels_mean, valid_argmax)\n","            cf = confusion_matrix(valid_labels_mean, valid_argmax)\n","            if val_metric> best_metric_mean_at_epoch:\n","                best_metric_mean_at_epoch = val_metric\n","                best_threshold_mean = i\n","                best_auc = val_auc\n","                best_cf = cf\n","            # print(f\"Threshold: {i:.4f}, val_acc: {val_acc:.4f}, val_f1: {val_f1:.4f}, val_auc: {val_auc:.4f}, val_metric: {val_metric:.4f}\")\n","        LOGGER.info(f\"Best metric at epoch {epoch}: {best_metric_mean_at_epoch:.4f} {best_threshold_mean:.4f} {best_auc:.4f}\")\n","        LOGGER.info(f\"Cf: {best_cf}\")\n","    #         print(f\"Train loss: {loss_train:.4f}, eval loss: {loss_valid.avg:.4f}\") \n","    #         print(f\"Accuracy score: {val_acc:.4f}, f1 score: {val_f1:.4f}\")\n","    #         print(f\"Comp metric: {val_metric:.4f}\")\n","        if best_metric_mean_at_epoch > best_metric:\n","\n","            LOGGER.info(f\"Model improve: {best_metric:.4f} -> {best_metric_mean_at_epoch:.4f}\")\n","            best_metric = best_metric_mean_at_epoch\n","        state = {'epoch': epoch, 'state_dict': model.state_dict(),'optimizer': optimizer.state_dict(), 'scheduler':scheduler.state_dict()}\n","        path = f'{CFG.model_name}_fold_{fold}_model_epoch_{epoch}_{best_metric_mean_at_epoch:.4f}_{best_threshold_mean:.3f}.pth'\n","        torch.save(state, path)\n","    #     loss_valid, valid_preds, valid_labels = valid_fn_no_sigmoid(valid_loader, model, criterion, device)\n","        \n","    #     # valid_preds = valid_preds[:, 1]\n","    #     valid_df['prediction_id'] = valid_df['patient_id'].astype(str) + '_' + valid_df['laterality'].astype(str)\n","    #     valid_preds = np.array(valid_preds).flatten()\n","        \n","    #     valid_df['raw_pred'] = valid_preds\n","    #     # LOGGER.info(f\"Valid loss:{loss_valid:.4f}\")\n","    #     LOGGER.info(f\"Train loss:{loss_train:.4f}, Valid loss:{loss_valid:.4f}\")\n","    #     # print(valid_df.head())\n","    #     grp_df = valid_df.groupby('prediction_id')['raw_pred', 'cancer'].mean()\n","    #     grp_df['cancer'] = grp_df['cancer'].astype(np.int)\n","    #     valid_labels_mean = grp_df['cancer'].values.tolist()\n","    #     valid_preds_mean = grp_df['raw_pred'].values.tolist()\n","    #     valid_preds_mean = [sigmoid(x) for x in valid_preds_mean]\n","    #     # print(valid_labels[:5], valid_preds_mean[:5])\n","    #     val_metric_mean = pfbeta(valid_labels_mean, valid_preds_mean)\n","    #     LOGGER.info(f\"Val metric mean prob: {val_metric_mean:.4f}\")\n","    #     best_metric_mean_at_epoch = 0\n","    #     best_threshold_mean = 0\n","    #     best_auc = 0\n","    #     best_cf = None\n","    #     for i in np.arange(0.001, 0.999, 0.001):\n","    #         valid_argmax = (valid_preds_mean>i).astype(np.int32)\n","    # #             print(valid_argmax)\n","    #         val_metric = pfbeta(valid_labels_mean, valid_argmax)\n","    #         val_acc = accuracy_score(valid_labels_mean, valid_argmax)\n","    #         val_f1 = f1_score(valid_labels_mean, valid_argmax)\n","    #         val_auc = roc_auc_score(valid_labels_mean, valid_argmax)\n","    #         cf = confusion_matrix(valid_labels_mean, valid_argmax)\n","    #         if val_metric> best_metric_mean_at_epoch:\n","    #             best_metric_mean_at_epoch = val_metric\n","    #             best_threshold_mean = i\n","    #             best_auc = val_auc\n","    #             best_cf = cf\n","    #         # print(f\"Threshold: {i:.4f}, val_acc: {val_acc:.4f}, val_f1: {val_f1:.4f}, val_auc: {val_auc:.4f}, val_metric: {val_metric:.4f}\")\n","    #     LOGGER.info(f\"Best metric at epoch {epoch}: {best_metric_mean_at_epoch:.4f} {best_threshold_mean:.4f} {best_auc:.4f}\")\n","    #     LOGGER.info(f\"Cf: {best_cf}\")\n","    # #         print(f\"Train loss: {loss_train:.4f}, eval loss: {loss_valid.avg:.4f}\") \n","    # #         print(f\"Accuracy score: {val_acc:.4f}, f1 score: {val_f1:.4f}\")\n","    # #         print(f\"Comp metric: {val_metric:.4f}\")\n","    #     if best_metric_mean_at_epoch > best_metric:\n","\n","    #         LOGGER.info(f\"Model improve: {best_metric:.4f} -> {best_metric_mean_at_epoch:.4f}\")\n","    #         best_metric = best_metric_mean_at_epoch\n","    #     state = {'epoch': epoch+1, 'state_dict': model.state_dict(),'optimizer': optimizer.state_dict(), 'scheduler':scheduler.state_dict()}\n","    #     path = f'{CFG.model_name}_fold_{fold}_model_epoch_{epoch+1}_{best_metric_mean_at_epoch:.4f}_{best_threshold_mean:.3f}.pth'\n","    #     torch.save(state, path)\n","#     torch.optim.swa_utils.update_bn(train_loader, swa_model, device=device)\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"zaloenv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"d81213625f550c7b434bbb4e964cd1250716e6d81b88f327aa7e418dc0078b84"}}},"nbformat":4,"nbformat_minor":4}
