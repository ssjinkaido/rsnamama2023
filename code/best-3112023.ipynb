{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-12-30T08:23:20.757433Z","iopub.status.busy":"2022-12-30T08:23:20.756596Z","iopub.status.idle":"2022-12-30T08:23:20.769292Z","shell.execute_reply":"2022-12-30T08:23:20.768338Z","shell.execute_reply.started":"2022-12-30T08:23:20.757395Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/tungnx/miniconda3/envs/zaloenv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import random\n","from glob import glob\n","import os, shutil\n","from tqdm import tqdm\n","tqdm.pandas()\n","import time\n","import copy\n","import joblib\n","from collections import defaultdict\n","import gc\n","from IPython import display as ipd\n","import math\n","# visualization\n","import cv2\n","from glob import glob\n","# Sklearn\n","from sklearn.model_selection import StratifiedKFold, KFold, StratifiedGroupKFold\n","from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, confusion_matrix, roc_curve\n","import timm\n","# PyTorch \n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from torch.utils.data import Dataset, DataLoader\n","from torch.cuda.amp import autocast, GradScaler\n","import torch.nn.functional as F\n","from torch.optim.swa_utils import AveragedModel, SWALR\n","from transformers import get_cosine_schedule_with_warmup\n","from collections import defaultdict\n","# import matplotlib.pyplot as plt\n","# Albumentations for augmentations\n","import albumentations as A\n","import albumentations\n","import albumentations as albu\n","from albumentations.pytorch import ToTensorV2\n","from datetime import datetime\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-12-30T08:23:20.771750Z","iopub.status.busy":"2022-12-30T08:23:20.771281Z","iopub.status.idle":"2022-12-30T08:23:20.782962Z","shell.execute_reply":"2022-12-30T08:23:20.782013Z","shell.execute_reply.started":"2022-12-30T08:23:20.771715Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda:0\n"]}],"source":["class CFG:\n","    seed = 1\n","    model_name = \"tf_efficientnetv2_b2\"\n","    train_bs = 16\n","    valid_bs = 64\n","    image_size = 1024\n","    epochs = 25\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","print(CFG.device)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-12-30T08:23:20.784720Z","iopub.status.busy":"2022-12-30T08:23:20.784325Z","iopub.status.idle":"2022-12-30T08:23:20.912644Z","shell.execute_reply":"2022-12-30T08:23:20.911668Z","shell.execute_reply.started":"2022-12-30T08:23:20.784684Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>site_id</th>\n","      <th>patient_id</th>\n","      <th>image_id</th>\n","      <th>laterality</th>\n","      <th>view</th>\n","      <th>age</th>\n","      <th>cancer</th>\n","      <th>biopsy</th>\n","      <th>invasive</th>\n","      <th>BIRADS</th>\n","      <th>implant</th>\n","      <th>density</th>\n","      <th>machine_id</th>\n","      <th>difficult_negative_case</th>\n","      <th>split</th>\n","      <th>fold</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2</td>\n","      <td>10006</td>\n","      <td>462822612</td>\n","      <td>L</td>\n","      <td>CC</td>\n","      <td>61.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>29</td>\n","      <td>False</td>\n","      <td>10006_L</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>10006</td>\n","      <td>1459541791</td>\n","      <td>L</td>\n","      <td>MLO</td>\n","      <td>61.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>29</td>\n","      <td>False</td>\n","      <td>10006_L</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>10006</td>\n","      <td>1864590858</td>\n","      <td>R</td>\n","      <td>MLO</td>\n","      <td>61.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>29</td>\n","      <td>False</td>\n","      <td>10006_R</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2</td>\n","      <td>10006</td>\n","      <td>1874946579</td>\n","      <td>R</td>\n","      <td>CC</td>\n","      <td>61.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>29</td>\n","      <td>False</td>\n","      <td>10006_R</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2</td>\n","      <td>10011</td>\n","      <td>220375232</td>\n","      <td>L</td>\n","      <td>CC</td>\n","      <td>55.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>21</td>\n","      <td>True</td>\n","      <td>10011_L</td>\n","      <td>5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   site_id  patient_id    image_id laterality view   age  cancer  biopsy  \\\n","0        2       10006   462822612          L   CC  61.0       0       0   \n","1        2       10006  1459541791          L  MLO  61.0       0       0   \n","2        2       10006  1864590858          R  MLO  61.0       0       0   \n","3        2       10006  1874946579          R   CC  61.0       0       0   \n","4        2       10011   220375232          L   CC  55.0       0       0   \n","\n","   invasive  BIRADS  implant density  machine_id  difficult_negative_case  \\\n","0         0     NaN        0     NaN          29                    False   \n","1         0     NaN        0     NaN          29                    False   \n","2         0     NaN        0     NaN          29                    False   \n","3         0     NaN        0     NaN          29                    False   \n","4         0     0.0        0     NaN          21                     True   \n","\n","     split  fold  \n","0  10006_L     0  \n","1  10006_L     0  \n","2  10006_R     2  \n","3  10006_R     2  \n","4  10011_L     5  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.read_csv(\"train_10folds.csv\")\n","df.head()"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["55864\n"]}],"source":["is_hol = df['cancer'] == 1\n","df_try = df[is_hol]\n","df1 = df.append([df_try]*1,ignore_index=True)\n","print(len(df1))"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["from functools import partial\n","\n","import torch\n","import torch.utils.checkpoint as checkpoint\n","from einops import rearrange\n","from timm.models.layers import DropPath, trunc_normal_\n","from timm.models.registry import register_model\n","from torch import nn\n","\n","NORM_EPS = 1e-5\n","\n","def merge_pre_bn(module, pre_bn_1, pre_bn_2=None):\n","    \"\"\" Merge pre BN to reduce inference runtime.\n","    \"\"\"\n","    weight = module.weight.data\n","    if module.bias is None:\n","        zeros = torch.zeros(module.out_channels, device=weight.device).type(weight.type())\n","        module.bias = nn.Parameter(zeros)\n","    bias = module.bias.data\n","    if pre_bn_2 is None:\n","        assert pre_bn_1.track_running_stats is True, \"Unsupport bn_module.track_running_stats is False\"\n","        assert pre_bn_1.affine is True, \"Unsupport bn_module.affine is False\"\n","\n","        scale_invstd = pre_bn_1.running_var.add(pre_bn_1.eps).pow(-0.5)\n","        extra_weight = scale_invstd * pre_bn_1.weight\n","        extra_bias = pre_bn_1.bias - pre_bn_1.weight * pre_bn_1.running_mean * scale_invstd\n","    else:\n","        assert pre_bn_1.track_running_stats is True, \"Unsupport bn_module.track_running_stats is False\"\n","        assert pre_bn_1.affine is True, \"Unsupport bn_module.affine is False\"\n","\n","        assert pre_bn_2.track_running_stats is True, \"Unsupport bn_module.track_running_stats is False\"\n","        assert pre_bn_2.affine is True, \"Unsupport bn_module.affine is False\"\n","\n","        scale_invstd_1 = pre_bn_1.running_var.add(pre_bn_1.eps).pow(-0.5)\n","        scale_invstd_2 = pre_bn_2.running_var.add(pre_bn_2.eps).pow(-0.5)\n","\n","        extra_weight = scale_invstd_1 * pre_bn_1.weight * scale_invstd_2 * pre_bn_2.weight\n","        extra_bias = scale_invstd_2 * pre_bn_2.weight *(pre_bn_1.bias - pre_bn_1.weight * pre_bn_1.running_mean * scale_invstd_1 - pre_bn_2.running_mean) + pre_bn_2.bias\n","\n","    if isinstance(module, nn.Linear):\n","        extra_bias = weight @ extra_bias\n","        weight.mul_(extra_weight.view(1, weight.size(1)).expand_as(weight))\n","    elif isinstance(module, nn.Conv2d):\n","        assert weight.shape[2] == 1 and weight.shape[3] == 1\n","        weight = weight.reshape(weight.shape[0], weight.shape[1])\n","        extra_bias = weight @ extra_bias\n","        weight.mul_(extra_weight.view(1, weight.size(1)).expand_as(weight))\n","        weight = weight.reshape(weight.shape[0], weight.shape[1], 1, 1)\n","    bias.add_(extra_bias)\n","\n","    module.weight.data = weight\n","    module.bias.data = bias\n","\n","class ConvBNReLU(nn.Module):\n","    def __init__(\n","            self,\n","            in_channels,\n","            out_channels,\n","            kernel_size,\n","            stride,\n","            groups=1):\n","        super(ConvBNReLU, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n","                              padding=1, groups=groups, bias=False)\n","        self.norm = nn.BatchNorm2d(out_channels, eps=NORM_EPS)\n","        self.act = nn.ReLU(inplace=True)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        x = self.norm(x)\n","        x = self.act(x)\n","        return x\n","\n","\n","def _make_divisible(v, divisor, min_value=None):\n","    if min_value is None:\n","        min_value = divisor\n","    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n","    # Make sure that round down does not go down by more than 10%.\n","    if new_v < 0.9 * v:\n","        new_v += divisor\n","    return new_v\n","\n","\n","class PatchEmbed(nn.Module):\n","    def __init__(self,\n","                 in_channels,\n","                 out_channels,\n","                 stride=1):\n","        super(PatchEmbed, self).__init__()\n","        norm_layer = partial(nn.BatchNorm2d, eps=NORM_EPS)\n","        if stride == 2:\n","            self.avgpool = nn.AvgPool2d((2, 2), stride=2, ceil_mode=True, count_include_pad=False)\n","            self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n","            self.norm = norm_layer(out_channels)\n","        elif in_channels != out_channels:\n","            self.avgpool = nn.Identity()\n","            self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False)\n","            self.norm = norm_layer(out_channels)\n","        else:\n","            self.avgpool = nn.Identity()\n","            self.conv = nn.Identity()\n","            self.norm = nn.Identity()\n","\n","    def forward(self, x):\n","        return self.norm(self.conv(self.avgpool(x)))\n","\n","\n","class MHCA(nn.Module):\n","    \"\"\"\n","    Multi-Head Convolutional Attention\n","    \"\"\"\n","    def __init__(self, out_channels, head_dim):\n","        super(MHCA, self).__init__()\n","        norm_layer = partial(nn.BatchNorm2d, eps=NORM_EPS)\n","        self.group_conv3x3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1,\n","                                       padding=1, groups=out_channels // head_dim, bias=False)\n","        self.norm = norm_layer(out_channels)\n","        self.act = nn.ReLU(inplace=True)\n","        self.projection = nn.Conv2d(out_channels, out_channels, kernel_size=1, bias=False)\n","\n","    def forward(self, x):\n","        out = self.group_conv3x3(x)\n","        out = self.norm(out)\n","        out = self.act(out)\n","        out = self.projection(out)\n","        return out\n","\n","\n","class Mlp(nn.Module):\n","    def __init__(self, in_features, out_features=None, mlp_ratio=None, drop=0., bias=True):\n","        super().__init__()\n","        out_features = out_features or in_features\n","        hidden_dim = _make_divisible(in_features * mlp_ratio, 32)\n","        self.conv1 = nn.Conv2d(in_features, hidden_dim, kernel_size=1, bias=bias)\n","        self.act = nn.ReLU(inplace=True)\n","        self.conv2 = nn.Conv2d(hidden_dim, out_features, kernel_size=1, bias=bias)\n","        self.drop = nn.Dropout(drop)\n","\n","    def merge_bn(self, pre_norm):\n","        merge_pre_bn(self.conv1, pre_norm)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.act(x)\n","        x = self.drop(x)\n","        x = self.conv2(x)\n","        x = self.drop(x)\n","        return x\n","\n","\n","class NCB(nn.Module):\n","    \"\"\"\n","    Next Convolution Block\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, stride=1, path_dropout=0,\n","                 drop=0, head_dim=32, mlp_ratio=3):\n","        super(NCB, self).__init__()\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        norm_layer = partial(nn.BatchNorm2d, eps=NORM_EPS)\n","        assert out_channels % head_dim == 0\n","\n","        self.patch_embed = PatchEmbed(in_channels, out_channels, stride)\n","        self.mhca = MHCA(out_channels, head_dim)\n","        self.attention_path_dropout = DropPath(path_dropout)\n","\n","        self.norm = norm_layer(out_channels)\n","        self.mlp = Mlp(out_channels, mlp_ratio=mlp_ratio, drop=drop, bias=True)\n","        self.mlp_path_dropout = DropPath(path_dropout)\n","        self.is_bn_merged = False\n","\n","    def merge_bn(self):\n","        if not self.is_bn_merged:\n","            self.mlp.merge_bn(self.norm)\n","            self.is_bn_merged = True\n","\n","    def forward(self, x):\n","        x = self.patch_embed(x)\n","        x = x + self.attention_path_dropout(self.mhca(x))\n","        if not torch.onnx.is_in_onnx_export() and not self.is_bn_merged:\n","            out = self.norm(x)\n","        else:\n","            out = x\n","        x = x + self.mlp_path_dropout(self.mlp(out))\n","        return x\n","\n","\n","class E_MHSA(nn.Module):\n","    \"\"\"\n","    Efficient Multi-Head Self Attention\n","    \"\"\"\n","    def __init__(self, dim, out_dim=None, head_dim=32, qkv_bias=True, qk_scale=None,\n","                 attn_drop=0, proj_drop=0., sr_ratio=1):\n","        super().__init__()\n","        self.dim = dim\n","        self.out_dim = out_dim if out_dim is not None else dim\n","        self.num_heads = self.dim // head_dim\n","        self.scale = qk_scale or head_dim ** -0.5\n","        self.q = nn.Linear(dim, self.dim, bias=qkv_bias)\n","        self.k = nn.Linear(dim, self.dim, bias=qkv_bias)\n","        self.v = nn.Linear(dim, self.dim, bias=qkv_bias)\n","        self.proj = nn.Linear(self.dim, self.out_dim)\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","        self.sr_ratio = sr_ratio\n","        self.N_ratio = sr_ratio ** 2\n","        if sr_ratio > 1:\n","            self.sr = nn.AvgPool1d(kernel_size=self.N_ratio, stride=self.N_ratio)\n","            self.norm = nn.BatchNorm1d(dim, eps=NORM_EPS)\n","        self.is_bn_merged = False\n","\n","    def merge_bn(self, pre_bn):\n","        merge_pre_bn(self.q, pre_bn)\n","        if self.sr_ratio > 1:\n","            merge_pre_bn(self.k, pre_bn, self.norm)\n","            merge_pre_bn(self.v, pre_bn, self.norm)\n","        else:\n","            merge_pre_bn(self.k, pre_bn)\n","            merge_pre_bn(self.v, pre_bn)\n","        self.is_bn_merged = True\n","\n","    def forward(self, x):\n","        B, N, C = x.shape\n","        q = self.q(x)\n","        q = q.reshape(B, N, self.num_heads, int(C // self.num_heads)).permute(0, 2, 1, 3)\n","\n","        if self.sr_ratio > 1:\n","            x_ = x.transpose(1, 2)\n","            x_ = self.sr(x_)\n","            if not torch.onnx.is_in_onnx_export() and not self.is_bn_merged:\n","                x_ = self.norm(x_)\n","            x_ = x_.transpose(1, 2)\n","            k = self.k(x_)\n","            k = k.reshape(B, -1, self.num_heads, int(C // self.num_heads)).permute(0, 2, 3, 1)\n","            v = self.v(x_)\n","            v = v.reshape(B, -1, self.num_heads, int(C // self.num_heads)).permute(0, 2, 1, 3)\n","        else:\n","            k = self.k(x)\n","            k = k.reshape(B, -1, self.num_heads, int(C // self.num_heads)).permute(0, 2, 3, 1)\n","            v = self.v(x)\n","            v = v.reshape(B, -1, self.num_heads, int(C // self.num_heads)).permute(0, 2, 1, 3)\n","        attn = (q @ k) * self.scale\n","\n","        attn = attn.softmax(dim=-1)\n","        attn = self.attn_drop(attn)\n","\n","        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","        return x\n","\n","\n","class NTB(nn.Module):\n","    \"\"\"\n","    Next Transformer Block\n","    \"\"\"\n","    def __init__(\n","            self, in_channels, out_channels, path_dropout, stride=1, sr_ratio=1,\n","            mlp_ratio=2, head_dim=32, mix_block_ratio=0.75, attn_drop=0, drop=0,\n","    ):\n","        super(NTB, self).__init__()\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.mix_block_ratio = mix_block_ratio\n","        norm_func = partial(nn.BatchNorm2d, eps=NORM_EPS)\n","\n","        self.mhsa_out_channels = _make_divisible(int(out_channels * mix_block_ratio), 32)\n","        self.mhca_out_channels = out_channels - self.mhsa_out_channels\n","\n","        self.patch_embed = PatchEmbed(in_channels, self.mhsa_out_channels, stride)\n","        self.norm1 = norm_func(self.mhsa_out_channels)\n","        self.e_mhsa = E_MHSA(self.mhsa_out_channels, head_dim=head_dim, sr_ratio=sr_ratio,\n","                             attn_drop=attn_drop, proj_drop=drop)\n","        self.mhsa_path_dropout = DropPath(path_dropout * mix_block_ratio)\n","\n","        self.projection = PatchEmbed(self.mhsa_out_channels, self.mhca_out_channels, stride=1)\n","        self.mhca = MHCA(self.mhca_out_channels, head_dim=head_dim)\n","        self.mhca_path_dropout = DropPath(path_dropout * (1 - mix_block_ratio))\n","\n","        self.norm2 = norm_func(out_channels)\n","        self.mlp = Mlp(out_channels, mlp_ratio=mlp_ratio, drop=drop)\n","        self.mlp_path_dropout = DropPath(path_dropout)\n","\n","        self.is_bn_merged = False\n","\n","    def merge_bn(self):\n","        if not self.is_bn_merged:\n","            self.e_mhsa.merge_bn(self.norm1)\n","            self.mlp.merge_bn(self.norm2)\n","            self.is_bn_merged = True\n","\n","    def forward(self, x):\n","        x = self.patch_embed(x)\n","        B, C, H, W = x.shape\n","        if not torch.onnx.is_in_onnx_export() and not self.is_bn_merged:\n","            out = self.norm1(x)\n","        else:\n","            out = x\n","        out = rearrange(out, \"b c h w -> b (h w) c\")  # b n c\n","        out = self.mhsa_path_dropout(self.e_mhsa(out))\n","        x = x + rearrange(out, \"b (h w) c -> b c h w\", h=H)\n","\n","        out = self.projection(x)\n","        out = out + self.mhca_path_dropout(self.mhca(out))\n","        x = torch.cat([x, out], dim=1)\n","\n","        if not torch.onnx.is_in_onnx_export() and not self.is_bn_merged:\n","            out = self.norm2(x)\n","        else:\n","            out = x\n","        x = x + self.mlp_path_dropout(self.mlp(out))\n","        return x\n","\n","\n","class NextViT(nn.Module):\n","    def __init__(self, stem_chs, depths, path_dropout, attn_drop=0, drop=0, num_classes=1000,\n","                 strides=[1, 2, 2, 2], sr_ratios=[8, 4, 2, 1], head_dim=32, mix_block_ratio=0.75,\n","                 use_checkpoint=False):\n","        super(NextViT, self).__init__()\n","        self.use_checkpoint = use_checkpoint\n","\n","        self.stage_out_channels = [[96] * (depths[0]),\n","                                   [192] * (depths[1] - 1) + [256],\n","                                   [384, 384, 384, 384, 512] * (depths[2] // 5),\n","                                   [768] * (depths[3] - 1) + [1024]]\n","\n","        # Next Hybrid Strategy\n","        self.stage_block_types = [[NCB] * depths[0],\n","                                  [NCB] * (depths[1] - 1) + [NTB],\n","                                  [NCB, NCB, NCB, NCB, NTB] * (depths[2] // 5),\n","                                  [NCB] * (depths[3] - 1) + [NTB]]\n","\n","        self.stem = nn.Sequential(\n","            ConvBNReLU(3, stem_chs[0], kernel_size=3, stride=2),\n","            ConvBNReLU(stem_chs[0], stem_chs[1], kernel_size=3, stride=1),\n","            ConvBNReLU(stem_chs[1], stem_chs[2], kernel_size=3, stride=1),\n","            ConvBNReLU(stem_chs[2], stem_chs[2], kernel_size=3, stride=2),\n","        )\n","        input_channel = stem_chs[-1]\n","        features = []\n","        idx = 0\n","        dpr = [x.item() for x in torch.linspace(0, path_dropout, sum(depths))]  # stochastic depth decay rule\n","        for stage_id in range(len(depths)):\n","            numrepeat = depths[stage_id]\n","            output_channels = self.stage_out_channels[stage_id]\n","            block_types = self.stage_block_types[stage_id]\n","            for block_id in range(numrepeat):\n","                if strides[stage_id] == 2 and block_id == 0:\n","                    stride = 2\n","                else:\n","                    stride = 1\n","                output_channel = output_channels[block_id]\n","                block_type = block_types[block_id]\n","                if block_type is NCB:\n","                    layer = NCB(input_channel, output_channel, stride=stride, path_dropout=dpr[idx + block_id],\n","                                drop=drop, head_dim=head_dim)\n","                    features.append(layer)\n","                elif block_type is NTB:\n","                    layer = NTB(input_channel, output_channel, path_dropout=dpr[idx + block_id], stride=stride,\n","                                sr_ratio=sr_ratios[stage_id], head_dim=head_dim, mix_block_ratio=mix_block_ratio,\n","                                attn_drop=attn_drop, drop=drop)\n","                    features.append(layer)\n","                input_channel = output_channel\n","            idx += numrepeat\n","        self.features = nn.Sequential(*features)\n","\n","        self.norm = nn.BatchNorm2d(output_channel, eps=NORM_EPS)\n","\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        self.proj_head = nn.Sequential(\n","            nn.Linear(output_channel, num_classes),\n","        )\n","\n","        self.stage_out_idx = [sum(depths[:idx + 1]) - 1 for idx in range(len(depths))]\n","        print('initialize_weights...')\n","        self._initialize_weights()\n","\n","    def merge_bn(self):\n","        self.eval()\n","        for idx, module in self.named_modules():\n","            if isinstance(module, NCB) or isinstance(module, NTB):\n","                module.merge_bn()\n","\n","    def _initialize_weights(self):\n","        for n, m in self.named_modules():\n","            if isinstance(m, (nn.BatchNorm2d, nn.GroupNorm, nn.LayerNorm, nn.BatchNorm1d)):\n","                nn.init.constant_(m.weight, 1.0)\n","                nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.Linear):\n","                trunc_normal_(m.weight, std=.02)\n","                if hasattr(m, 'bias') and m.bias is not None:\n","                    nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.Conv2d):\n","                trunc_normal_(m.weight, std=.02)\n","                if hasattr(m, 'bias') and m.bias is not None:\n","                    nn.init.constant_(m.bias, 0)\n","\n","    def forward(self, x):\n","        x = self.stem(x)\n","        for idx, layer in enumerate(self.features):\n","            if self.use_checkpoint:\n","                x = checkpoint.checkpoint(layer, x)\n","            else:\n","                x = layer(x)\n","        x = self.norm(x)\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.proj_head(x)\n","        return x\n","\n","\n","@register_model\n","def nextvit_small(pretrained=False, pretrained_cfg=None, **kwargs):\n","    model = NextViT(stem_chs=[64, 32, 64], depths=[3, 4, 10, 3], path_dropout=0.1, **kwargs)\n","    return model\n","\n","\n","@register_model\n","def nextvit_base(pretrained=False, pretrained_cfg=None, **kwargs):\n","    model = NextViT(stem_chs=[64, 32, 64], depths=[3, 4, 20, 3], path_dropout=0.2, **kwargs)\n","    return model\n","\n","\n","@register_model\n","def nextvit_large(pretrained=False, pretrained_cfg=None, **kwargs):\n","    model = NextViT(stem_chs=[64, 32, 64], depths=[3, 4, 30, 3], path_dropout=0.2, **kwargs)\n","    return model\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Date :01/30/2023, 14:08:30\n"]}],"source":["def init_logger(log_file='train1.log'):\n","    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n","    logger = getLogger(__name__)\n","    logger.setLevel(INFO)\n","    handler1 = StreamHandler()\n","    handler1.setFormatter(Formatter(\"%(message)s\"))\n","    handler2 = FileHandler(filename=log_file)\n","    handler2.setFormatter(Formatter(\"%(message)s\"))\n","    logger.addHandler(handler1)\n","    logger.addHandler(handler2)\n","    return logger\n","\n","LOGGER = init_logger()\n","now = datetime.now()\n","datetime_now = now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n","LOGGER.info(f\"Date :{datetime_now}\")"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["train transformCompose([\n","  VerticalFlip(always_apply=False, p=0.5),\n","  ColorJitter(always_apply=False, p=0.5, brightness=[0.8, 1.2], contrast=[0.8, 1.2], saturation=[0.8, 1.2], hue=[-0.2, 0.2]),\n","  ShiftScaleRotate(always_apply=False, p=0.5, shift_limit_x=(-0.0625, 0.0625), shift_limit_y=(-0.0625, 0.0625), scale_limit=(-0.050000000000000044, 0.050000000000000044), rotate_limit=(-10, 10), interpolation=1, border_mode=4, value=None, mask_value=None, rotate_method='largest_box'),\n","  HorizontalFlip(always_apply=False, p=0.5),\n","  Cutout(always_apply=False, p=0.5, num_holes=5, max_h_size=102, max_w_size=102),\n","  ToTensorV2(always_apply=True, p=1.0, transpose_mask=False),\n","], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={})\n"]}],"source":["from albumentations import DualTransform\n","image_size = 1024\n","def isotropically_resize_image(img, size, interpolation_down=cv2.INTER_AREA, interpolation_up=cv2.INTER_CUBIC):\n","    h, w = img.shape[:2]\n","    if max(w, h) == size:\n","        return img\n","    if w > h:\n","        scale = size / w\n","        h = h * scale\n","        w = size\n","    else:\n","        scale = size / h\n","        w = w * scale\n","        h = size\n","    interpolation = interpolation_up if scale > 1 else interpolation_down\n","    resized = cv2.resize(img, (int(w), int(h)), interpolation=interpolation)\n","    return resized\n","\n","\n","class IsotropicResize(DualTransform):\n","    def __init__(self, max_side, interpolation_down=cv2.INTER_AREA, interpolation_up=cv2.INTER_CUBIC,\n","                 always_apply=False, p=1):\n","        super(IsotropicResize, self).__init__(always_apply, p)\n","        self.max_side = max_side\n","        self.interpolation_down = interpolation_down\n","        self.interpolation_up = interpolation_up\n","\n","    def apply(self, img, interpolation_down=cv2.INTER_AREA, interpolation_up=cv2.INTER_CUBIC, **params):\n","        return isotropically_resize_image(img, size=self.max_side, interpolation_down=interpolation_down,\n","                                          interpolation_up=interpolation_up)\n","\n","    def apply_to_mask(self, img, **params):\n","        return self.apply(img, interpolation_down=cv2.INTER_NEAREST, interpolation_up=cv2.INTER_NEAREST, **params)\n","\n","    def get_transform_init_args_names(self):\n","        return (\"max_side\", \"interpolation_down\", \"interpolation_up\")\n","    \n","data_transforms = {\n","    \"train\": A.Compose([\n","        # A.Resize(image_size, image_size),\n","        # IsotropicResize(max_side = image_size),\n","        # A.PadIfNeeded(min_height=image_size, min_width=image_size, border_mode=cv2.BORDER_CONSTANT),\n","        # A.RandomBrightnessContrast(),\n","        # A.VerticalFlip(p=0.5),   \n","        # A.ColorJitter(),\n","        # A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.05, rotate_limit=10, p=0.5),\n","        # A.HorizontalFlip(p=0.5),\n","        # A.Cutout(max_h_size=int(image_size * 0.1), max_w_size=int(image_size * 0.1), num_holes=5, p=0.5),\n","        A.VerticalFlip(p=0.5),   \n","        A.ColorJitter(),\n","        A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.05, rotate_limit=10, p=0.5),\n","        A.HorizontalFlip(p=0.5),\n","        A.Cutout(max_h_size=102, max_w_size=102, num_holes=5, p=0.5),\n","        # A.CLAHE(p=1.0),\n","        # albumentations.HorizontalFlip(p=0.5),\n","        # # albumentations.VerticalFlip(p=0.5),\n","        # albumentations.RandomBrightness(limit=0.2, p=0.75),\n","        # albumentations.RandomContrast(limit=0.2, p=0.75),\n","\n","        # albumentations.OneOf([\n","        #     albumentations.OpticalDistortion(distort_limit=1.),\n","        #     albumentations.GridDistortion(num_steps=5, distort_limit=1.),\n","        # ], p=0.75),\n","\n","        # albumentations.HueSaturationValue(hue_shift_limit=40, sat_shift_limit=40, val_shift_limit=0, p=0.75),\n","        # albumentations.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.3, rotate_limit=30, border_mode=0, p=0.75),\n","        # A.Cutout(always_apply=False, p=0.5, num_holes=1, max_h_size=409, max_w_size=409),\n","        # A.OneOf([ \n","        # A.OpticalDistortion(distort_limit=1.0), \n","        # A.GridDistortion(num_steps=5, distort_limit=1.),\n","        # A.ElasticTransform(alpha=3), ], p=0.2),\n","        # A.OneOf([\n","        #     # A.GaussNoise(var_limit=[10, 50]),\n","        #     A.GaussianBlur(),\n","        #     A.MotionBlur(),\n","        #     A.MedianBlur(), ], p=0.2),\n","        # A.OneOf([\n","        #     A.GridDistortion(num_steps=5, distort_limit=0.05, p=1.0),\n","        #     A.OpticalDistortion(distort_limit=0.05, shift_limit=0.05, p=1.0),\n","        #     A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=1.0)\n","        # ], p=0.25),\n","        # A.CoarseDropout(max_holes=8, max_height=image_size//20, max_width=image_size//20,\n","        #                  min_holes=5, fill_value=0, mask_fill_value=0, p=0.5),\n","        # A.Normalize(mean=0, std=1),\n","        ToTensorV2(),], p=1.0),\n","    \n","    \"valid\": A.Compose([\n","        # IsotropicResize(max_side =image_size),\n","        # A.PadIfNeeded(min_height=image_size, min_width=image_size, border_mode=cv2.BORDER_CONSTANT),\n","        # A.Normalize(mean=0, std=1),\n","        # A.Resize(image_size, image_size),\n","        ToTensorV2(),\n","        ], p=1.0)\n","}\n","\n","LOGGER.info(f\"train transform{data_transforms['train']}\")\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-12-30T08:23:20.915927Z","iopub.status.busy":"2022-12-30T08:23:20.915346Z","iopub.status.idle":"2022-12-30T08:23:20.931477Z","shell.execute_reply":"2022-12-30T08:23:20.930433Z","shell.execute_reply.started":"2022-12-30T08:23:20.915890Z"},"trusted":true},"outputs":[],"source":["# from albumentations import DualTransform\n","# image_size = 1024\n","# def isotropically_resize_image(img, size, interpolation_down=cv2.INTER_AREA, interpolation_up=cv2.INTER_CUBIC):\n","#     h, w = img.shape[:2]\n","#     if max(w, h) == size:\n","#         return img\n","#     if w > h:\n","#         scale = size / w\n","#         h = h * scale\n","#         w = size\n","#     else:\n","#         scale = size / h\n","#         w = w * scale\n","#         h = size\n","#     interpolation = interpolation_up if scale > 1 else interpolation_down\n","#     resized = cv2.resize(img, (int(w), int(h)), interpolation=interpolation)\n","#     return resized\n","\n","\n","# class IsotropicResize(DualTransform):\n","#     def __init__(self, max_side, interpolation_down=cv2.INTER_AREA, interpolation_up=cv2.INTER_CUBIC,\n","#                  always_apply=False, p=1):\n","#         super(IsotropicResize, self).__init__(always_apply, p)\n","#         self.max_side = max_side\n","#         self.interpolation_down = interpolation_down\n","#         self.interpolation_up = interpolation_up\n","\n","#     def apply(self, img, interpolation_down=cv2.INTER_AREA, interpolation_up=cv2.INTER_CUBIC, **params):\n","#         return isotropically_resize_image(img, size=self.max_side, interpolation_down=interpolation_down,\n","#                                           interpolation_up=interpolation_up)\n","\n","#     def apply_to_mask(self, img, **params):\n","#         return self.apply(img, interpolation_down=cv2.INTER_NEAREST, interpolation_up=cv2.INTER_NEAREST, **params)\n","\n","#     def get_transform_init_args_names(self):\n","#         return (\"max_side\", \"interpolation_down\", \"interpolation_up\")\n","    \n","# data_transforms = {\n","#     \"train\": A.Compose([\n","# #         A.Resize(image_size, image_size),\n","#         # IsotropicResize(max_side = image_size),\n","#        A.PadIfNeeded(min_width=image_size, border_mode=cv2.BORDER_CONSTANT),\n","#         albumentations.HorizontalFlip(p=0.5),\n","#         # albumentations.VerticalFlip(p=0.5),\n","#         albumentations.RandomBrightness(limit=0.2, p=0.75),\n","#         albumentations.RandomContrast(limit=0.2, p=0.75),\n","\n","#         albumentations.OneOf([\n","#             albumentations.OpticalDistortion(distort_limit=1.),\n","#             albumentations.GridDistortion(num_steps=5, distort_limit=1.),\n","#         ], p=0.75),\n","\n","#         albumentations.HueSaturationValue(hue_shift_limit=40, sat_shift_limit=40, val_shift_limit=0, p=0.75),\n","#         albumentations.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.3, rotate_limit=30, border_mode=0, p=0.75),\n","#         A.Cutout(always_apply=False, p=0.5, num_holes=1, max_h_size=409, max_w_size=409),\n","#         # A.RandomBrightnessContrast(),\n","#         # A.VerticalFlip(p=0.5),   \n","#         A.ColorJitter(p = 0.7),\n","#         # A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.05, rotate_limit=10, p=0.5),\n","#         # A.HorizontalFlip(p=0.5),\n","#         # A.Cutout(max_h_size=int(image_size * 0.1), max_w_size=int(image_size * 0.1), num_holes=5, p=0.5),\n","#         # albumentations.RandomBrightness(limit=0.2, p=0.75),\n","#         # albumentations.RandomContrast(limit=0.2, p=0.75),\n","\n","#         # albumentations.OneOf([\n","#         #     albumentations.OpticalDistortion(distort_limit=1.),\n","#         #     albumentations.GridDistortion(num_steps=5, distort_limit=1.),\n","#         # ], p=0.75),\n","\n","#         # albumentations.HueSaturationValue(hue_shift_limit=40, sat_shift_limit=40, val_shift_limit=0, p=0.75),\n","#         # albumentations.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.3, rotate_limit=30, border_mode=0, p=0.75),\n","#         # A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=10, val_shift_limit=10, p=0.7),\n","#         # A.RandomBrightnessContrast(brightness_limit=(-0.2,0.2), contrast_limit=(-0.2, 0.2), p=0.7),\n","#         # A.CLAHE(p=0.5),\n","#         # albumentations.OneOf([\n","#         # albumentations.OpticalDistortion(distort_limit=1.),\n","#         # albumentations.GridDistortion(num_steps=5, distort_limit=1.),\n","#         # ], p=0.75),\n","#         # A.OneOf([\n","#         # A.GaussianBlur(),\n","#         # A.MotionBlur(),\n","#         # A.MedianBlur(), ], p=0.5),\n","#         # A.IAASharpen(p = 0.2),\n","#         # A.JpegCompression(p=0.2),\n","#         # A.Downscale(scale_min=0.5, scale_max=0.75),\n","#         # A.OneOf([ A.JpegCompression(), A.Downscale(scale_min=0.1, scale_max=0.15), ], p=0.2), \n","#         # A.IAAPiecewiseAffine(),\n","# #         A.OneOf([ \n","# #         A.OpticalDistortion(distort_limit=1.0), \n","# #         A.GridDistortion(num_steps=5, distort_limit=1.),\n","# #         A.ElasticTransform(alpha=3), ], p=0.2),\n","# #         A.OneOf([\n","# #             A.GaussNoise(var_limit=[10, 50]),\n","# #             A.GaussianBlur(),\n","# #             A.MotionBlur(),\n","# #             A.MedianBlur(), ], p=0.2),\n","#         # A.OneOf([\n","#         #     A.GridDistortion(num_steps=5, distort_limit=0.05, p=1.0),\n","#         #     A.OpticalDistortion(distort_limit=0.05, shift_limit=0.05, p=1.0),\n","#         #     A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=1.0)\n","#         # ], p=0.25),\n","#         # A.CoarseDropout(max_holes=8, max_height=image_size//20, max_width=image_size//20,\n","#         #                  min_holes=5, fill_value=0, mask_fill_value=0, p=0.5),\n","#         # A.Normalize(mean=0, std=1),\n","#         ToTensorV2(),], p=1.0),\n","    \n","#     \"valid\": A.Compose([\n","#         # IsotropicResize(max_side = image_size),\n","#         A.PadIfNeeded(min_height=image_size, min_width=image_size, border_mode=cv2.BORDER_CONSTANT),\n","#         # A.Normalize(mean=0, std=1),\n","# #         A.Resize(image_size, image_size),\n","#         ToTensorV2(),\n","#         ], p=1.0)\n","# }\n","\n","# LOGGER.info(f\"train transform{data_transforms['train']}\")\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-12-30T08:23:20.934703Z","iopub.status.busy":"2022-12-30T08:23:20.933649Z","iopub.status.idle":"2022-12-30T08:23:21.010802Z","shell.execute_reply":"2022-12-30T08:23:21.009678Z","shell.execute_reply.started":"2022-12-30T08:23:20.934663Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([3, 1344, 840]) tensor(0)\n","tensor(253.)\n"]}],"source":["def pad(array, target_shape):\n","    return np.pad(\n","        array,\n","        [(0, target_shape[i] - array.shape[i]) for i in range(len(array.shape))],\n","        \"constant\",\n","    )\n","    \n","def load_img(img_path):\n","    image = cv2.imread(img_path)\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    # image = pad(image, (1024, 800, 3))\n","        # img = img.reshape((*resize))\n","    return image\n","#     image = cv2.resize(image, (320, 320), cv2.INTER_NEAREST)\n","#     image = image.astype(np.float32)\n","#     mx = np.max(image)\n","#     if mx:\n","#         image/=mx\n","#     image = image /255.0\n","    \n","    return image\n","class BreastDataset(Dataset):\n","    def __init__(self, df, transforms=None):\n","        self.df = df\n","        self.transforms = transforms\n","        \n","    def __getitem__(self, index):\n","        row = self.df.iloc[index]\n","        img_path = f\"flip/{row.patient_id}_{row.image_id}.png\"\n","        img = load_img(img_path)\n","        label = row['cancer']\n","        # img = np.transpose(img, (2, 0, 1))\n","        data = self.transforms(image=img)\n","        img  = data['image']\n","        # img = img/255.0\n","        return torch.tensor(img).float(), torch.tensor(label).long()\n","        \n","    def __len__(self):\n","        return len(self.df)\n","    \n","fold0 = df[df['fold']==0]\n","train_dataset = BreastDataset(fold0, transforms = data_transforms['train'])\n","image, label = train_dataset[0]\n","print(image.shape, label)\n","print(image.max())"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["\n","# from pylab import rcParams\n","\n","# f, axarr = plt.subplots(1,15, figsize = (20, 20))\n","# imgs = []\n","# for p in range(15):\n","#     img, label = train_dataset[p]\n","#     img = img.transpose(0, 1).transpose(1,2).cpu().numpy()\n","#     img = img.astype(np.uint8)\n","#     imgs.append(img)\n","#     axarr[p].imshow(img)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-12-30T08:23:21.012682Z","iopub.status.busy":"2022-12-30T08:23:21.012267Z","iopub.status.idle":"2022-12-30T08:23:21.020148Z","shell.execute_reply":"2022-12-30T08:23:21.019023Z","shell.execute_reply.started":"2022-12-30T08:23:21.012626Z"},"trusted":true},"outputs":[],"source":["class Model(nn.Module):\n","    def __init__(self, model_name):\n","        super().__init__()\n","        # ,drop_rate = 0.3, drop_path_rate = 0.2\n","        self.backbone = timm.create_model(model_name, pretrained=True,drop_rate = 0.3, drop_path_rate = 0.2)\n","        self.fc = nn.Linear(self.backbone.classifier.in_features,2)\n","        self.backbone.classifier = nn.Identity()\n","        self.dropout = nn.Dropout(0.5)\n","    def forward(self, x):\n","        x = self.backbone(x)\n","        x = self.fc(self.dropout(x))\n","        return x\n","\n","class ModelNextVit(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        # ,drop_rate = 0.3, drop_path_rate = 0.2\n","        self.checkpoint = torch.load('nextvit_small_in1k_384.pth')\n","        self.backbone = nextvit_small()\n","        self.backbone.load_state_dict(self.checkpoint['model'])\n","        self.backbone.proj_head = nn.Linear(1024, 2)\n","\n","    def forward(self, x):\n","        x = self.backbone(x)\n","        return x"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-12-30T08:23:21.022923Z","iopub.status.busy":"2022-12-30T08:23:21.022147Z","iopub.status.idle":"2022-12-30T08:23:21.032555Z","shell.execute_reply":"2022-12-30T08:23:21.031346Z","shell.execute_reply.started":"2022-12-30T08:23:21.022887Z"},"trusted":true},"outputs":[],"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim.optimizer import Optimizer, required\n","import math\n","\n","class AdamP(Optimizer):\n","    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n","                 weight_decay=0, delta=0.1, wd_ratio=0.1, nesterov=False):\n","        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,\n","                        delta=delta, wd_ratio=wd_ratio, nesterov=nesterov)\n","        super(AdamP, self).__init__(params, defaults)\n","\n","    def _channel_view(self, x):\n","        return x.view(x.size(0), -1)\n","\n","    def _layer_view(self, x):\n","        return x.view(1, -1)\n","\n","    def _cosine_similarity(self, x, y, eps, view_func):\n","        x = view_func(x)\n","        y = view_func(y)\n","\n","        return F.cosine_similarity(x, y, dim=1, eps=eps).abs_()\n","\n","    def _projection(self, p, grad, perturb, delta, wd_ratio, eps):\n","        wd = 1\n","        expand_size = [-1] + [1] * (len(p.shape) - 1)\n","        for view_func in [self._channel_view, self._layer_view]:\n","\n","            cosine_sim = self._cosine_similarity(grad, p.data, eps, view_func)\n","\n","            if cosine_sim.max() < delta / math.sqrt(view_func(p.data).size(1)):\n","                p_n = p.data / view_func(p.data).norm(dim=1).view(expand_size).add_(eps)\n","                perturb -= p_n * view_func(p_n * perturb).sum(dim=1).view(expand_size)\n","                wd = wd_ratio\n","\n","                return perturb, wd\n","\n","        return perturb, wd\n","\n","    def step(self, closure=None):\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","\n","                grad = p.grad.data\n","                beta1, beta2 = group['betas']\n","                nesterov = group['nesterov']\n","\n","                state = self.state[p]\n","\n","                # State initialization\n","                if len(state) == 0:\n","                    state['step'] = 0\n","                    state['exp_avg'] = torch.zeros_like(p.data)\n","                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n","\n","                # Adam\n","                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n","\n","                state['step'] += 1\n","                bias_correction1 = 1 - beta1 ** state['step']\n","                bias_correction2 = 1 - beta2 ** state['step']\n","\n","                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n","                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n","\n","                denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n","                step_size = group['lr'] / bias_correction1\n","\n","                if nesterov:\n","                    perturb = (beta1 * exp_avg + (1 - beta1) * grad) / denom\n","                else:\n","                    perturb = exp_avg / denom\n","\n","                # Projection\n","                wd_ratio = 1\n","                if len(p.shape) > 1:\n","                    perturb, wd_ratio = self._projection(p, grad, perturb, group['delta'], group['wd_ratio'], group['eps'])\n","\n","                # Weight decay\n","                if group['weight_decay'] > 0:\n","                    p.data.mul_(1 - group['lr'] * group['weight_decay'] * wd_ratio)\n","\n","                # Step\n","                p.data.add_(perturb, alpha=-step_size)\n","\n","        return loss\n","\n","class SGDP(Optimizer):\n","    def __init__(self, params, lr=required, momentum=0, dampening=0,\n","                 weight_decay=0, nesterov=False, eps=1e-8, delta=0.1, wd_ratio=0.1):\n","        defaults = dict(lr=lr, momentum=momentum, dampening=dampening, weight_decay=weight_decay,\n","                        nesterov=nesterov, eps=eps, delta=delta, wd_ratio=wd_ratio)\n","        super(SGDP, self).__init__(params, defaults)\n","\n","    def _channel_view(self, x):\n","        return x.view(x.size(0), -1)\n","\n","    def _layer_view(self, x):\n","        return x.view(1, -1)\n","\n","    def _cosine_similarity(self, x, y, eps, view_func):\n","        x = view_func(x)\n","        y = view_func(y)\n","\n","        return F.cosine_similarity(x, y, dim=1, eps=eps).abs_()\n","\n","    def _projection(self, p, grad, perturb, delta, wd_ratio, eps):\n","        wd = 1\n","        expand_size = [-1] + [1] * (len(p.shape) - 1)\n","        for view_func in [self._channel_view, self._layer_view]:\n","\n","            cosine_sim = self._cosine_similarity(grad, p.data, eps, view_func)\n","\n","            if cosine_sim.max() < delta / math.sqrt(view_func(p.data).size(1)):\n","                p_n = p.data / view_func(p.data).norm(dim=1).view(expand_size).add_(eps)\n","                perturb -= p_n * view_func(p_n * perturb).sum(dim=1).view(expand_size)\n","                wd = wd_ratio\n","\n","                return perturb, wd\n","\n","        return perturb, wd\n","\n","    def step(self, closure=None):\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","            momentum = group['momentum']\n","            dampening = group['dampening']\n","            nesterov = group['nesterov']\n","\n","            for p in group['params']:\n","                if p.grad is None:\n","                    continue\n","                grad = p.grad.data\n","                state = self.state[p]\n","\n","                # State initialization\n","                if len(state) == 0:\n","                    state['momentum'] = torch.zeros_like(p.data)\n","\n","                # SGD\n","                buf = state['momentum']\n","                buf.mul_(momentum).add_(grad, alpha=1 - dampening)\n","                if nesterov:\n","                    d_p = grad + momentum * buf\n","                else:\n","                    d_p = buf\n","\n","                # Projection\n","                wd_ratio = 1\n","                if len(p.shape) > 1:\n","                    d_p, wd_ratio = self._projection(p, grad, d_p, group['delta'], group['wd_ratio'], group['eps'])\n","\n","                # Weight decay\n","                if group['weight_decay'] > 0:\n","                    p.data.mul_(1 - group['lr'] * group['weight_decay'] * wd_ratio / (1-momentum))\n","\n","                # Step\n","                p.data.add_(d_p, alpha=-group['lr'])\n","\n","        return loss\n","\n","class SAM(torch.optim.Optimizer):\n","    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\n","        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n","\n","        defaults = dict(rho=rho, **kwargs)\n","        super(SAM, self).__init__(params, defaults)\n","\n","        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n","        self.param_groups = self.base_optimizer.param_groups\n","\n","    @torch.no_grad()\n","    def first_step(self, zero_grad=False):\n","        grad_norm = self._grad_norm()\n","        for group in self.param_groups:\n","            scale = group[\"rho\"] / (grad_norm + 1e-12)\n","\n","            for p in group[\"params\"]:\n","                if p.grad is None: continue\n","                e_w = p.grad * scale.to(p)\n","                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n","                self.state[p][\"e_w\"] = e_w\n","\n","        if zero_grad: self.zero_grad()\n","\n","    @torch.no_grad()\n","    def second_step(self, zero_grad=False):\n","        for group in self.param_groups:\n","            for p in group[\"params\"]:\n","                if p.grad is None: continue\n","                p.sub_(self.state[p][\"e_w\"])  # get back to \"w\" from \"w + e(w)\"\n","\n","        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n","\n","        if zero_grad: self.zero_grad()\n","\n","    def step(self, closure=None):\n","        raise NotImplementedError(\"SAM doesn't work like the other optimizers, you should first call `first_step` and the `second_step`; see the documentation for more info.\")\n","\n","    def _grad_norm(self):\n","        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n","        norm = torch.norm(\n","                    torch.stack([\n","                        p.grad.norm(p=2).to(shared_device)\n","                        for group in self.param_groups for p in group[\"params\"]\n","                        if p.grad is not None\n","                    ]),\n","                    p=2\n","               )\n","        return norm"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["from torch.optim.lr_scheduler import _LRScheduler\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","\n","\n","class GradualWarmupScheduler(_LRScheduler):\n","    \"\"\" Gradually warm-up(increasing) learning rate in optimizer.\n","    Proposed in 'Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour'.\n","    Args:\n","        optimizer (Optimizer): Wrapped optimizer.\n","        multiplier: target learning rate = base lr * multiplier if multiplier > 1.0. if multiplier = 1.0, lr starts from 0 and ends up with the base_lr.\n","        total_epoch: target learning rate is reached at total_epoch, gradually\n","        after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)\n","    \"\"\"\n","\n","    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n","        self.multiplier = multiplier\n","        if self.multiplier < 1.:\n","            raise ValueError('multiplier should be greater thant or equal to 1.')\n","        self.total_epoch = total_epoch\n","        self.after_scheduler = after_scheduler\n","        self.finished = False\n","        super(GradualWarmupScheduler, self).__init__(optimizer)\n","\n","    def get_lr(self):\n","        if self.last_epoch > self.total_epoch:\n","            if self.after_scheduler:\n","                if not self.finished:\n","                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n","                    self.finished = True\n","                return self.after_scheduler.get_last_lr()\n","            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n","\n","        if self.multiplier == 1.0:\n","            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n","        else:\n","            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n","\n","    def step_ReduceLROnPlateau(self, metrics, epoch=None):\n","        if epoch is None:\n","            epoch = self.last_epoch + 1\n","        self.last_epoch = epoch if epoch != 0 else 1  # ReduceLROnPlateau is called at the end of epoch, whereas others are called at beginning\n","        if self.last_epoch <= self.total_epoch:\n","            warmup_lr = [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n","            for param_group, lr in zip(self.optimizer.param_groups, warmup_lr):\n","                param_group['lr'] = lr\n","        else:\n","            if epoch is None:\n","                self.after_scheduler.step(metrics, None)\n","            else:\n","                self.after_scheduler.step(metrics, epoch - self.total_epoch)\n","\n","    def step(self, epoch=None, metrics=None):\n","        if type(self.after_scheduler) != ReduceLROnPlateau:\n","            if self.finished and self.after_scheduler:\n","                if epoch is None:\n","                    self.after_scheduler.step(None)\n","                else:\n","                    self.after_scheduler.step(epoch - self.total_epoch)\n","                self._last_lr = self.after_scheduler.get_last_lr()\n","            else:\n","                return super(GradualWarmupScheduler, self).step(epoch)\n","        else:\n","            self.step_ReduceLROnPlateau(metrics, epoch)\n","\n","class GradualWarmupSchedulerV2(GradualWarmupScheduler):\n","    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n","        super(GradualWarmupSchedulerV2, self).__init__(optimizer, multiplier, total_epoch, after_scheduler)\n","    def get_lr(self):\n","        if self.last_epoch > self.total_epoch:\n","            if self.after_scheduler:\n","                if not self.finished:\n","                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n","                    self.finished = True\n","                return self.after_scheduler.get_lr()\n","            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n","        if self.multiplier == 1.0:\n","            return [base_lr * (float(self.last_epoch) / self.total_epoch) for base_lr in self.base_lrs]\n","        else:\n","            return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n","\n","class Lookahead(optim.Optimizer):\n","    def __init__(self, base_optimizer, alpha=0.5, k=6):\n","        if not 0.0 <= alpha <= 1.0:\n","            raise ValueError(f'Invalid slow update rate: {alpha}')\n","        if not 1 <= k:\n","            raise ValueError(f'Invalid lookahead steps: {k}')\n","        defaults = dict(lookahead_alpha=alpha, lookahead_k=k, lookahead_step=0)\n","        self.base_optimizer = base_optimizer\n","        self.param_groups = self.base_optimizer.param_groups\n","        self.defaults = base_optimizer.defaults\n","        self.defaults.update(defaults)\n","        self.state = defaultdict(dict)\n","        # manually add our defaults to the param groups\n","        for name, default in defaults.items():\n","            for group in self.param_groups:\n","                group.setdefault(name, default)\n","\n","    def update_slow(self, group):\n","        for fast_p in group[\"params\"]:\n","            if fast_p.grad is None:\n","                continue\n","            param_state = self.state[fast_p]\n","            if 'slow_buffer' not in param_state:\n","                param_state['slow_buffer'] = torch.empty_like(fast_p.data)\n","                param_state['slow_buffer'].copy_(fast_p.data)\n","            slow = param_state['slow_buffer']\n","            slow.add_(group['lookahead_alpha'], fast_p.data - slow)\n","            fast_p.data.copy_(slow)\n","\n","    def sync_lookahead(self):\n","        for group in self.param_groups:\n","            self.update_slow(group)\n","\n","    def step(self, closure=None):\n","        #assert id(self.param_groups) == id(self.base_optimizer.param_groups)\n","        loss = self.base_optimizer.step(closure)\n","        for group in self.param_groups:\n","            group['lookahead_step'] += 1\n","            if group['lookahead_step'] % group['lookahead_k'] == 0:\n","                self.update_slow(group)\n","        return loss\n","\n","    def state_dict(self):\n","        fast_state_dict = self.base_optimizer.state_dict()\n","        slow_state = {\n","            (id(k) if isinstance(k, torch.Tensor) else k): v\n","            for k, v in self.state.items()\n","        }\n","        fast_state = fast_state_dict['state']\n","        param_groups = fast_state_dict['param_groups']\n","        return {\n","            'state': fast_state,\n","            'slow_state': slow_state,\n","            'param_groups': param_groups,\n","        }\n","\n","    def load_state_dict(self, state_dict):\n","        fast_state_dict = {\n","            'state': state_dict['state'],\n","            'param_groups': state_dict['param_groups'],\n","        }\n","        self.base_optimizer.load_state_dict(fast_state_dict)\n","\n","        # We want to restore the slow state, but share param_groups reference\n","        # with base_optimizer. This is a bit redundant but least code\n","        slow_state_new = False\n","        if 'slow_state' not in state_dict:\n","            print('Loading state_dict from optimizer without Lookahead applied.')\n","            state_dict['slow_state'] = defaultdict(dict)\n","            slow_state_new = True\n","        slow_state_dict = {\n","            'state': state_dict['slow_state'],\n","            'param_groups': state_dict['param_groups'],  # this is pointless but saves code\n","        }\n","        super(Lookahead, self).load_state_dict(slow_state_dict)\n","        self.param_groups = self.base_optimizer.param_groups  # make both ref same container\n","        if slow_state_new:\n","            # reapply defaults to catch missing lookahead specific ones\n","            for name, default in self.defaults.items():\n","                for group in self.param_groups:\n","                    group.setdefault(name, default)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["def log_t(u, t):\n","    \"\"\"Compute log_t for `u'.\"\"\"\n","    if t==1.0:\n","        return u.log()\n","    else:\n","        return (u.pow(1.0 - t) - 1.0) / (1.0 - t)\n","\n","def exp_t(u, t):\n","    \"\"\"Compute exp_t for `u'.\"\"\"\n","    if t==1:\n","        return u.exp()\n","    else:\n","        return (1.0 + (1.0-t)*u).relu().pow(1.0 / (1.0 - t))\n","\n","def compute_normalization_fixed_point(activations, t, num_iters):\n","\n","    \"\"\"Returns the normalization value for each example (t > 1.0).\n","    Args:\n","      activations: A multi-dimensional tensor with last dimension `num_classes`.\n","      t: Temperature 2 (> 1.0 for tail heaviness).\n","      num_iters: Number of iterations to run the method.\n","    Return: A tensor of same shape as activation with the last dimension being 1.\n","    \"\"\"\n","    mu, _ = torch.max(activations, -1, keepdim=True)\n","    normalized_activations_step_0 = activations - mu\n","\n","    normalized_activations = normalized_activations_step_0\n","\n","    for _ in range(num_iters):\n","        logt_partition = torch.sum(\n","                exp_t(normalized_activations, t), -1, keepdim=True)\n","        normalized_activations = normalized_activations_step_0 * \\\n","                logt_partition.pow(1.0-t)\n","\n","    logt_partition = torch.sum(\n","            exp_t(normalized_activations, t), -1, keepdim=True)\n","    normalization_constants = - log_t(1.0 / logt_partition, t) + mu\n","\n","    return normalization_constants\n","\n","def compute_normalization_binary_search(activations, t, num_iters):\n","\n","    \"\"\"Returns the normalization value for each example (t < 1.0).\n","    Args:\n","      activations: A multi-dimensional tensor with last dimension `num_classes`.\n","      t: Temperature 2 (< 1.0 for finite support).\n","      num_iters: Number of iterations to run the method.\n","    Return: A tensor of same rank as activation with the last dimension being 1.\n","    \"\"\"\n","\n","    mu, _ = torch.max(activations, -1, keepdim=True)\n","    normalized_activations = activations - mu\n","\n","    effective_dim = \\\n","        torch.sum(\n","                (normalized_activations > -1.0 / (1.0-t)).to(torch.int32),\n","            dim=-1, keepdim=True).to(activations.dtype)\n","\n","    shape_partition = activations.shape[:-1] + (1,)\n","    lower = torch.zeros(shape_partition, dtype=activations.dtype, device=activations.device)\n","    upper = -log_t(1.0/effective_dim, t) * torch.ones_like(lower)\n","\n","    for _ in range(num_iters):\n","        logt_partition = (upper + lower)/2.0\n","        sum_probs = torch.sum(\n","                exp_t(normalized_activations - logt_partition, t),\n","                dim=-1, keepdim=True)\n","        update = (sum_probs < 1.0).to(activations.dtype)\n","        lower = torch.reshape(\n","                lower * update + (1.0-update) * logt_partition,\n","                shape_partition)\n","        upper = torch.reshape(\n","                upper * (1.0 - update) + update * logt_partition,\n","                shape_partition)\n","\n","    logt_partition = (upper + lower)/2.0\n","    return logt_partition + mu\n","\n","class ComputeNormalization(torch.autograd.Function):\n","    \"\"\"\n","    Class implementing custom backward pass for compute_normalization. See compute_normalization.\n","    \"\"\"\n","    @staticmethod\n","    def forward(ctx, activations, t, num_iters):\n","        if t < 1.0:\n","            normalization_constants = compute_normalization_binary_search(activations, t, num_iters)\n","        else:\n","            normalization_constants = compute_normalization_fixed_point(activations, t, num_iters)\n","\n","        ctx.save_for_backward(activations, normalization_constants)\n","        ctx.t=t\n","        return normalization_constants\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        activations, normalization_constants = ctx.saved_tensors\n","        t = ctx.t\n","        normalized_activations = activations - normalization_constants \n","        probabilities = exp_t(normalized_activations, t)\n","        escorts = probabilities.pow(t)\n","        escorts = escorts / escorts.sum(dim=-1, keepdim=True)\n","        grad_input = escorts * grad_output\n","        \n","        return grad_input, None, None\n","\n","def compute_normalization(activations, t, num_iters=5):\n","    \"\"\"Returns the normalization value for each example. \n","    Backward pass is implemented.\n","    Args:\n","      activations: A multi-dimensional tensor with last dimension `num_classes`.\n","      t: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n","      num_iters: Number of iterations to run the method.\n","    Return: A tensor of same rank as activation with the last dimension being 1.\n","    \"\"\"\n","    return ComputeNormalization.apply(activations, t, num_iters)\n","\n","def tempered_sigmoid(activations, t, num_iters = 5):\n","    \"\"\"Tempered sigmoid function.\n","    Args:\n","      activations: Activations for the positive class for binary classification.\n","      t: Temperature tensor > 0.0.\n","      num_iters: Number of iterations to run the method.\n","    Returns:\n","      A probabilities tensor.\n","    \"\"\"\n","    internal_activations = torch.stack([activations,\n","        torch.zeros_like(activations)],\n","        dim=-1)\n","    internal_probabilities = tempered_softmax(internal_activations, t, num_iters)\n","    return internal_probabilities[..., 0]\n","\n","\n","def tempered_softmax(activations, t, num_iters=5):\n","    \"\"\"Tempered softmax function.\n","    Args:\n","      activations: A multi-dimensional tensor with last dimension `num_classes`.\n","      t: Temperature > 1.0.\n","      num_iters: Number of iterations to run the method.\n","    Returns:\n","      A probabilities tensor.\n","    \"\"\"\n","    if t == 1.0:\n","        return activations.softmax(dim=-1)\n","\n","    normalization_constants = compute_normalization(activations, t, num_iters)\n","    return exp_t(activations - normalization_constants, t)\n","\n","def bi_tempered_binary_logistic_loss(activations,\n","        labels,\n","        t1,\n","        t2,\n","        label_smoothing = 0.0,\n","        num_iters=5,\n","        reduction='mean'):\n","\n","    \"\"\"Bi-Tempered binary logistic loss.\n","    Args:\n","      activations: A tensor containing activations for class 1.\n","      labels: A tensor with shape as activations, containing probabilities for class 1\n","      t1: Temperature 1 (< 1.0 for boundedness).\n","      t2: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n","      label_smoothing: Label smoothing\n","      num_iters: Number of iterations to run the method.\n","    Returns:\n","      A loss tensor.\n","    \"\"\"\n","    internal_activations = torch.stack([activations,\n","        torch.zeros_like(activations)],\n","        dim=-1)\n","    internal_labels = torch.stack([labels.to(activations.dtype),\n","        1.0 - labels.to(activations.dtype)],\n","        dim=-1)\n","    return bi_tempered_logistic_loss(internal_activations, \n","            internal_labels,\n","            t1,\n","            t2,\n","            label_smoothing = label_smoothing,\n","            num_iters = num_iters,\n","            reduction = reduction)\n","\n","def bi_tempered_logistic_loss(activations,\n","        labels,\n","        t1,\n","        t2,\n","        label_smoothing=0.0,\n","        num_iters=5,\n","        reduction = 'mean'):\n","\n","    \"\"\"Bi-Tempered Logistic Loss.\n","    Args:\n","      activations: A multi-dimensional tensor with last dimension `num_classes`.\n","      labels: A tensor with shape and dtype as activations (onehot), \n","        or a long tensor of one dimension less than activations (pytorch standard)\n","      t1: Temperature 1 (< 1.0 for boundedness).\n","      t2: Temperature 2 (> 1.0 for tail heaviness, < 1.0 for finite support).\n","      label_smoothing: Label smoothing parameter between [0, 1). Default 0.0.\n","      num_iters: Number of iterations to run the method. Default 5.\n","      reduction: ``'none'`` | ``'mean'`` | ``'sum'``. Default ``'mean'``.\n","        ``'none'``: No reduction is applied, return shape is shape of\n","        activations without the last dimension.\n","        ``'mean'``: Loss is averaged over minibatch. Return shape (1,)\n","        ``'sum'``: Loss is summed over minibatch. Return shape (1,)\n","    Returns:\n","      A loss tensor.\n","    \"\"\"\n","\n","    if len(labels.shape)<len(activations.shape): #not one-hot\n","        labels_onehot = torch.zeros_like(activations)\n","        labels_onehot.scatter_(1, labels[..., None], 1)\n","    else:\n","        labels_onehot = labels\n","\n","    if label_smoothing > 0:\n","        num_classes = labels_onehot.shape[-1]\n","        labels_onehot = ( 1 - label_smoothing * num_classes / (num_classes - 1) ) \\\n","                * labels_onehot + \\\n","                label_smoothing / (num_classes - 1)\n","\n","    probabilities = tempered_softmax(activations, t2, num_iters)\n","\n","    loss_values = labels_onehot * log_t(labels_onehot + 1e-10, t1) \\\n","            - labels_onehot * log_t(probabilities, t1) \\\n","            - labels_onehot.pow(2.0 - t1) / (2.0 - t1) \\\n","            + probabilities.pow(2.0 - t1) / (2.0 - t1)\n","    loss_values = loss_values.sum(dim = -1) #sum over classes\n","\n","    if reduction == 'none':\n","        return loss_values\n","    if reduction == 'sum':\n","        return loss_values.sum()\n","    if reduction == 'mean':\n","        return loss_values.mean()\n","\n","class BiTemperedLogisticLoss(nn.Module): \n","    def __init__(self, t1, t2, smoothing=0.0): \n","        super(BiTemperedLogisticLoss, self).__init__() \n","        self.t1 = t1\n","        self.t2 = t2\n","        self.smoothing = smoothing\n","    def forward(self, logit_label, truth_label):\n","        loss_label = bi_tempered_logistic_loss(\n","            logit_label, truth_label,\n","            t1=self.t1, t2=self.t2,\n","            label_smoothing=self.smoothing,\n","            reduction='none'\n","        )\n","        \n","        loss_label = loss_label.mean()\n","        return loss_label"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-12-30T08:23:21.036233Z","iopub.status.busy":"2022-12-30T08:23:21.035928Z","iopub.status.idle":"2022-12-30T08:23:21.059619Z","shell.execute_reply":"2022-12-30T08:23:21.058523Z","shell.execute_reply.started":"2022-12-30T08:23:21.036197Z"},"trusted":true},"outputs":[],"source":["\n","def train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device):\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","    batch_time = AverageMeter()\n","    data_time = AverageMeter()\n","    losses = AverageMeter()\n","    # switch to train mode\n","    model.train()\n","    start = end = time.time()\n","    truth = []\n","    pred = []\n","    global_step = 0\n","    scaler = GradScaler()\n","    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc='Train')\n","    for step, (images, labels) in pbar:\n","        optimizer.zero_grad()\n","        data_time.update(time.time() - end)\n","        images = images.to(device)\n","        \n","        \n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","        with autocast():\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            # loss.backward()\n","            # optimizer.first_step(zero_grad=True)\n","            # criterion(model(images), labels).backward()\n","            # optimizer.second_step(zero_grad=True)\n","            # record loss\n","        losses.update(loss.item(), batch_size)\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        # global_step += 1\n","        scheduler.step()\n","            # measure elapsed time\n","        batch_time.update(time.time() - end)\n","        end = time.time()\n","#         if step % 100 == 0 or step == (len(train_loader)-1):\n","#             print('Epoch: [{0}][{1}/{2}] '\n","#                       'Data {data_time.val:.6f} ({data_time.avg:.6f}) '\n","#                       'Elapsed {remain:s} '\n","#                       'Loss: {loss.val:.6f}({loss.avg:.6f}) '\n","#                       'LR: {lr:.6f}  '\n","#                       .format(\n","#                        epoch+1, step, len(train_loader), batch_time=batch_time,\n","#                        data_time=data_time, loss=losses,\n","#                        remain=timeSince(start, float(step+1)/len(train_loader)),\n","#                        lr=scheduler.get_lr()[0],\n","#                        ))\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n","        current_lr = optimizer.param_groups[0]['lr']\n","        pbar.set_postfix(train_loss=f'{losses.avg:0.4f}',\n","                        lr=f'{current_lr:0.8f}',\n","                        gpu_mem=f'{mem:0.2f} GB')\n","\n","    return losses.avg\n","\n","def valid_fn_no_sigmoid(val_dataloader, model, criterion, device):\n","    losses = AverageMeter()\n","    model.eval()\n","    truth = []\n","    preds = []\n","    valid_labels = []\n","    start = end = time.time()\n","    pbar = tqdm(enumerate(val_dataloader), total=len(val_dataloader), desc='Val')\n","    for step, (images, labels) in pbar:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","        with torch.no_grad():\n","            outputs = model(images)\n","        valid_labels.append(labels.cpu().numpy())\n","        loss = criterion(outputs, labels)\n","#         loss = bi_tempered_logistic_loss(outputs, labels, t1=0.8, t2 = 1.4)\n","        losses.update(loss.item(), batch_size)\n","#         print(outputs)\n","        preds.append((outputs).to('cpu').numpy())\n","        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n","        pbar.set_postfix(eval_loss=f'{losses.avg:0.4f}',\n","                        gpu_mem=f'{mem:0.2f} GB')\n","    predictions = np.concatenate(preds)\n","    valid_labels = np.concatenate(valid_labels)\n","    return losses.avg, predictions, valid_labels\n","\n","\n","def valid_fn(val_dataloader, model, criterion, device):\n","    losses = AverageMeter()\n","    model.eval()\n","    truth = []\n","    preds = []\n","    valid_labels = []\n","    start = end = time.time()\n","    pbar = tqdm(enumerate(val_dataloader), total=len(val_dataloader), desc='Val')\n","    for step, (images, labels) in pbar:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","        with torch.no_grad():\n","            outputs = model(images)\n","        valid_labels.append(labels.cpu().numpy())\n","        loss = criterion(outputs, labels)\n","#         loss = bi_tempered_logistic_loss(outputs, labels, t1=0.8, t2 = 1.4)\n","        losses.update(loss.item(), batch_size)\n","#         print(outputs)\n","        preds.append(torch.sigmoid(outputs).to('cpu').numpy())\n","        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n","        pbar.set_postfix(eval_loss=f'{losses.avg:0.4f}',\n","                        gpu_mem=f'{mem:0.2f} GB')\n","    predictions = np.concatenate(preds)\n","    valid_labels = np.concatenate(valid_labels)\n","    return losses.avg, predictions, valid_labels\n","def valid_fn_two(val_dataloader, model, criterion, device):\n","    losses = AverageMeter()\n","    model.eval()\n","    truth = []\n","    preds = []\n","    valid_labels = []\n","    start = end = time.time()\n","    pbar = tqdm(enumerate(val_dataloader), total=len(val_dataloader), desc='Val')\n","    for step, (images, labels) in pbar:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","        with torch.no_grad():\n","            outputs = model(images)\n","        valid_labels.append(labels.cpu().numpy())\n","        loss = criterion(outputs, labels)\n","#         loss = bi_tempered_logistic_loss(outputs, labels, t1=0.8, t2 = 1.4)\n","        losses.update(loss.item(), batch_size)\n","#         print(outputs)\n","        preds.append(F.softmax(outputs).to('cpu').numpy())\n","        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n","        pbar.set_postfix(eval_loss=f'{losses.avg:0.4f}',\n","                        gpu_mem=f'{mem:0.2f} GB')\n","    predictions = np.concatenate(preds)\n","    valid_labels = np.concatenate(valid_labels)\n","    return losses.avg, predictions, valid_labels\n","\n","def valid_fn_two_flip(val_dataloader, model, criterion, device):\n","    losses = AverageMeter()\n","    model.eval()\n","    truth = []\n","    preds = []\n","    valid_labels = []\n","    start = end = time.time()\n","    pbar = tqdm(enumerate(val_dataloader), total=len(val_dataloader), desc='Val')\n","    for step, (images, labels) in pbar:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","        images = torch.flip(images, [3])\n","        with torch.no_grad():\n","            outputs = model(images)\n","        valid_labels.append(labels.cpu().numpy())\n","        loss = criterion(outputs, labels)\n","#         loss = bi_tempered_logistic_loss(outputs, labels, t1=0.8, t2 = 1.4)\n","        losses.update(loss.item(), batch_size)\n","#         print(outputs)\n","        preds.append(F.softmax(outputs).to('cpu').numpy())\n","        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n","        pbar.set_postfix(eval_loss=f'{losses.avg:0.4f}',\n","                        gpu_mem=f'{mem:0.2f} GB')\n","    predictions = np.concatenate(preds)\n","    valid_labels = np.concatenate(valid_labels)\n","    return losses.avg, predictions, valid_labels\n","\n","def valid_fn_flip(val_dataloader, model, criterion, device):\n","    losses = AverageMeter()\n","    model.eval()\n","    truth = []\n","    preds = []\n","    valid_labels = []\n","    start = end = time.time()\n","    pbar = tqdm(enumerate(val_dataloader), total=len(val_dataloader), desc='Val')\n","    for step, (images, labels) in pbar:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","        images = torch.flip(images, [3])\n","        with torch.no_grad():\n","            outputs = model(images)\n","        valid_labels.append(labels.cpu().numpy())\n","        loss = criterion(outputs, labels)\n","#         loss = bi_tempered_logistic_loss(outputs, labels, t1=0.8, t2 = 1.4)\n","        losses.update(loss.item(), batch_size)\n","#         print(outputs)\n","        preds.append(torch.sigmoid(outputs).to('cpu').numpy())\n","        mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0\n","        pbar.set_postfix(eval_loss=f'{losses.avg:0.4f}',\n","                        gpu_mem=f'{mem:0.2f} GB')\n","    predictions = np.concatenate(preds)\n","    valid_labels = np.concatenate(valid_labels)\n","    return losses.avg, predictions, valid_labels"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-12-30T08:32:56.198968Z","iopub.status.busy":"2022-12-30T08:32:56.198538Z","iopub.status.idle":"2022-12-30T08:38:54.946641Z","shell.execute_reply":"2022-12-30T08:38:54.945288Z","shell.execute_reply.started":"2022-12-30T08:32:56.198933Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Fold: 0\n","Model name: tf_efficientnetv2_b2\n"]},{"name":"stdout","output_type":"stream","text":["> SEEDING DONE\n"]},{"name":"stderr","output_type":"stream","text":["Len train df: 50278\n","Len valid df: 5471\n","Train bs: 16\n","optimizer: AdamW (\n","Parameter Group 0\n","    amsgrad: False\n","    betas: (0.9, 0.999)\n","    capturable: False\n","    eps: 1e-08\n","    foreach: None\n","    initial_lr: 0.0001\n","    lr: 0.0\n","    maximize: False\n","    weight_decay: 0.0005\n",")\n","total_epoch :13\n","Epoch: 2/13\n","Train:   4%|         | 136/3142 [01:24<31:18,  1.60it/s, gpu_mem=0.48 GB, lr=0.00000433, train_loss=0.7055]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 125\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m, total_epoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m    119\u001b[0m     \u001b[39m# if epoch >=7:\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     \u001b[39m#     swa_model.update_parameters(model)\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[39m#     swa_scheduler.step()\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[39m# else:\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     \u001b[39m# scheduler.step(epoch-1)\u001b[39;00m\n\u001b[1;32m    124\u001b[0m     LOGGER\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mtotal_epoch\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 125\u001b[0m     loss_train \u001b[39m=\u001b[39m train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, CFG\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    126\u001b[0m     \u001b[39m# state = {'epoch': epoch, 'state_dict': model.state_dict(),'optimizer': optimizer.state_dict(), 'scheduler':scheduler.state_dict()}\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     \u001b[39m# path = f'{CFG.model_name}_fold_{fold}_model_epoch_{epoch}.pth'\u001b[39;00m\n\u001b[1;32m    128\u001b[0m     \u001b[39m# torch.save(state, path\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     loss_valid, valid_preds, valid_labels \u001b[39m=\u001b[39m valid_fn_two(valid_loader, model, criterion, CFG\u001b[39m.\u001b[39mdevice)\n","Cell \u001b[0;32mIn[16], line 24\u001b[0m, in \u001b[0;36mtrain_fn\u001b[0;34m(train_loader, model, criterion, optimizer, epoch, scheduler, device)\u001b[0m\n\u001b[1;32m     22\u001b[0m batch_size \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[39mwith\u001b[39;00m autocast():\n\u001b[0;32m---> 24\u001b[0m     outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     25\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     26\u001b[0m     \u001b[39m# loss.backward()\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[39m# optimizer.first_step(zero_grad=True)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[39m# criterion(model(images), labels).backward()\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[39m# optimizer.second_step(zero_grad=True)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     \u001b[39m# record loss\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/zaloenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[11], line 10\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 10\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(x)\n\u001b[1;32m     11\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x))\n\u001b[1;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n","File \u001b[0;32m~/miniconda3/envs/zaloenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/miniconda3/envs/zaloenv/lib/python3.10/site-packages/timm/models/efficientnet.py:557\u001b[0m, in \u001b[0;36mEfficientNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 557\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_features(x)\n\u001b[1;32m    558\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_head(x)\n\u001b[1;32m    559\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n","File \u001b[0;32m~/miniconda3/envs/zaloenv/lib/python3.10/site-packages/timm/models/efficientnet.py:545\u001b[0m, in \u001b[0;36mEfficientNet.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    543\u001b[0m     x \u001b[39m=\u001b[39m checkpoint_seq(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks, x, flatten\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    544\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 545\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblocks(x)\n\u001b[1;32m    546\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_head(x)\n\u001b[1;32m    547\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(x)\n","File \u001b[0;32m~/miniconda3/envs/zaloenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/miniconda3/envs/zaloenv/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n","File \u001b[0;32m~/miniconda3/envs/zaloenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/miniconda3/envs/zaloenv/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n","File \u001b[0;32m~/miniconda3/envs/zaloenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/miniconda3/envs/zaloenv/lib/python3.10/site-packages/timm/models/efficientnet_blocks.py:185\u001b[0m, in \u001b[0;36mInvertedResidual.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    183\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_dw(x)\n\u001b[1;32m    184\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(x)\n\u001b[0;32m--> 185\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mse(x)\n\u001b[1;32m    186\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_pwl(x)\n\u001b[1;32m    187\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn3(x)\n","File \u001b[0;32m~/miniconda3/envs/zaloenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/miniconda3/envs/zaloenv/lib/python3.10/site-packages/timm/models/efficientnet_blocks.py:53\u001b[0m, in \u001b[0;36mSqueezeExcite.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     52\u001b[0m     x_se \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mmean((\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m), keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 53\u001b[0m     x_se \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv_reduce(x_se)\n\u001b[1;32m     54\u001b[0m     x_se \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact1(x_se)\n\u001b[1;32m     55\u001b[0m     x_se \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_expand(x_se)\n","File \u001b[0;32m~/miniconda3/envs/zaloenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/miniconda3/envs/zaloenv/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n","File \u001b[0;32m~/miniconda3/envs/zaloenv/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from exhaustive_weighted_random_sampler import ExhaustiveWeightedRandomSampler\n","def pfbeta(labels, predictions, beta=1):\n","    y_true_count = 0\n","    ctp = 0\n","    cfp = 0\n","\n","    for idx in range(len(labels)):\n","        prediction = min(max(predictions[idx], 0), 1)\n","        if (labels[idx]):\n","            y_true_count += 1\n","            ctp += prediction\n","        else:\n","            cfp += prediction\n","\n","    beta_squared = beta * beta\n","    c_precision = ctp / (ctp + cfp)\n","    c_recall = ctp / y_true_count\n","    if (c_precision > 0 and c_recall > 0):\n","        result = (1 + beta_squared) * (c_precision * c_recall) / (beta_squared * c_precision + c_recall)\n","        return result\n","    else:\n","        return 0\n","    \n","def dfs_freeze(module):\n","    for param in module.parameters():\n","        param.requires_grad = False\n","        \n","def dfs_unfreeze(module):\n","    for param in module.parameters():\n","        param.requires_grad = True\n","    \n","def set_seed(seed = 42):\n","    '''Sets the seed of the entire notebook so results are the same every time we run.\n","    This is for REPRODUCIBILITY.'''\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    # When running on the CuDNN backend, two further options must be set\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    # Set a fixed value for the hash seed\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    print('> SEEDING DONE')\n","\n","def sigmoid(x):\n","  return 1 / (1 + math.exp(-x))\n","\n","set_seed(1)\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","gc.collect()\n","torch.cuda.empty_cache()\n","for fold in [0, 1, 2, 3, 4]:\n","    LOGGER.info(f\"Fold: {fold}\")\n","    LOGGER.info(f\"Model name: {CFG.model_name}\")\n","    # LOGGER.info(f\"Model name: nextvit-small\")\n","    # model = ModelNextVit().to(CFG.device)\n","    model = Model(model_name=CFG.model_name).to(device)\n","    # model = ModelVIT().to(CFG.device)\n","    train_df = df1[df1['fold']!=fold].reset_index(drop=True)\n","    valid_df = df[df['fold']==fold].reset_index(drop=True)\n","    # print(len(valid_df))\n","    LOGGER.info(f\"Len train df: {len(train_df)}\")\n","    LOGGER.info(f\"Len valid df: {len(valid_df)}\")\n","    # cancer_labels = train_df['cancer'].values.tolist()\n","    # class_zero =len(train_df[train_df['cancer']==0])\n","    # class_one = len(train_df[train_df['cancer']==1])\n","    # class_sample_count = np.array([class_zero, class_one*32])\n","    # weight = 1. / class_sample_count\n","    # samples_weight = np.array([weight[t] for t in cancer_labels])\n","    # samples_weight = torch.from_numpy(samples_weight)\n","    # samples_weight = samples_weight.double()\n","    # print(samples_weight)\n","    # sampler = torch.utils.data.sampler.WeightedRandomSampler(samples_weight, len(samples_weight), replacement=True)\n","    \n","    train_dataset = BreastDataset(train_df, transforms=data_transforms['train'])\n","\n","    train_loader = DataLoader(train_dataset, batch_size = CFG.train_bs,\n","                                  num_workers=1, shuffle=True, pin_memory=True, drop_last=True)\n","    \n","    valid_dataset = BreastDataset(valid_df, transforms=data_transforms['valid'])\n","\n","    valid_loader = DataLoader(valid_dataset, batch_size = CFG.valid_bs, \n","                                  num_workers=1, shuffle=False, pin_memory=True, drop_last=False)\n","    \n","    LEN_DL_TRAIN = len(train_loader)\n","    best_f1 = 0\n","    best_metric = 0\n","    total_epoch = 13\n","    #checkpoint = torch.load(\"fold0/tf_efficientnetv2_b2_fold_0_model_epoch_1_0.0476_0.098.pth\")\n","    #model.load_state_dict(checkpoint['state_dict'])\n","    # base_optimizer =torch.optim.AdamW\n","    # optimizer = SAM(model.parameters(),\n","    #                 base_optimizer,\n","    #                 rho=0.05,\n","    #                 lr=1e-4,\n","    #                 weight_decay=5e-4)\n","    optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-4, weight_decay=5e-4)\n","    # optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)  \n","    #optimizer.load_state_dict(checkpoint['optimizer'])\n","    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps = 1*LEN_DL_TRAIN, num_training_steps =total_epoch*LEN_DL_TRAIN)\n","    # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, total_epoch)\n","    # scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, total_epoch-1)\n","    # scheduler = GradualWarmupSchedulerV2(optimizer, multiplier=10, total_epoch=1, after_scheduler=scheduler_cosine)\n","    # swa_model = AveragedModel(model)\n","    # swa_scheduler = SWALR(optimizer, swa_lr=1e-4, anneal_epochs=0)\n","    #scheduler.load_state_dict(checkpoint['scheduler'])\n","    criterion = nn.CrossEntropyLoss().to(CFG.device)\n","    # criterion = BiTemperedLogisticLoss(t1=0.3, t2=1.00, smoothing=0.05).to(device)\n","    # criterion1 = nn.BCEWithLogitsLoss().to(device)\n","    # criterion = nn.BCEWithLogitsLoss().to(CFG.device)\n","    LOGGER.info(f\"Train bs: {CFG.train_bs}\")\n","    # LOGGER.info(f\"Model: {model}\")\n","    \n","    LOGGER.info(f\"optimizer: {optimizer}\")\n","    LOGGER.info(f\"total_epoch :{total_epoch}\")\n","#     criterion = FocalLoss().to(device)\n","    for epoch in range(2, total_epoch+1):\n","        # if epoch >=7:\n","        #     swa_model.update_parameters(model)\n","        #     swa_scheduler.step()\n","        # else:\n","        # scheduler.step(epoch-1)\n","        LOGGER.info(f\"Epoch: {epoch}/{total_epoch}\")\n","        loss_train = train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, CFG.device)\n","        # state = {'epoch': epoch, 'state_dict': model.state_dict(),'optimizer': optimizer.state_dict(), 'scheduler':scheduler.state_dict()}\n","        # path = f'{CFG.model_name}_fold_{fold}_model_epoch_{epoch}.pth'\n","        # torch.save(state, path\n","        loss_valid, valid_preds, valid_labels = valid_fn_two(valid_loader, model, criterion, CFG.device)\n","        valid_preds = valid_preds[:, 1]\n","        # loss_valid, valid_preds_flip, valid_labels1 = valid_fn_two_flip(valid_loader, model, criterion, device)\n","        # print(valid_preds)\n","        # valid_preds = valid_preds[:, 1].reshape(-1, 1)\n","        # valid_preds_flip = valid_preds_flip[:, 1].reshape(-1, 1)\n","        # print(valid_preds.shape)\n","        # print(valid_preds_flip.shape)\n","        # valid_preds_final = np.reshape(np.average(np.concatenate([np.array(valid_preds), np.array(valid_preds_flip)], axis = 1), axis = 1), (-1, 1))\n","        # print(valid_preds_final.shape)\n","        valid_df['prediction_id'] = valid_df['patient_id'].astype(str) + '_' + valid_df['laterality'].astype(str)\n","        valid_preds = np.array(valid_preds).flatten()\n","        \n","        valid_df['raw_pred'] = valid_preds\n","        # LOGGER.info(f\"Valid loss:{loss_valid:.4f}\")\n","        LOGGER.info(f\"Train loss:{loss_train:.4f}, Valid loss:{loss_valid:.4f}\")\n","        # print(valid_df.head())\n","        grp_df = valid_df.groupby('prediction_id')['raw_pred', 'cancer'].mean()\n","        grp_df['cancer'] = grp_df['cancer'].astype(np.int)\n","        valid_labels_mean = grp_df['cancer'].values.tolist()\n","        valid_preds_mean = grp_df['raw_pred'].values.tolist()\n","        # print(valid_labels[:5], valid_preds_mean[:5])\n","        val_metric_mean = pfbeta(valid_labels_mean, valid_preds_mean)\n","        LOGGER.info(f\"Val metric mean prob: {val_metric_mean:.4f}\")\n","        best_metric_mean_at_epoch = 0\n","        best_threshold_mean = 0\n","        best_auc = 0\n","        best_cf = None\n","        for i in np.arange(0.001, 0.599, 0.001):\n","            valid_argmax = (valid_preds_mean>i).astype(np.int32)\n","    #             print(valid_argmax)\n","            val_metric = pfbeta(valid_labels_mean, valid_argmax)\n","            val_acc = accuracy_score(valid_labels_mean, valid_argmax)\n","            val_f1 = f1_score(valid_labels_mean, valid_argmax)\n","            val_auc = roc_auc_score(valid_labels_mean, valid_argmax)\n","            cf = confusion_matrix(valid_labels_mean, valid_argmax)\n","            if val_metric> best_metric_mean_at_epoch:\n","                best_metric_mean_at_epoch = val_metric\n","                best_threshold_mean = i\n","                best_auc = val_auc\n","                best_cf = cf\n","            # print(f\"Threshold: {i:.4f}, val_acc: {val_acc:.4f}, val_f1: {val_f1:.4f}, val_auc: {val_auc:.4f}, val_metric: {val_metric:.4f}\")\n","        LOGGER.info(f\"Best metric at epoch {epoch}: {best_metric_mean_at_epoch:.4f} {best_threshold_mean:.4f} {best_auc:.4f}\")\n","        LOGGER.info(f\"Cf: {best_cf}\")\n","    #         print(f\"Train loss: {loss_train:.4f}, eval loss: {loss_valid.avg:.4f}\") \n","    #         print(f\"Accuracy score: {val_acc:.4f}, f1 score: {val_f1:.4f}\")\n","    #         print(f\"Comp metric: {val_metric:.4f}\")\n","        if best_metric_mean_at_epoch > best_metric:\n","\n","            LOGGER.info(f\"Model improve: {best_metric:.4f} -> {best_metric_mean_at_epoch:.4f}\")\n","            best_metric = best_metric_mean_at_epoch\n","        state = {'epoch': epoch, 'state_dict': model.state_dict(),'optimizer': optimizer.state_dict(), 'scheduler':scheduler.state_dict()}\n","        path = f'fold{fold}/{CFG.model_name}_fold_{fold}_model_epoch_{epoch}_{best_metric_mean_at_epoch:.4f}_{best_threshold_mean:.3f}.pth'\n","        torch.save(state, path)\n","    #     loss_valid, valid_preds, valid_labels = valid_fn_no_sigmoid(valid_loader, model, criterion, device)\n","        \n","    #     # valid_preds = valid_preds[:, 1]\n","    #     valid_df['prediction_id'] = valid_df['patient_id'].astype(str) + '_' + valid_df['laterality'].astype(str)\n","    #     valid_preds = np.array(valid_preds).flatten()\n","        \n","    #     valid_df['raw_pred'] = valid_preds\n","    #     # LOGGER.info(f\"Valid loss:{loss_valid:.4f}\")\n","    #     LOGGER.info(f\"Train loss:{loss_train:.4f}, Valid loss:{loss_valid:.4f}\")\n","    #     # print(valid_df.head())\n","    #     grp_df = valid_df.groupby('prediction_id')['raw_pred', 'cancer'].mean()\n","    #     grp_df['cancer'] = grp_df['cancer'].astype(np.int)\n","    #     valid_labels_mean = grp_df['cancer'].values.tolist()\n","    #     valid_preds_mean = grp_df['raw_pred'].values.tolist()\n","    #     valid_preds_mean = [sigmoid(x) for x in valid_preds_mean]\n","    #     # print(valid_labels[:5], valid_preds_mean[:5])\n","    #     val_metric_mean = pfbeta(valid_labels_mean, valid_preds_mean)\n","    #     LOGGER.info(f\"Val metric mean prob: {val_metric_mean:.4f}\")\n","    #     best_metric_mean_at_epoch = 0\n","    #     best_threshold_mean = 0\n","    #     best_auc = 0\n","    #     best_cf = None\n","    #     for i in np.arange(0.001, 0.999, 0.001):\n","    #         valid_argmax = (valid_preds_mean>i).astype(np.int32)\n","    # #             print(valid_argmax)\n","    #         val_metric = pfbeta(valid_labels_mean, valid_argmax)\n","    #         val_acc = accuracy_score(valid_labels_mean, valid_argmax)\n","    #         val_f1 = f1_score(valid_labels_mean, valid_argmax)\n","    #         val_auc = roc_auc_score(valid_labels_mean, valid_argmax)\n","    #         cf = confusion_matrix(valid_labels_mean, valid_argmax)\n","    #         if val_metric> best_metric_mean_at_epoch:\n","    #             best_metric_mean_at_epoch = val_metric\n","    #             best_threshold_mean = i\n","    #             best_auc = val_auc\n","    #             best_cf = cf\n","    #         # print(f\"Threshold: {i:.4f}, val_acc: {val_acc:.4f}, val_f1: {val_f1:.4f}, val_auc: {val_auc:.4f}, val_metric: {val_metric:.4f}\")\n","    #     LOGGER.info(f\"Best metric at epoch {epoch}: {best_metric_mean_at_epoch:.4f} {best_threshold_mean:.4f} {best_auc:.4f}\")\n","    #     LOGGER.info(f\"Cf: {best_cf}\")\n","    # #         print(f\"Train loss: {loss_train:.4f}, eval loss: {loss_valid.avg:.4f}\") \n","    # #         print(f\"Accuracy score: {val_acc:.4f}, f1 score: {val_f1:.4f}\")\n","    # #         print(f\"Comp metric: {val_metric:.4f}\")\n","    #     if best_metric_mean_at_epoch > best_metric:\n","\n","    #         LOGGER.info(f\"Model improve: {best_metric:.4f} -> {best_metric_mean_at_epoch:.4f}\")\n","    #         best_metric = best_metric_mean_at_epoch\n","    #     state = {'epoch': epoch+1, 'state_dict': model.state_dict(),'optimizer': optimizer.state_dict(), 'scheduler':scheduler.state_dict()}\n","    #     path = f'{CFG.model_name}_fold_{fold}_model_epoch_{epoch+1}_{best_metric_mean_at_epoch:.4f}_{best_threshold_mean:.3f}.pth'\n","    #     torch.save(state, path)+\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","    \n","#     torch.optim.swa_utils.update_bn(train_loader, swa_model, device=device)\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["fold1/tf_efficientnetv2_b2_fold_1_model_epoch_4_0.4385_0.205.pth\n","fold1/tf_efficientnetv2_b2_fold_1_model_epoch_5_0.4393_0.278.pth\n","fold1/tf_efficientnetv2_b2_fold_1_model_epoch_6_0.4432_0.319.pth\n","fold1/tf_efficientnetv2_b2_fold_1_model_epoch_8_0.4231_0.320.pth\n","fold1/tf_efficientnetv2_b2_fold_1_model_epoch_7_0.4578_0.382.pth\n","fold1/tf_efficientnetv2_b2_fold_1_model_epoch_10_0.4339_0.246.pth\n","fold1/tf_efficientnetv2_b2_fold_1_model_epoch_11_0.4211_0.242.pth\n","fold1/tf_efficientnetv2_b2_fold_1_model_epoch_6_0.4235_0.473.pth\n","\n","swa_model_fold1.pth\n"]}],"source":["out_file = 'swa_model_fold1.pth' \n","iteration = [\n","    # 'fold1/tf_efficientnetv2_b2_fold_1_model_epoch_4_0.4385_0.205.pth',\n","    # 'fold1/tf_efficientnetv2_b2_fold_1_model_epoch_5_0.4393_0.278.pth',\n","    # 'fold1/tf_efficientnetv2_b2_fold_1_model_epoch_6_0.4432_0.319.pth',\n","    # 'fold1/tf_efficientnetv2_b2_fold_1_model_epoch_8_0.4231_0.320.pth',\n","    # 'fold1/tf_efficientnetv2_b2_fold_1_model_epoch_7_0.4578_0.382.pth',\n","    # 'fold1/tf_efficientnetv2_b2_fold_1_model_epoch_10_0.4339_0.246.pth',\n","    # 'fold1/tf_efficientnetv2_b2_fold_1_model_epoch_11_0.4211_0.242.pth',\n","    \n","    # 'fold0/tf_efficientnetv2_b2_fold_0_model_epoch_4_0.4151_0.352.pth',\n","    # 'fold0/tf_efficientnetv2_b2_fold_0_model_epoch_5_0.4757_0.230.pth',\n","    # 'fold0/tf_efficientnetv2_b2_fold_0_model_epoch_6_0.4520_0.128.pth',\n","    # 'fold0/tf_efficientnetv2_b2_fold_0_model_epoch_7_0.4510_0.266.pth',\n","    # 'fold0/tf_efficientnetv2_b2_fold_0_model_epoch_8_0.4403_0.415.pth',\n","    # 'fold0/tf_efficientnetv2_b2_fold_0_model_epoch_9_0.4713_0.430.pth',\n","    # 'fold0/tf_efficientnetv2_b2_fold_0_model_epoch_10_0.4569_0.259.pth',\n","    # 'fold0/tf_efficientnetv2_b2_fold_0_model_epoch_11_0.4387_0.436.pth'\n","    # 'fold2/tf_efficientnetv2_b2_fold_2_model_epoch_3_0.4000_0.122.pth',\n","    # 'fold2/tf_efficientnetv2_b2_fold_2_model_epoch_4_0.4585_0.236.pth',\n","    # 'fold2/tf_efficientnetv2_b2_fold_2_model_epoch_5_0.4149_0.131.pth',\n","    # 'fold2/tf_efficientnetv2_b2_fold_2_model_epoch_6_0.4516_0.188.pth',\n","    # 'fold2/tf_efficientnetv2_b2_fold_2_model_epoch_7_0.4557_0.241.pth',\n","    # 'fold2/tf_efficientnetv2_b2_fold_2_model_epoch_8_0.4455_0.208.pth',\n","    # 'fold2/tf_efficientnetv2_b2_fold_2_model_epoch_9_0.4681_0.319.pth',\n","    # 'fold2/tf_efficientnetv2_b2_fold_2_model_epoch_10_0.4550_0.245.pth',\n","    # 'fold2/tf_efficientnetv2_b2_fold_2_model_epoch_11_0.4500_0.373.pth',\n","    # 'fold2/tf_efficientnetv2_b2_fold_2_model_epoch_12_0.4457_0.298.pth',\n","    # 'fold3/tf_efficientnetv2_b2_fold_3_model_epoch_3_0.3867_0.302.pth',\n","    # 'fold3/tf_efficientnetv2_b2_fold_3_model_epoch_4_0.3924_0.275.pth',\n","    # 'fold3/tf_efficientnetv2_b2_fold_3_model_epoch_6_0.4030_0.339.pth',\n","    # 'fold3/tf_efficientnetv2_b2_fold_3_model_epoch_5_0.3850_0.161.pth',\n","    # 'fold3/tf_efficientnetv2_b2_fold_3_model_epoch_7_0.4192_0.270.pth',\n","    # 'fold3/tf_efficientnetv2_b2_fold_3_model_epoch_8_0.3913_0.362.pth'\n","    # 'fold3/tf_efficientnetv2_b2_fold_3_model_epoch_4_0.4103_0.343.pth',\n","    # 'fold3/tf_efficientnetv2_b2_fold_3_model_epoch_5_0.4041_0.141.pth',\n","    # 'fold3/tf_efficientnetv2_b2_fold_3_model_epoch_6_0.4648_0.444.pth',\n","    # 'fold3/tf_efficientnetv2_b2_fold_3_model_epoch_7_0.4103_0.310.pth',\n","    # 'fold3/tf_efficientnetv2_b2_fold_3_model_epoch_8_0.4471_0.371.pth',\n","    # 'fold3/tf_efficientnetv2_b2_fold_3_model_epoch_10_0.4062_0.202.pth',\n","    # 'fold3/tf_efficientnetv2_b2_fold_3_model_epoch_7_0.4192_0.270.pth',\n","    # 'fold3/tf_efficientnetv2_b2_fold_3_model_epoch_11_0.4309_0.199.pth',\n","    # 'fold3/tf_efficientnetv2_b2_fold_3_model_epoch_12_0.4074_0.278.pth'\n","    # 'fold4/tf_efficientnetv2_b2_fold_4_model_epoch_5_0.3889_0.407.pth',\n","    # 'fold4/tf_efficientnetv2_b2_fold_4_model_epoch_4_0.4276_0.403.pth',\n","    # 'fold4/tf_efficientnetv2_b2_fold_4_model_epoch_6_0.4000_0.586.pth',\n","    # 'fold4/tf_efficientnetv2_b2_fold_4_model_epoch_7_0.3913_0.444.pth',\n","    # 'fold4/tf_efficientnetv2_b2_fold_4_model_epoch_8_0.3916_0.483.pth'\n","\n","]\n","#46789101112 4824\n","state_dict = None\n","for i in iteration:\n","    f = i\n","    print(f)\n","    f = torch.load(f, map_location=lambda storage, loc: storage)\n","    if state_dict is None:\n","        state_dict = f['state_dict']\n","    else:\n","        key = list(f['state_dict'].keys())\n","        for k in key:\n","            state_dict[k] = state_dict[k] + f['state_dict'][k]\n","\n","for k in key:\n","    state_dict[k] = state_dict[k] / len(iteration)\n","print('')\n","\n","print(out_file)\n","torch.save({'state_dict': state_dict}, out_file)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['adv_inception_v3', 'bat_resnext26ts', 'beit_base_patch16_224', 'beit_base_patch16_224_in22k', 'beit_base_patch16_384', 'beit_large_patch16_224', 'beit_large_patch16_224_in22k', 'beit_large_patch16_384', 'beit_large_patch16_512', 'beitv2_base_patch16_224', 'beitv2_base_patch16_224_in22k', 'beitv2_large_patch16_224', 'beitv2_large_patch16_224_in22k', 'botnet26t_256', 'cait_m36_384', 'cait_m48_448', 'cait_s24_224', 'cait_s24_384', 'cait_s36_384', 'cait_xs24_384', 'cait_xxs24_224', 'cait_xxs24_384', 'cait_xxs36_224', 'cait_xxs36_384', 'coat_lite_mini', 'coat_lite_small', 'coat_lite_tiny', 'coat_mini', 'coat_tiny', 'coatnet_0_rw_224', 'coatnet_1_rw_224', 'coatnet_bn_0_rw_224', 'coatnet_nano_rw_224', 'coatnet_rmlp_1_rw_224', 'coatnet_rmlp_nano_rw_224', 'convit_base', 'convit_small', 'convit_tiny', 'convmixer_768_32', 'convmixer_1024_20_ks9_p14', 'convmixer_1536_20', 'convnext_atto', 'convnext_atto_ols', 'convnext_base', 'convnext_base_384_in22ft1k', 'convnext_base_in22ft1k', 'convnext_base_in22k', 'convnext_femto', 'convnext_femto_ols', 'convnext_large', 'convnext_large_384_in22ft1k', 'convnext_large_in22ft1k', 'convnext_large_in22k', 'convnext_nano', 'convnext_nano_ols', 'convnext_pico', 'convnext_pico_ols', 'convnext_small', 'convnext_small_384_in22ft1k', 'convnext_small_in22ft1k', 'convnext_small_in22k', 'convnext_tiny', 'convnext_tiny_384_in22ft1k', 'convnext_tiny_hnf', 'convnext_tiny_in22ft1k', 'convnext_tiny_in22k', 'convnext_xlarge_384_in22ft1k', 'convnext_xlarge_in22ft1k', 'convnext_xlarge_in22k', 'crossvit_9_240', 'crossvit_9_dagger_240', 'crossvit_15_240', 'crossvit_15_dagger_240', 'crossvit_15_dagger_408', 'crossvit_18_240', 'crossvit_18_dagger_240', 'crossvit_18_dagger_408', 'crossvit_base_240', 'crossvit_small_240', 'crossvit_tiny_240', 'cs3darknet_focus_l', 'cs3darknet_focus_m', 'cs3darknet_l', 'cs3darknet_m', 'cs3darknet_x', 'cs3edgenet_x', 'cs3se_edgenet_x', 'cs3sedarknet_l', 'cs3sedarknet_x', 'cspdarknet53', 'cspresnet50', 'cspresnext50', 'darknet53', 'darknetaa53', 'deit3_base_patch16_224', 'deit3_base_patch16_224_in21ft1k', 'deit3_base_patch16_384', 'deit3_base_patch16_384_in21ft1k', 'deit3_huge_patch14_224', 'deit3_huge_patch14_224_in21ft1k', 'deit3_large_patch16_224', 'deit3_large_patch16_224_in21ft1k', 'deit3_large_patch16_384', 'deit3_large_patch16_384_in21ft1k', 'deit3_medium_patch16_224', 'deit3_medium_patch16_224_in21ft1k', 'deit3_small_patch16_224', 'deit3_small_patch16_224_in21ft1k', 'deit3_small_patch16_384', 'deit3_small_patch16_384_in21ft1k', 'deit_base_distilled_patch16_224', 'deit_base_distilled_patch16_384', 'deit_base_patch16_224', 'deit_base_patch16_384', 'deit_small_distilled_patch16_224', 'deit_small_patch16_224', 'deit_tiny_distilled_patch16_224', 'deit_tiny_patch16_224', 'densenet121', 'densenet161', 'densenet169', 'densenet201', 'densenetblur121d', 'dla34', 'dla46_c', 'dla46x_c', 'dla60', 'dla60_res2net', 'dla60_res2next', 'dla60x', 'dla60x_c', 'dla102', 'dla102x', 'dla102x2', 'dla169', 'dm_nfnet_f0', 'dm_nfnet_f1', 'dm_nfnet_f2', 'dm_nfnet_f3', 'dm_nfnet_f4', 'dm_nfnet_f5', 'dm_nfnet_f6', 'dpn68', 'dpn68b', 'dpn92', 'dpn98', 'dpn107', 'dpn131', 'eca_botnext26ts_256', 'eca_halonext26ts', 'eca_nfnet_l0', 'eca_nfnet_l1', 'eca_nfnet_l2', 'eca_resnet33ts', 'eca_resnext26ts', 'ecaresnet26t', 'ecaresnet50d', 'ecaresnet50d_pruned', 'ecaresnet50t', 'ecaresnet101d', 'ecaresnet101d_pruned', 'ecaresnet269d', 'ecaresnetlight', 'edgenext_base', 'edgenext_small', 'edgenext_small_rw', 'edgenext_x_small', 'edgenext_xx_small', 'efficientformer_l1', 'efficientformer_l3', 'efficientformer_l7', 'efficientnet_b0', 'efficientnet_b1', 'efficientnet_b1_pruned', 'efficientnet_b2', 'efficientnet_b2_pruned', 'efficientnet_b3', 'efficientnet_b3_pruned', 'efficientnet_b4', 'efficientnet_el', 'efficientnet_el_pruned', 'efficientnet_em', 'efficientnet_es', 'efficientnet_es_pruned', 'efficientnet_lite0', 'efficientnetv2_rw_m', 'efficientnetv2_rw_s', 'efficientnetv2_rw_t', 'ens_adv_inception_resnet_v2', 'ese_vovnet19b_dw', 'ese_vovnet39b', 'fbnetc_100', 'fbnetv3_b', 'fbnetv3_d', 'fbnetv3_g', 'gc_efficientnetv2_rw_t', 'gcresnet33ts', 'gcresnet50t', 'gcresnext26ts', 'gcresnext50ts', 'gcvit_base', 'gcvit_small', 'gcvit_tiny', 'gcvit_xtiny', 'gcvit_xxtiny', 'gernet_l', 'gernet_m', 'gernet_s', 'ghostnet_100', 'gluon_inception_v3', 'gluon_resnet18_v1b', 'gluon_resnet34_v1b', 'gluon_resnet50_v1b', 'gluon_resnet50_v1c', 'gluon_resnet50_v1d', 'gluon_resnet50_v1s', 'gluon_resnet101_v1b', 'gluon_resnet101_v1c', 'gluon_resnet101_v1d', 'gluon_resnet101_v1s', 'gluon_resnet152_v1b', 'gluon_resnet152_v1c', 'gluon_resnet152_v1d', 'gluon_resnet152_v1s', 'gluon_resnext50_32x4d', 'gluon_resnext101_32x4d', 'gluon_resnext101_64x4d', 'gluon_senet154', 'gluon_seresnext50_32x4d', 'gluon_seresnext101_32x4d', 'gluon_seresnext101_64x4d', 'gluon_xception65', 'gmixer_24_224', 'gmlp_s16_224', 'halo2botnet50ts_256', 'halonet26t', 'halonet50ts', 'haloregnetz_b', 'hardcorenas_a', 'hardcorenas_b', 'hardcorenas_c', 'hardcorenas_d', 'hardcorenas_e', 'hardcorenas_f', 'hrnet_w18', 'hrnet_w18_small', 'hrnet_w18_small_v2', 'hrnet_w30', 'hrnet_w32', 'hrnet_w40', 'hrnet_w44', 'hrnet_w48', 'hrnet_w64', 'ig_resnext101_32x8d', 'ig_resnext101_32x16d', 'ig_resnext101_32x32d', 'ig_resnext101_32x48d', 'inception_resnet_v2', 'inception_v3', 'inception_v4', 'jx_nest_base', 'jx_nest_small', 'jx_nest_tiny', 'lambda_resnet26rpt_256', 'lambda_resnet26t', 'lambda_resnet50ts', 'lamhalobotnet50ts_256', 'lcnet_050', 'lcnet_075', 'lcnet_100', 'legacy_senet154', 'legacy_seresnet18', 'legacy_seresnet34', 'legacy_seresnet50', 'legacy_seresnet101', 'legacy_seresnet152', 'legacy_seresnext26_32x4d', 'legacy_seresnext50_32x4d', 'legacy_seresnext101_32x4d', 'levit_128', 'levit_128s', 'levit_192', 'levit_256', 'levit_384', 'maxvit_nano_rw_256', 'maxvit_rmlp_nano_rw_256', 'maxvit_rmlp_pico_rw_256', 'maxvit_rmlp_tiny_rw_256', 'maxvit_tiny_rw_224', 'mixer_b16_224', 'mixer_b16_224_in21k', 'mixer_b16_224_miil', 'mixer_b16_224_miil_in21k', 'mixer_l16_224', 'mixer_l16_224_in21k', 'mixnet_l', 'mixnet_m', 'mixnet_s', 'mixnet_xl', 'mnasnet_100', 'mnasnet_small', 'mobilenetv2_050', 'mobilenetv2_100', 'mobilenetv2_110d', 'mobilenetv2_120d', 'mobilenetv2_140', 'mobilenetv3_large_100', 'mobilenetv3_large_100_miil', 'mobilenetv3_large_100_miil_in21k', 'mobilenetv3_rw', 'mobilenetv3_small_050', 'mobilenetv3_small_075', 'mobilenetv3_small_100', 'mobilevit_s', 'mobilevit_xs', 'mobilevit_xxs', 'mobilevitv2_050', 'mobilevitv2_075', 'mobilevitv2_100', 'mobilevitv2_125', 'mobilevitv2_150', 'mobilevitv2_150_384_in22ft1k', 'mobilevitv2_150_in22ft1k', 'mobilevitv2_175', 'mobilevitv2_175_384_in22ft1k', 'mobilevitv2_175_in22ft1k', 'mobilevitv2_200', 'mobilevitv2_200_384_in22ft1k', 'mobilevitv2_200_in22ft1k', 'mvitv2_base', 'mvitv2_large', 'mvitv2_small', 'mvitv2_tiny', 'nasnetalarge', 'nf_regnet_b1', 'nf_resnet50', 'nfnet_l0', 'pit_b_224', 'pit_b_distilled_224', 'pit_s_224', 'pit_s_distilled_224', 'pit_ti_224', 'pit_ti_distilled_224', 'pit_xs_224', 'pit_xs_distilled_224', 'pnasnet5large', 'poolformer_m36', 'poolformer_m48', 'poolformer_s12', 'poolformer_s24', 'poolformer_s36', 'pvt_v2_b0', 'pvt_v2_b1', 'pvt_v2_b2', 'pvt_v2_b2_li', 'pvt_v2_b3', 'pvt_v2_b4', 'pvt_v2_b5', 'regnetv_040', 'regnetv_064', 'regnetx_002', 'regnetx_004', 'regnetx_006', 'regnetx_008', 'regnetx_016', 'regnetx_032', 'regnetx_040', 'regnetx_064', 'regnetx_080', 'regnetx_120', 'regnetx_160', 'regnetx_320', 'regnety_002', 'regnety_004', 'regnety_006', 'regnety_008', 'regnety_016', 'regnety_032', 'regnety_040', 'regnety_064', 'regnety_080', 'regnety_120', 'regnety_160', 'regnety_320', 'regnetz_040', 'regnetz_040h', 'regnetz_b16', 'regnetz_c16', 'regnetz_c16_evos', 'regnetz_d8', 'regnetz_d8_evos', 'regnetz_d32', 'regnetz_e8', 'repvgg_a2', 'repvgg_b0', 'repvgg_b1', 'repvgg_b1g4', 'repvgg_b2', 'repvgg_b2g4', 'repvgg_b3', 'repvgg_b3g4', 'res2net50_14w_8s', 'res2net50_26w_4s', 'res2net50_26w_6s', 'res2net50_26w_8s', 'res2net50_48w_2s', 'res2net101_26w_4s', 'res2next50', 'resmlp_12_224', 'resmlp_12_224_dino', 'resmlp_12_distilled_224', 'resmlp_24_224', 'resmlp_24_224_dino', 'resmlp_24_distilled_224', 'resmlp_36_224', 'resmlp_36_distilled_224', 'resmlp_big_24_224', 'resmlp_big_24_224_in22ft1k', 'resmlp_big_24_distilled_224', 'resnest14d', 'resnest26d', 'resnest50d', 'resnest50d_1s4x24d', 'resnest50d_4s2x40d', 'resnest101e', 'resnest200e', 'resnest269e', 'resnet10t', 'resnet14t', 'resnet18', 'resnet18d', 'resnet26', 'resnet26d', 'resnet26t', 'resnet32ts', 'resnet33ts', 'resnet34', 'resnet34d', 'resnet50', 'resnet50_gn', 'resnet50d', 'resnet51q', 'resnet61q', 'resnet101', 'resnet101d', 'resnet152', 'resnet152d', 'resnet200d', 'resnetaa50', 'resnetblur50', 'resnetrs50', 'resnetrs101', 'resnetrs152', 'resnetrs200', 'resnetrs270', 'resnetrs350', 'resnetrs420', 'resnetv2_50', 'resnetv2_50d_evos', 'resnetv2_50d_gn', 'resnetv2_50x1_bit_distilled', 'resnetv2_50x1_bitm', 'resnetv2_50x1_bitm_in21k', 'resnetv2_50x3_bitm', 'resnetv2_50x3_bitm_in21k', 'resnetv2_101', 'resnetv2_101x1_bitm', 'resnetv2_101x1_bitm_in21k', 'resnetv2_101x3_bitm', 'resnetv2_101x3_bitm_in21k', 'resnetv2_152x2_bit_teacher', 'resnetv2_152x2_bit_teacher_384', 'resnetv2_152x2_bitm', 'resnetv2_152x2_bitm_in21k', 'resnetv2_152x4_bitm', 'resnetv2_152x4_bitm_in21k', 'resnext26ts', 'resnext50_32x4d', 'resnext50d_32x4d', 'resnext101_32x8d', 'resnext101_64x4d', 'rexnet_100', 'rexnet_130', 'rexnet_150', 'rexnet_200', 'sebotnet33ts_256', 'sehalonet33ts', 'selecsls42b', 'selecsls60', 'selecsls60b', 'semnasnet_075', 'semnasnet_100', 'sequencer2d_l', 'sequencer2d_m', 'sequencer2d_s', 'seresnet33ts', 'seresnet50', 'seresnet152d', 'seresnext26d_32x4d', 'seresnext26t_32x4d', 'seresnext26ts', 'seresnext50_32x4d', 'seresnext101_32x8d', 'seresnext101d_32x8d', 'seresnextaa101d_32x8d', 'skresnet18', 'skresnet34', 'skresnext50_32x4d', 'spnasnet_100', 'ssl_resnet18', 'ssl_resnet50', 'ssl_resnext50_32x4d', 'ssl_resnext101_32x4d', 'ssl_resnext101_32x8d', 'ssl_resnext101_32x16d', 'swin_base_patch4_window7_224', 'swin_base_patch4_window7_224_in22k', 'swin_base_patch4_window12_384', 'swin_base_patch4_window12_384_in22k', 'swin_large_patch4_window7_224', 'swin_large_patch4_window7_224_in22k', 'swin_large_patch4_window12_384', 'swin_large_patch4_window12_384_in22k', 'swin_s3_base_224', 'swin_s3_small_224', 'swin_s3_tiny_224', 'swin_small_patch4_window7_224', 'swin_tiny_patch4_window7_224', 'swinv2_base_window8_256', 'swinv2_base_window12_192_22k', 'swinv2_base_window12to16_192to256_22kft1k', 'swinv2_base_window12to24_192to384_22kft1k', 'swinv2_base_window16_256', 'swinv2_cr_small_224', 'swinv2_cr_small_ns_224', 'swinv2_cr_tiny_ns_224', 'swinv2_large_window12_192_22k', 'swinv2_large_window12to16_192to256_22kft1k', 'swinv2_large_window12to24_192to384_22kft1k', 'swinv2_small_window8_256', 'swinv2_small_window16_256', 'swinv2_tiny_window8_256', 'swinv2_tiny_window16_256', 'swsl_resnet18', 'swsl_resnet50', 'swsl_resnext50_32x4d', 'swsl_resnext101_32x4d', 'swsl_resnext101_32x8d', 'swsl_resnext101_32x16d', 'tf_efficientnet_b0', 'tf_efficientnet_b0_ap', 'tf_efficientnet_b0_ns', 'tf_efficientnet_b1', 'tf_efficientnet_b1_ap', 'tf_efficientnet_b1_ns', 'tf_efficientnet_b2', 'tf_efficientnet_b2_ap', 'tf_efficientnet_b2_ns', 'tf_efficientnet_b3', 'tf_efficientnet_b3_ap', 'tf_efficientnet_b3_ns', 'tf_efficientnet_b4', 'tf_efficientnet_b4_ap', 'tf_efficientnet_b4_ns', 'tf_efficientnet_b5', 'tf_efficientnet_b5_ap', 'tf_efficientnet_b5_ns', 'tf_efficientnet_b6', 'tf_efficientnet_b6_ap', 'tf_efficientnet_b6_ns', 'tf_efficientnet_b7', 'tf_efficientnet_b7_ap', 'tf_efficientnet_b7_ns', 'tf_efficientnet_b8', 'tf_efficientnet_b8_ap', 'tf_efficientnet_cc_b0_4e', 'tf_efficientnet_cc_b0_8e', 'tf_efficientnet_cc_b1_8e', 'tf_efficientnet_el', 'tf_efficientnet_em', 'tf_efficientnet_es', 'tf_efficientnet_l2_ns', 'tf_efficientnet_l2_ns_475', 'tf_efficientnet_lite0', 'tf_efficientnet_lite1', 'tf_efficientnet_lite2', 'tf_efficientnet_lite3', 'tf_efficientnet_lite4', 'tf_efficientnetv2_b0', 'tf_efficientnetv2_b1', 'tf_efficientnetv2_b2', 'tf_efficientnetv2_b3', 'tf_efficientnetv2_l', 'tf_efficientnetv2_l_in21ft1k', 'tf_efficientnetv2_l_in21k', 'tf_efficientnetv2_m', 'tf_efficientnetv2_m_in21ft1k', 'tf_efficientnetv2_m_in21k', 'tf_efficientnetv2_s', 'tf_efficientnetv2_s_in21ft1k', 'tf_efficientnetv2_s_in21k', 'tf_efficientnetv2_xl_in21ft1k', 'tf_efficientnetv2_xl_in21k', 'tf_inception_v3', 'tf_mixnet_l', 'tf_mixnet_m', 'tf_mixnet_s', 'tf_mobilenetv3_large_075', 'tf_mobilenetv3_large_100', 'tf_mobilenetv3_large_minimal_100', 'tf_mobilenetv3_small_075', 'tf_mobilenetv3_small_100', 'tf_mobilenetv3_small_minimal_100', 'tinynet_a', 'tinynet_b', 'tinynet_c', 'tinynet_d', 'tinynet_e', 'tnt_s_patch16_224', 'tresnet_l', 'tresnet_l_448', 'tresnet_m', 'tresnet_m_448', 'tresnet_m_miil_in21k', 'tresnet_v2_l', 'tresnet_xl', 'tresnet_xl_448', 'tv_densenet121', 'tv_resnet34', 'tv_resnet50', 'tv_resnet101', 'tv_resnet152', 'tv_resnext50_32x4d', 'twins_pcpvt_base', 'twins_pcpvt_large', 'twins_pcpvt_small', 'twins_svt_base', 'twins_svt_large', 'twins_svt_small', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19', 'vgg19_bn', 'visformer_small', 'vit_base_patch8_224', 'vit_base_patch8_224_dino', 'vit_base_patch8_224_in21k', 'vit_base_patch16_224', 'vit_base_patch16_224_dino', 'vit_base_patch16_224_in21k', 'vit_base_patch16_224_miil', 'vit_base_patch16_224_miil_in21k', 'vit_base_patch16_224_sam', 'vit_base_patch16_384', 'vit_base_patch16_rpn_224', 'vit_base_patch32_224', 'vit_base_patch32_224_clip_laion2b', 'vit_base_patch32_224_in21k', 'vit_base_patch32_224_sam', 'vit_base_patch32_384', 'vit_base_r50_s16_224_in21k', 'vit_base_r50_s16_384', 'vit_giant_patch14_224_clip_laion2b', 'vit_huge_patch14_224_clip_laion2b', 'vit_huge_patch14_224_in21k', 'vit_large_patch14_224_clip_laion2b', 'vit_large_patch16_224', 'vit_large_patch16_224_in21k', 'vit_large_patch16_384', 'vit_large_patch32_224_in21k', 'vit_large_patch32_384', 'vit_large_r50_s32_224', 'vit_large_r50_s32_224_in21k', 'vit_large_r50_s32_384', 'vit_relpos_base_patch16_224', 'vit_relpos_base_patch16_clsgap_224', 'vit_relpos_base_patch32_plus_rpn_256', 'vit_relpos_medium_patch16_224', 'vit_relpos_medium_patch16_cls_224', 'vit_relpos_medium_patch16_rpn_224', 'vit_relpos_small_patch16_224', 'vit_small_patch8_224_dino', 'vit_small_patch16_224', 'vit_small_patch16_224_dino', 'vit_small_patch16_224_in21k', 'vit_small_patch16_384', 'vit_small_patch32_224', 'vit_small_patch32_224_in21k', 'vit_small_patch32_384', 'vit_small_r26_s32_224', 'vit_small_r26_s32_224_in21k', 'vit_small_r26_s32_384', 'vit_srelpos_medium_patch16_224', 'vit_srelpos_small_patch16_224', 'vit_tiny_patch16_224', 'vit_tiny_patch16_224_in21k', 'vit_tiny_patch16_384', 'vit_tiny_r_s16_p8_224', 'vit_tiny_r_s16_p8_224_in21k', 'vit_tiny_r_s16_p8_384', 'volo_d1_224', 'volo_d1_384', 'volo_d2_224', 'volo_d2_384', 'volo_d3_224', 'volo_d3_448', 'volo_d4_224', 'volo_d4_448', 'volo_d5_224', 'volo_d5_448', 'volo_d5_512', 'wide_resnet50_2', 'wide_resnet101_2', 'xception', 'xception41', 'xception41p', 'xception65', 'xception65p', 'xception71', 'xcit_large_24_p8_224', 'xcit_large_24_p8_224_dist', 'xcit_large_24_p8_384_dist', 'xcit_large_24_p16_224', 'xcit_large_24_p16_224_dist', 'xcit_large_24_p16_384_dist', 'xcit_medium_24_p8_224', 'xcit_medium_24_p8_224_dist', 'xcit_medium_24_p8_384_dist', 'xcit_medium_24_p16_224', 'xcit_medium_24_p16_224_dist', 'xcit_medium_24_p16_384_dist', 'xcit_nano_12_p8_224', 'xcit_nano_12_p8_224_dist', 'xcit_nano_12_p8_384_dist', 'xcit_nano_12_p16_224', 'xcit_nano_12_p16_224_dist', 'xcit_nano_12_p16_384_dist', 'xcit_small_12_p8_224', 'xcit_small_12_p8_224_dist', 'xcit_small_12_p8_384_dist', 'xcit_small_12_p16_224', 'xcit_small_12_p16_224_dist', 'xcit_small_12_p16_384_dist', 'xcit_small_24_p8_224', 'xcit_small_24_p8_224_dist', 'xcit_small_24_p8_384_dist', 'xcit_small_24_p16_224', 'xcit_small_24_p16_224_dist', 'xcit_small_24_p16_384_dist', 'xcit_tiny_12_p8_224', 'xcit_tiny_12_p8_224_dist', 'xcit_tiny_12_p8_384_dist', 'xcit_tiny_12_p16_224', 'xcit_tiny_12_p16_224_dist', 'xcit_tiny_12_p16_384_dist', 'xcit_tiny_24_p8_224', 'xcit_tiny_24_p8_224_dist', 'xcit_tiny_24_p8_384_dist', 'xcit_tiny_24_p16_224', 'xcit_tiny_24_p16_224_dist', 'xcit_tiny_24_p16_384_dist']\n"]}],"source":["avail_pretrained_models = timm.list_models(pretrained=True)\n","print(avail_pretrained_models)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Fold: 0\n","Len train df: 45577\n","Len valid df: 10979\n","Fold: 1\n","Len train df: 45673\n","Len valid df: 10879\n","Fold: 2\n","Len train df: 45512\n","Len valid df: 11012\n","Fold: 3\n","Len train df: 45689\n","Len valid df: 10891\n","Fold: 4\n","Len train df: 45637\n","Len valid df: 10945\n"]}],"source":["# for fold in [0, 1, 2, 3, 4]:\n","#     LOGGER.info(f\"Fold: {fold}\")\n","#     model = Model(model_name=CFG.model_name).to(device)\n","#     # model = ModelVIT().to(CFG.device)\n","#     train_df = df1[df1['fold']!=fold].reset_index(drop=True)\n","#     valid_df = df[df['fold']==fold].reset_index(drop=True)\n","#     # print(len(valid_df))\n","#     LOGGER.info(f\"Len train df: {len(train_df)}\")\n","#     LOGGER.info(f\"Len valid df: {len(valid_df)}\")"]}],"metadata":{"kernelspec":{"display_name":"zaloenv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"d81213625f550c7b434bbb4e964cd1250716e6d81b88f327aa7e418dc0078b84"}}},"nbformat":4,"nbformat_minor":4}
